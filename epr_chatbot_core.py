"""
EPR Legal Chatbot - Core Module
Vietnamese EPR (Extended Producer Responsibility) Legal Question-Answering System
"""

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Set environment variables
os.environ['LANGCHAIN_TRACING_V2'] = os.getenv('LANGCHAIN_TRACING_V2', 'true')
os.environ['LANGCHAIN_ENDPOINT'] = os.getenv('LANGCHAIN_ENDPOINT', 'https://api.smith.langchain.com')
os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY', '')
os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY', '')
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', '')


# Updated imports - ChromaTranslator is now in a different location
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# Storage and vectorstore
from langchain_core.stores import InMemoryByteStore
from langchain_chroma import Chroma

# Retrievers
from langchain.retrievers.multi_vector import MultiVectorRetriever

# Self-query imports
from langchain.chains.query_constructor.schema import AttributeInfo
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import (
    StructuredQueryOutputParser,
    get_query_constructor_prompt,
)

# FIXED: ChromaTranslator import
try:
    from langchain.retrievers.self_query.chroma import ChromaTranslator
except:
    from langchain_community.query_constructors.chroma import ChromaTranslator

import uuid
import tiktoken

print("‚úì All imports successful!")

from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import uuid

# ========== TOKEN COUNTING UTILITIES ==========

def count_tokens(text: str, model: str = "gpt-3.5-turbo") -> int:
    """Count the number of tokens in a text string"""
    try:
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except Exception as e:
        print(f"  ‚ö†Ô∏è Error counting tokens: {e}")
        # Rough estimation: ~4 characters per token
        return len(text) // 4

def truncate_text(text: str, max_tokens: int = 1000, model: str = "gpt-3.5-turbo") -> str:
    """Truncate text to fit within max_tokens"""
    try:
        encoding = tiktoken.encoding_for_model(model)
        tokens = encoding.encode(text)

        if len(tokens) <= max_tokens:
            return text

        # Truncate and decode back to text
        truncated_tokens = tokens[:max_tokens]
        return encoding.decode(truncated_tokens) + "..."
    except Exception as e:
        print(f"  ‚ö†Ô∏è Error truncating text: {e}")
        # Rough fallback: character-based truncation
        max_chars = max_tokens * 4
        if len(text) <= max_chars:
            return text
        return text[:max_chars] + "..."

print("‚úì Token counting utilities loaded")

# ========== CONFIGURATION ==========

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Initialize Qdrant client - Cloud or Local
USE_QDRANT_CLOUD = os.getenv('USE_QDRANT_CLOUD', 'false').lower() == 'true'
QDRANT_CLOUD_URL = os.getenv('QDRANT_CLOUD_URL')
QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')

if USE_QDRANT_CLOUD and QDRANT_CLOUD_URL and QDRANT_API_KEY:
    # Use Qdrant Cloud
    try:
        client = QdrantClient(
            url=QDRANT_CLOUD_URL,
            api_key=QDRANT_API_KEY,
        )
        print("‚úÖ Connected to Qdrant Cloud")
        print(f"   URL: {QDRANT_CLOUD_URL}")
    except Exception as e:
        print(f"‚ùå Failed to connect to Qdrant Cloud: {e}")
        print("‚ö†Ô∏è  Falling back to local storage...")
        try:
            client = QdrantClient(path="./qdrant_faq_db")
            print("‚úÖ Using persistent Qdrant database at ./qdrant_faq_db")
        except Exception as e2:
            print(f"‚ö†Ô∏è  Could not use file-based database: {e2}")
            print("üìù Using in-memory Qdrant database instead")
            client = QdrantClient(":memory:")
else:
    # Use local Qdrant
    print("üìç Using local Qdrant storage")
    try:
        client = QdrantClient(path="./qdrant_faq_db")
        print("‚úÖ Using persistent Qdrant database at ./qdrant_faq_db")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not use file-based database: {e}")
        print("üìù Using in-memory Qdrant database instead")
        client = QdrantClient(":memory:")

collection_name = "faq_collection"

# ========== FAQ DATA ==========

faq = {
    "meta": [
        {
            "C√¢u h·ªèi": "Ki·∫øn th·ª©c c·ªßa b·∫°n bao g·ªìm nh·ªØng g√¨?",
            "Tr·∫£ l·ªùi": "Ki·∫øn th·ª©c c·ªßa t√¥i bao g·ªìm c√°c ƒëi·ªÅu lu·∫≠t c·ªßa vƒÉn b·∫£n ph√°p lu·∫≠t v·ªÅ EPR c·ªßa Vi·ªát Nam"
        },
        {
            "C√¢u h·ªèi": "C√°c ƒë·ªëi t∆∞·ª£ng n√†o ph·∫£i th·ª±c hi·ªán tr√°ch nhi·ªám t√°i ch·∫ø?",
            "Tr·∫£ l·ªùi": "Theo ƒêi·ªÅu 77 v√† Ph·ª• l·ª•c XXII Ngh·ªã ƒë·ªãnh s·ªë 08/2022/Nƒê-CP quy ƒë·ªãnh chi ti·∫øt m·ªôt s·ªë ƒëi·ªÅu c·ªßa Lu·∫≠t B·∫£o v·ªá m√¥i tr∆∞·ªùng, c√°c t·ªï ch·ª©c s·∫£n xu·∫•t, nh·∫≠p kh·∫©u s·∫£n ph·∫©m, bao b√¨ ph·∫£i th·ª±c hi·ªán tr√°ch nhi·ªám t√°i ch·∫ø."
        },
        {
            "C√¢u h·ªèi": "Bao b√¨ th∆∞∆°ng ph·∫©m ƒë∆∞·ª£c hi·ªÉu nh∆∞ th·∫ø n√†o?",
            "Tr·∫£ l·ªùi": "Theo ƒêi·ªÅu 3 Ngh·ªã ƒë·ªãnh s·ªë 43/2017/Nƒê-CP c·ªßa Ch√≠nh ph·ªß v·ªÅ nh√£n h√†ng h√≥a, bao b√¨ th∆∞∆°ng ph·∫©m l√†..."
        },
        {
            "C√¢u h·ªèi": "Khi n√†o nh√† s·∫£n xu·∫•t, nh·∫≠p kh·∫©u s·∫£n ph·∫©m, bao b√¨ ph·∫£i th·ª±c hi·ªán tr√°ch nhi·ªám t√°i ch·∫ø?",
            "Tr·∫£ l·ªùi": "Theo kho·∫£n 4 ƒêi·ªÅu 77 Ngh·ªã ƒë·ªãnh s·ªë 08/2022/Nƒê-CP th√¨ nh√† s·∫£n xu·∫•t, nh·∫≠p kh·∫©u s·∫£n ph·∫©m ph·∫£i th·ª±c hi·ªán..."
        }
    ]
}

# ========== RECREATE COLLECTION FUNCTION ==========

def recreate_faq_collection(force=False):
    """
    Recreate FAQ collection with fresh embeddings

    Args:
        force: If True, delete existing collection and recreate
    """
    print("="*80)
    print("üîÑ FAQ COLLECTION SETUP")
    print("="*80)

    try:
        # Check if collection exists
        existing_collections = client.get_collections().collections
        collection_exists = any(col.name == collection_name for col in existing_collections)

        if collection_exists:
            if force:
                print(f"üóëÔ∏è  Deleting existing collection '{collection_name}'...")
                client.delete_collection(collection_name)
                print(f"‚úÖ Deleted old collection")
            else:
                print(f"‚úÖ Collection '{collection_name}' already exists")
                count = client.get_collection(collection_name).points_count
                print(f"   Points in collection: {count}")
                print("üí° Set force=True to recreate with fresh embeddings")
                return True

        # Create collection
        print(f"üìù Creating collection '{collection_name}'...")
        sample_emb = embeddings.embed_query("test")
        dim = len(sample_emb)

        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=dim, distance=Distance.COSINE)
        )
        print(f"‚úÖ Created collection (dimension: {dim})")

        # Add FAQ documents with fresh embeddings
        print(f"üìÑ Adding {len(faq['meta'])} FAQ documents...")
        points = []

        for idx, item in enumerate(faq["meta"], 1):
            question = item["C√¢u h·ªèi"]
            answer = item["Tr·∫£ l·ªùi"]

            print(f"   {idx}. Embedding: {question[:50]}...")
            vector = embeddings.embed_query(question)

            point = PointStruct(
                id=str(uuid.uuid4()),
                vector=vector,
                payload={
                    "C√¢u_h·ªèi": question,
                    "Tr·∫£_l·ªùi": answer
                }
            )
            points.append(point)

        # Upload all at once
        client.upsert(collection_name=collection_name, points=points)
        print(f"‚úÖ Added {len(points)} documents to collection")

        # Verify
        count = client.get_collection(collection_name).points_count
        print(f"‚úÖ Verified: Collection has {count} points")
        print("="*80)

        return True

    except Exception as e:
        print(f"‚ùå Error: {e}")
        print("="*80)
        return False

# ========== RETRIEVAL FUNCTION ==========

def retrieve_faq_top1(query: str, score_threshold: float = 0.6):
    """Retrieve top 1 FAQ with detailed scoring info"""
    print(f"\n{'='*80}")
    print(f"üîç FAQ RETRIEVAL")
    print(f"{'='*80}")
    print(f"Query: {query}")
    print(f"Threshold: {score_threshold}")
    print(f"{'-'*80}")

    # Get query embedding
    query_vector = embeddings.embed_query(query)

    # Search
    results = client.query_points(
        collection_name=collection_name,
        query=query_vector,
        limit=3  # Get top 3 to see scores
    )

    if not results or not results.points:
        print("  ‚ùå No results found")
        print(f"{'='*80}\n")
        return []

    # Show all top matches
    print(f"  üìä Top matches:")
    for i, point in enumerate(results.points, 1):
        score = point.score
        question = point.payload['C√¢u_h·ªèi']
        status = "‚úÖ PASS" if score >= score_threshold else "‚ùå FAIL"
        print(f"     {i}. {status} Score: {score:.4f} - {question[:50]}...")

    # Get best match
    best_point = results.points[0]
    best_score = best_point.score

    print(f"{'-'*80}")

    if best_score >= score_threshold:
        doc = Document(
            page_content=best_point.payload["Tr·∫£_l·ªùi"],
            metadata={
                "C√¢u_h·ªèi": best_point.payload["C√¢u_h·ªèi"],
                "score": best_score
            }
        )
        print(f"  ‚úÖ Returning match (score: {best_score:.4f} >= {score_threshold})")
        print(f"{'='*80}\n")
        return [doc]
    else:
        print(f"  ‚ö†Ô∏è  Best score {best_score:.4f} < threshold {score_threshold}")
        print(f"  üí° Try threshold={best_score:.2f} or lower")
        print(f"{'='*80}\n")
        return []

# ========== RUN SETUP ==========

print("üöÄ Initializing FAQ system...")
print()

# Only recreate if doesn't exist (force=False for faster loading)
recreate_faq_collection(force=False)  # Set to False to skip if already exists

print("‚úÖ FAQ system ready!")

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# ========== INITIALIZE LLM FOR ANSWER GENERATION ==========

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

print("‚úÖ LLM initialized for answer generation")

# ========== ANSWER GENERATION FUNCTION ==========

def generate_answer_from_faq(query: str, documents: list):
    """
    Generate answer based on retrieved FAQ documents

    Args:
        query: User's original question
        documents: List of Document objects from retrieve_faq_top1

    Returns:
        str: Generated answer
    """
    print(f"\n{'='*80}")
    print(f"üí¨ GENERATING ANSWER FROM FAQ")
    print(f"{'='*80}")
    print(f"Query: {query}")
    print(f"Documents: {len(documents)}")
    print(f"{'-'*80}")

    # If no documents, return default message
    if not documents:
        print("  ‚ö†Ô∏è  No FAQ documents found, returning default message")
        print(f"{'='*80}\n")
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong FAQ. B·∫°n c√≥ th·ªÉ h·ªèi c√¢u h·ªèi kh√°c ho·∫∑c cung c·∫•p th√™m chi ti·∫øt kh√¥ng?"

    # Get the FAQ document
    doc = documents[0]
    faq_question = doc.metadata.get("C√¢u_h·ªèi", "")
    faq_answer = doc.page_content

    print(f"  üìã FAQ matched: {faq_question[:60]}...")
    print(f"{'-'*80}")

    # Create prompt for answer generation
    prompt = ChatPromptTemplate.from_messages([
        ("system", """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t EPR Vi·ªát Nam.

Nhi·ªám v·ª• c·ªßa b·∫°n:
1. D·ª±a v√†o c√¢u h·ªèi FAQ v√† c√¢u tr·∫£ l·ªùi c√≥ s·∫µn
2. Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch t·ª± nhi√™n, th√¢n thi·ªán
3. Gi·ªØ nguy√™n th√¥ng tin ch√≠nh x√°c t·ª´ FAQ
4. C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh c√°ch di·ªÖn ƒë·∫°t cho ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng

Quy t·∫Øc:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
- Gi·ªØ th√¥ng tin ch√≠nh x√°c t·ª´ FAQ
- N·∫øu c√¢u h·ªèi ng∆∞·ªùi d√πng kh√°c m·ªôt ch√∫t so v·ªõi FAQ, h√£y ƒëi·ªÅu ch·ªânh c√¢u tr·∫£ l·ªùi cho ph√π h·ª£p
N·∫øu c√¢u h·ªèi KH√îNG li√™n quan (v√≠ d·ª•: n·∫•u ƒÉn, du l·ªãch, th·ªÉ thao, etc):"T√¥i ch·ªâ h·ªó tr·ª£ c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn lu·∫≠t EPR c·ªßa Vi·ªát Nam"
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng"""),

        ("""

C√¢u h·ªèi FAQ t∆∞∆°ng t·ª±: {faq_question}
C√¢u tr·∫£ l·ªùi FAQ: {faq_answer}

C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {user_question}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n th√¥ng tin FAQ tr√™n:""")
    ])

    # Generate answer
    chain = prompt | llm

    result = chain.invoke({
        "faq_question": faq_question,
        "faq_answer": faq_answer,
        "user_question": query
    })

    answer = result.content

    print(f"  ‚úÖ Answer generated")
    print(f"{'='*80}\n")

    return answer


# ========== COMPLETE FAQ RAG PIPELINE ==========

def faq_rag_pipeline(query: str, score_threshold: float = 0.6):
    """
    Complete FAQ RAG pipeline: Retrieve + Generate

    Args:
        query: User question
        score_threshold: Minimum similarity score for retrieval
        chat_history: Optional chat history

    Returns:
        dict: {
            "answer": str,
            "documents": list[Document],
            "source": str ("faq" or "not_found")
        }
    """
    print(f"\n{'#'*80}")
    print(f"ü§ñ FAQ RAG PIPELINE")
    print(f"{'#'*80}")
    print(f"Query: {query}")
    print(f"{'#'*80}\n")

    # Step 1: Retrieve FAQ documents
    documents = retrieve_faq_top1(query, score_threshold=score_threshold)

    # Step 2: Generate answer
    answer = generate_answer_from_faq(query, documents)

    # Step 3: Return result
    result = {
        "answer": answer,
        "documents": documents,
    }

    print(f"{'#'*80}")
    print(f"‚úÖ PIPELINE COMPLETE")
    print(f"{'#'*80}\n")

    return result



llm_rewrite_legal = ChatOpenAI(model="gpt-4o-mini", temperature=0)

rewrite_prompt_legal_improved = ChatPromptTemplate.from_messages([
    ("system", """B·∫°n l√† chuy√™n gia vi·∫øt l·∫°i c√¢u h·ªèi ph√°p lu·∫≠t.

**NHI·ªÜM V·ª§:**
1. N·∫øu c√¢u h·ªèi c√≥ ƒê·∫†I T·ª™ tham chi·∫øu (ƒë√≥, n√†y, n√≥) ‚Üí Thay th·∫ø b·∫±ng th√¥ng tin c·ª• th·ªÉ t·ª´ l·ªãch s·ª≠
2. N·∫øu c√¢u h·ªèi ƒê√É R√ï R√ÄNG (kh√¥ng c√≥ ƒë·∫°i t·ª´ m∆° h·ªì) ‚Üí GI·ªÆ NGUY√äN
3. N·∫øu c√¢u h·ªèi KH√îNG li√™n quan ƒë·∫øn ph√°p lu·∫≠t ‚Üí GI·ªÆ NGUY√äN

**C√ÅC D·∫†NG THAM CHI·∫æU C·∫¶N X·ª¨ L√ù:**
- "n√≥", "ƒë√≥", "n√†y", "ƒëi·ªÅu ƒë√≥", "lu·∫≠t ƒë√≥", "·ªü tr√™n", "v·ª´a r·ªìi", "ƒëi·ªÅu v·ª´a ƒë·ªÅ c·∫≠p" ‚Üí Thay b·∫±ng ƒêi·ªÅu/Lu·∫≠t/Ch∆∞∆°ng c·ª• th·ªÉ
- "c√°c ƒëi·ªÅu ·ªü tr√™n", "nh·ªØng ƒëi·ªÅu ƒë√£ n√≥i", "c√°c lu·∫≠t ·ªü tr√™n" ‚Üí Li·ªát k√™ c√°c ƒêi·ªÅu c·ª• th·ªÉ t·ª´ l·ªãch s·ª≠
- "t·ª´ c√°c ƒëi·ªÅu tr√™n", "d·ª±a v√†o c√°c ƒëi·ªÅu ƒë√£ n√≥i" ‚Üí X√°c ƒë·ªãnh c√°c ƒêi·ªÅu t·ª´ l·ªãch s·ª≠

**‚ö†Ô∏è C·ª∞C K·ª≤ QUAN TR·ªåNG:**
- CH·ªà thay th·∫ø ƒë·∫°i t·ª´, KH√îNG thay ƒë·ªïi s·ªë ƒëi·ªÅu c·ª• th·ªÉ
- N·∫øu c√¢u h·ªèi ƒë√£ c√≥ S·ªê ƒêI·ªÄU C·ª§ TH·ªÇ (v√≠ d·ª•: "ƒëi·ªÅu 2", "ƒêi·ªÅu 77") ‚Üí GI·ªÆ NGUY√äN HO√ÄN TO√ÄN
- TUY·ªÜT ƒê·ªêI KH√îNG thay ƒë·ªïi s·ªë ƒëi·ªÅu trong c√¢u h·ªèi g·ªëc
- KH√îNG th√™m t·ª´ kh√≥a t·ª´ l·ªãch s·ª≠ v√†o c√¢u h·ªèi ƒë√£ r√µ r√†ng

**QUY T·∫ÆC QUAN TR·ªåNG:**
‚úÖ CH·ªà thay th·∫ø khi c√≥ ƒë·∫°i t·ª´ m∆° h·ªì
‚úÖ KH√îNG th√™m ng·ªØ c·∫£nh v√†o c√¢u h·ªèi ƒë√£ r√µ r√†ng
‚úÖ KH√îNG th√™m "theo ƒêi·ªÅu X" v√†o c√¢u h·ªèi m·ªõi v·ªÅ ch·ªß ƒë·ªÅ kh√°c
‚úÖ ƒê·ªåC K·ª∏ l·ªãch s·ª≠ ƒë·ªÉ t√¨m ƒêi·ªÅu/Ch∆∞∆°ng/Lu·∫≠t ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn
‚úÖ CH·ªà tr·∫£ v·ªÅ c√¢u h·ªèi ng·∫Øn g·ªçn (10-20 t·ª´)
‚úÖ LU√îN gi·ªØ d·∫°ng c√¢u h·ªèi v·ªõi d·∫•u "?"

‚ùå TUY·ªÜT ƒê·ªêI KH√îNG tr·∫£ l·ªùi c√¢u h·ªèi
‚ùå TUY·ªÜT ƒê·ªêI KH√îNG gi·∫£i th√≠ch n·ªôi dung lu·∫≠t
‚ùå TUY·ªÜT ƒê·ªêI KH√îNG th√™m ng·ªØ c·∫£nh khi c√¢u h·ªèi ƒë√£ r√µ r√†ng

**PH√ÇN BI·ªÜT C√ÇU H·ªéI M·ªöI vs C√ÇU H·ªéI TI·∫æP THEO:**

C√¢u h·ªèi M·ªöI (ch·ªß ƒë·ªÅ kh√°c) ‚Üí GI·ªÆ NGUY√äN:
- "Ai ch·ªãu tr√°ch nhi·ªám t√°i ch·∫ø?" (ƒë√£ r√µ r√†ng, kh√¥ng c·∫ßn th√™m "theo ƒêi·ªÅu 7")
- "EPR l√† g√¨?" (c√¢u h·ªèi m·ªõi, ƒë·∫ßy ƒë·ªß)
- "Quy ƒë·ªãnh v·ªÅ bao b√¨?" (c√¢u h·ªèi m·ªõi)

C√¢u h·ªèi TI·∫æP THEO (c√≥ ƒë·∫°i t·ª´) ‚Üí THAY TH·∫æ:
- "ƒêi·ªÅu ƒë√≥ c√≥ n√≥i v·ªÅ X kh√¥ng?" ‚Üí "ƒêi·ªÅu 7 c√≥ n√≥i v·ªÅ X kh√¥ng?"
- "N√≥ quy ƒë·ªãnh g√¨?" ‚Üí "ƒêi·ªÅu 7 quy ƒë·ªãnh g√¨?"
- "C√°i n√†y li√™n quan g√¨?" ‚Üí "ƒêi·ªÅu 7 li√™n quan g√¨?"
"""),

    # Few-shot examples - Legal questions with pronouns (NEED TRANSFORMATION)
    ("human", """L·ªãch s·ª≠: User: Cho t√¥i bi·∫øt v·ªÅ ƒëi·ªÅu 1? Assistant: Theo ƒêi·ªÅu 1...
User: Cho t√¥i bi·∫øt v·ªÅ ƒëi·ªÅu 3? Assistant: Theo ƒêi·ªÅu 3...

C√¢u h·ªèi: T·ª´ c√°c ƒëi·ªÅu ·ªü tr√™n h√£y cho t√¥i bi·∫øt √°p d·ª•ng ƒë∆∞·ª£c g√¨ kh√¥ng?"""),
    ("assistant", "ƒêi·ªÅu 1 v√† ƒêi·ªÅu 3 c√≥ th·ªÉ √°p d·ª•ng ƒë∆∞·ª£c g√¨"),

    ("human", """L·ªãch s·ª≠: User: Cho t√¥i h·ªèi v·ªÅ ƒëi·ªÅu lu·∫≠t s·ªë 7? Assistant: Theo ƒêi·ªÅu 7...

C√¢u h·ªèi: ƒêi·ªÅu lu·∫≠t ƒë√≥ c√≥ n√≥i v·ªÅ kh√¥ng kh√≠ hay kh√¥ng?"""),
    ("assistant", "ƒêi·ªÅu 7 c√≥ n√≥i v·ªÅ kh√¥ng kh√≠ kh√¥ng"),

    # Few-shot examples - Clear legal questions (KEEP ORIGINAL)
    ("human", """L·ªãch s·ª≠: User: Cho t√¥i h·ªèi v·ªÅ ƒêi·ªÅu 7? Assistant: Theo ƒêi·ªÅu 7... n√≥i v·ªÅ qu·∫£n l√Ω kh√¥ng kh√≠

C√¢u h·ªèi: Ai ch·ªãu tr√°ch nhi·ªám t√°i ch·∫ø?"""),
    ("assistant", "Ai ch·ªãu tr√°ch nhi·ªám t√°i ch·∫ø?"),

    ("human", """L·ªãch s·ª≠: User: ƒêi·ªÅu 77 l√† g√¨? Assistant: ƒêi·ªÅu 77 v·ªÅ t√°i ch·∫ø...

C√¢u h·ªèi: Quy ƒë·ªãnh v·ªÅ bao b√¨ l√† g√¨?"""),
    ("assistant", "Quy ƒë·ªãnh v·ªÅ bao b√¨ l√† g√¨?"),

    # IMPORTANT: Questions with specific article numbers - NEVER CHANGE THEM
    ("human", """L·ªãch s·ª≠: User: Cho t√¥i h·ªèi v·ªÅ ƒêi·ªÅu 5? Assistant: Theo ƒêi·ªÅu 5...
User: ƒêi·ªÅu 6 quy ƒë·ªãnh g√¨? Assistant: Theo ƒêi·ªÅu 6...

C√¢u h·ªèi: Cho t√¥i h·ªèi chi ti·∫øt v·ªÅ ƒëi·ªÅu 2 v√† ƒëi·ªÅu 3?"""),
    ("assistant", "Cho t√¥i h·ªèi chi ti·∫øt v·ªÅ ƒëi·ªÅu 2 v√† ƒëi·ªÅu 3?"),

    ("human", """L·ªãch s·ª≠: User: ƒêi·ªÅu 10 l√† g√¨? Assistant: ƒêi·ªÅu 10 v·ªÅ...

C√¢u h·ªèi: ƒêi·ªÅu 1 quy ƒë·ªãnh g√¨?"""),
    ("assistant", "ƒêi·ªÅu 1 quy ƒë·ªãnh g√¨?"),

    ("human", """L·ªãch s·ª≠: (tr·ªëng)

C√¢u h·ªèi: Cho t√¥i h·ªèi v·ªÅ ƒëi·ªÅu lu·∫≠t s·ªë 1?"""),
    ("assistant", "ƒêi·ªÅu 1 quy ƒë·ªãnh g√¨"),

    # Few-shot examples - Non-legal questions (KEEP ORIGINAL)
    ("human", """L·ªãch s·ª≠: (tr·ªëng)

C√¢u h·ªèi: Xin ch√†o!"""),
    ("assistant", "Xin ch√†o!"),

    ("human", """L·ªãch s·ª≠: (tr·ªëng)

C√¢u h·ªèi: C·∫£m ∆°n b·∫°n"""),
    ("assistant", "C·∫£m ∆°n b·∫°n"),

    ("human", """L·ªãch s·ª≠: (tr·ªëng)

C√¢u h·ªèi: L√†m th·∫ø n√†o ƒë·ªÉ n·∫•u ph·ªü?"""),
    ("assistant", "L√†m th·∫ø n√†o ƒë·ªÉ n·∫•u ph·ªü?"),

    # Actual query
    ("human", """L·ªãch s·ª≠: {chat_history}

C√¢u h·ªèi: {question}

**H∆Ø·ªöNG D·∫™N PH√ÇN T√çCH:**
1. C√¢u h·ªèi c√≥ S·ªê ƒêI·ªÄU C·ª§ TH·ªÇ kh√¥ng? (ƒëi·ªÅu 1, ƒêi·ªÅu 77, ƒëi·ªÅu 2 v√† ƒëi·ªÅu 3)
   - C√ì S·ªê C·ª§ TH·ªÇ ‚Üí GI·ªÆ NGUY√äN HO√ÄN TO√ÄN (ƒë·ª´ng thay ƒë·ªïi s·ªë ƒëi·ªÅu!)
   - KH√îNG C√ì S·ªê ‚Üí Chuy·ªÉn sang b∆∞·ªõc 2

2. C√¢u h·ªèi c√≥ ch·ª©a ƒë·∫°i t·ª´ m∆° h·ªì kh√¥ng? (ƒë√≥, n√†y, n√≥, ·ªü tr√™n, v·ª´a r·ªìi)
   - C√ì ‚Üí Thay th·∫ø b·∫±ng th√¥ng tin t·ª´ l·ªãch s·ª≠
   - KH√îNG ‚Üí Chuy·ªÉn sang b∆∞·ªõc 3

3. C√¢u h·ªèi ƒë√£ ƒë·∫ßy ƒë·ªß v√† r√µ r√†ng ch∆∞a?
   - ƒê√É R√ï R√ÄNG ‚Üí GI·ªÆ NGUY√äN (kh√¥ng th√™m g√¨)
   - CH∆ØA R√ï ‚Üí L√†m r√µ t·ª´ l·ªãch s·ª≠

**L∆ØU √ù:**
- ‚ö†Ô∏è TUY·ªÜT ƒê·ªêI GI·ªÆ NGUY√äN s·ªë ƒëi·ªÅu trong c√¢u h·ªèi g·ªëc (ƒëi·ªÅu 2 ph·∫£i v·∫´n l√† ƒëi·ªÅu 2, KH√îNG thay th√†nh s·ªë kh√°c!)
- N·∫øu c√≥ "c√°c ƒëi·ªÅu ·ªü tr√™n", "nh·ªØng ƒëi·ªÅu ƒë√£ n√≥i" ‚Üí T√åM T·∫§T C·∫¢ ƒêi·ªÅu trong l·ªãch s·ª≠ v√† li·ªát k√™
- N·∫øu c√≥ "ƒëi·ªÅu ƒë√≥", "n√≥" ‚Üí T√åM ƒêi·ªÅu G·∫¶N NH·∫§T trong l·ªãch s·ª≠
- LU√îN LU√îN gi·ªØ d·∫°ng c√¢u h·ªèi v·ªõi d·∫•u "?"
- TUY·ªÜT ƒê·ªêI KH√îNG th√™m "theo ƒêi·ªÅu X" v√†o c√¢u h·ªèi ƒë√£ r√µ r√†ng
- N·∫øu c√¢u h·ªèi KH√îNG li√™n quan ph√°p lu·∫≠t ‚Üí GI·ªÆ NGUY√äN

**V√ç D·ª§ QUAN TR·ªåNG:**

‚ùå SAI:
L·ªãch s·ª≠: "User: c√≥ ƒëi·ªÅu n√†o v·ªÅ t√°i ch·∫ø?\nAssistant: ƒêi·ªÅu 3 v·ªÅ t√°i ch·∫ø..."
C√¢u g·ªëc: "n√≥i r√µ c√°c ƒëi·ªÅu ƒë√≥ ra"
Chuy·ªÉn th√†nh: "N√≥i r√µ ƒêi·ªÅu 3 v·ªÅ t√°i ch·∫ø ra?"  ‚ùå TH√äM "v·ªÅ t√°i ch·∫ø" kh√¥ng c·∫ßn thi·∫øt!

‚úÖ ƒê√öNG:
L·ªãch s·ª≠: "User: c√≥ ƒëi·ªÅu n√†o v·ªÅ t√°i ch·∫ø?\nAssistant: ƒêi·ªÅu 3 v·ªÅ t√°i ch·∫ø..."
C√¢u g·ªëc: "n√≥i r√µ c√°c ƒëi·ªÅu ƒë√≥ ra"
Chuy·ªÉn th√†nh: "N√≥i r√µ ƒêi·ªÅu 3 ra?"  ‚úÖ CH·ªà thay "ƒëi·ªÅu ƒë√≥" ‚Üí "ƒêi·ªÅu 3"

‚ùå SAI:
L·ªãch s·ª≠: "User: ƒêi·ªÅu 77 l√† g√¨?\nAssistant: ƒêi·ªÅu 77 v·ªÅ tr√°ch nhi·ªám..."
C√¢u g·ªëc: "ƒêi·ªÅu ƒë√≥ c√≥ n√≥i v·ªÅ bao b√¨ kh√¥ng?"
Chuy·ªÉn th√†nh: "ƒêi·ªÅu 77 c√≥ n√≥i v·ªÅ bao b√¨ v√† tr√°ch nhi·ªám kh√¥ng?"  ‚ùå TH√äM "tr√°ch nhi·ªám"!

‚úÖ ƒê√öNG:
L·ªãch s·ª≠: "User: ƒêi·ªÅu 77 l√† g√¨?\nAssistant: ƒêi·ªÅu 77 v·ªÅ tr√°ch nhi·ªám..."
C√¢u g·ªëc: "ƒêi·ªÅu ƒë√≥ c√≥ n√≥i v·ªÅ bao b√¨ kh√¥ng?"
Chuy·ªÉn th√†nh: "ƒêi·ªÅu 77 c√≥ n√≥i v·ªÅ bao b√¨ kh√¥ng?"  ‚úÖ CH·ªà thay "ƒë√≥" ‚Üí "77"

‚ùå SAI - THAY ƒê·ªîI S·ªê ƒêI·ªÄU:
L·ªãch s·ª≠: "User: ƒêi·ªÅu 5 l√† g√¨?\nAssistant: ƒêi·ªÅu 5...\nUser: ƒêi·ªÅu 6?\nAssistant: ƒêi·ªÅu 6..."
C√¢u g·ªëc: "Cho t√¥i h·ªèi chi ti·∫øt v·ªÅ ƒëi·ªÅu 2 v√† ƒëi·ªÅu 3?"
Chuy·ªÉn th√†nh: "Cho t√¥i h·ªèi chi ti·∫øt v·ªÅ ƒêi·ªÅu 6 v√† ƒêi·ªÅu 7?"  ‚ùå SAI! ƒê√£ thay ƒë·ªïi s·ªë ƒëi·ªÅu!

‚úÖ ƒê√öNG - GI·ªÆ NGUY√äN S·ªê ƒêI·ªÄU:
L·ªãch s·ª≠: "User: ƒêi·ªÅu 5 l√† g√¨?\nAssistant: ƒêi·ªÅu 5...\nUser: ƒêi·ªÅu 6?\nAssistant: ƒêi·ªÅu 6..."
C√¢u g·ªëc: "Cho t√¥i h·ªèi chi ti·∫øt v·ªÅ ƒëi·ªÅu 2 v√† ƒëi·ªÅu 3?"
Chuy·ªÉn th√†nh: "Cho t√¥i h·ªèi chi ti·∫øt v·ªÅ ƒëi·ªÅu 2 v√† ƒëi·ªÅu 3?"  ‚úÖ ƒê√öNG! Gi·ªØ nguy√™n s·ªë ƒëi·ªÅu g·ªëc!

C√¢u h·ªèi vi·∫øt l·∫°i (CH·ªà c√¢u h·ªèi ng·∫Øn, ho·∫∑c gi·ªØ nguy√™n n·∫øu ƒë√£ r√µ):"""),
])

question_rewriter_legal = rewrite_prompt_legal_improved | llm_rewrite_legal | StrOutputParser()

print("‚úÖ Question rewriter v·ªõi x·ª≠ l√Ω reference context v√† ngƒÉn over-adding")


def transform_query(state):
    print("---CHUY·ªÇN H√ìA C√ÇU H·ªéI---")

    question = state.get("question", "")
    documents = state.get("documents", [])
    chat_history = state.get("chat_history", "")

    # ‚úÖ L∆∞u c√¢u h·ªèi g·ªëc n·∫øu ch∆∞a c√≥
    original_question = state.get("original_question", question)

    print(f"  C√¢u h·ªèi g·ªëc: {question}")

    better_question = question_rewriter_legal.invoke({
        "question": question,
        "chat_history": chat_history
    })

    print(f"  C√¢u h·ªèi ƒë√£ chuy·ªÉn h√≥a: {better_question}")

    retries = state.get("retries", 0) + 1
    return {
        "question": better_question,
        "original_question": original_question,  # ‚úÖ L∆∞u c√¢u h·ªèi g·ªëc
        "documents": documents,
        "chat_history": chat_history,
        "generation": state.get("generation", ""),
        "retries": retries,
    }

print("‚úì H√†m transform_query s·∫µn s√†ng")


from langchain.memory import ConversationBufferMemory

# Create conversation memory
conversation_memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    input_key="input",
    output_key="generation"
)

def chitchat(state):
    """Tr√≤ chuy·ªán th√¢n thi·ªán v·ªõi tr·ª£ l√Ω ph√°p lu·∫≠t, c√≥ truy c·∫≠p ƒë·∫ßy ƒë·ªß l·ªãch s·ª≠"""
    print("---TR√í CHUY·ªÜN PH√ÅP LU·∫¨T TH√ÇN THI·ªÜN---")

    question = state["question"]
    chat_history = state.get("chat_history", "")

    # N·∫øu chat_history qu√° ng·∫Øn, load t·ª´ memory
    if not chat_history or len(chat_history) < 200:
        try:
            memory_vars = conversation_memory.load_memory_variables({})
            if "chat_history" in memory_vars:
                messages = memory_vars["chat_history"]
                if messages:
                    formatted = []
                    for msg in messages:
                        if hasattr(msg, 'type'):
                            role = "Ng∆∞·ªùi d√πng" if msg.type == "human" else "Tr·ª£ l√Ω ph√°p lu·∫≠t EPR"
                            content = msg.content
                        else:
                            role = "Ng∆∞·ªùi d√πng"
                            content = str(msg)
                        formatted.append(f"{role}: {content}")
                    chat_history = "\n".join(formatted)
        except Exception as e:
            print(f"  ‚ö†Ô∏è Kh√¥ng th·ªÉ load full history: {e}")

    print(f"  ƒê·ªô d√†i l·ªãch s·ª≠: {len(chat_history)} k√Ω t·ª±")

    llm_chat = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

    chitchat_prompt = ChatPromptTemplate.from_messages([
        ("system", """B·∫°n l√† **tr·ª£ l√Ω ph√°p l√Ω th√¥ng minh** h·ªó tr·ª£ ng∆∞·ªùi d√πng tra c·ª©u v√† gi·∫£i th√≠ch vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam.

**QUY T·∫ÆC QUAN TR·ªåNG V·ªÄ B·ªò NH·ªö:**
1. **LU√îN ƒê·ªåC K·ª∏ l·ªãch s·ª≠ h·ªôi tho·∫°i** tr∆∞·ªõc khi tr·∫£ l·ªùi
2. **S·ª¨ D·ª§NG th√¥ng tin** m√† ng∆∞·ªùi d√πng ƒë√£ cung c·∫•p trong l·ªãch s·ª≠ (t√™n, c√¥ng ty, ho√†n c·∫£nh, etc.)
3. **GHI NH·ªö context** t·ª´ c√°c c√¢u h·ªèi v√† tr·∫£ l·ªùi tr∆∞·ªõc ƒë√≥
4. N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ th√¥ng tin h·ªç ƒë√£ cung c·∫•p ‚Üí **TR·∫¢ L·ªúI d·ª±a tr√™n l·ªãch s·ª≠**, KH√îNG n√≥i "kh√¥ng bi·∫øt"

**V√ç D·ª§:**
- N·∫øu l·ªãch s·ª≠ c√≥: "User: T√¥i t√™n l√† Danh Thu·∫≠n"
  ‚Üí Khi user h·ªèi "T√™n t√¥i l√† g√¨?" ‚Üí Tr·∫£ l·ªùi: "T√™n c·ªßa b·∫°n l√† Danh Thu·∫≠n"

- N·∫øu l·ªãch s·ª≠ c√≥: "User: T√¥i l√†m vi·ªác t·∫°i c√¥ng ty ABC"
  ‚Üí Khi user h·ªèi "T√¥i l√†m ·ªü ƒë√¢u?" ‚Üí Tr·∫£ l·ªùi: "B·∫°n l√†m vi·ªác t·∫°i c√¥ng ty ABC"

**H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI:**
- Gi·∫£i th√≠ch lu·∫≠t m·ªôt c√°ch r√µ r√†ng, trung l·∫≠p, d·ªÖ hi·ªÉu
- N·∫øu c√¢u tr·∫£ l·ªùi d·ª±a tr√™n vƒÉn b·∫£n ph√°p lu·∫≠t ‚Üí n√™u r√µ t√™n vƒÉn b·∫£n v√† ƒêi·ªÅu/M·ª•c/Ch∆∞∆°ng
- N·∫øu th√¥ng tin t·ª´ web ‚Üí n√≥i r√µ l√† tham kh·∫£o
- Gi·ªØ gi·ªçng ƒëi·ªáu th√¢n thi·ªán, chuy√™n nghi·ªáp

üìã L·ªãch s·ª≠ h·ªôi tho·∫°i (ƒê·ªåC K·ª∏):
{chat_history}"""),
        ("human", "{question}"),
    ])

    chitchat_chain = chitchat_prompt | llm_chat | StrOutputParser()

    generation = chitchat_chain.invoke({
        "question": question,
        "chat_history": chat_history if chat_history else "(kh√¥ng c√≥ h·ªôi tho·∫°i tr∆∞·ªõc)"
    })

    state["generation"] = generation
    state["history"] = chat_history

    return {
        "question": question,
        "documents": [],
        "chat_history": chat_history,
        "generation": generation,
        "retries": state.get("retries", 0)
    }

print("‚úì H√†m chitchat v·ªõi memory emphasis")

from typing import Literal
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
class FaqRouteQuery(BaseModel):
    """Ph√¢n lo·∫°i c√¢u h·ªèi ng∆∞·ªùi d√πng t·ªõi FAQ, web search ho·∫∑c chitchat"""
    datasource: Literal["vectorstore_faq", "chitchat"] = Field(
        ...,
        description=(
            "vectorstore_faq (FAQ), "
            "chitchat (giao ti·∫øp th√¢n thi·ªán)"
        )
    )

# ========== KH·ªûI T·∫†O LLM ROUTER ==========
llm_router_faq = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
structured_llm_router_faq = llm_router_faq.with_structured_output(FaqRouteQuery)

# ========== SYSTEM PROMPT ==========
router_system_faq = """B·∫°n l√† chuy√™n gia ph√¢n lo·∫°i c√¢u h·ªèi ng∆∞·ªùi d√πng t·ªõi ngu·ªìn d·ªØ li·ªáu ph√π h·ª£p.

B·∫°n c√≥ quy·ªÅn truy c·∫≠p c√°c ngu·ªìn:
1. **vectorstore_faq** - FAQ ph√°p lu·∫≠t ƒë√£ ƒë∆∞·ª£c bi√™n so·∫°n
2. **chitchat** - Giao ti·∫øp th√¢n thi·ªán, h·ªèi thƒÉm, c·∫£m ∆°n, ch√†o h·ªèi

Quy t·∫Øc ∆∞u ti√™n:
- N·∫øu c√¢u h·ªèi mang t√≠nh ch√†o h·ªèi,tr√≤ chuy·ªán, c·∫£m ∆°n, gi·ªõi thi·ªáu b·∫£n th√¢n ‚Üí **chitchat**
- N·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn n·ªôi dung ph√°p lu·∫≠t EPR ‚Üí **vectorstore_faq**

C√¢u h·ªèi hi·ªán t·∫°i: {question}"""

# ========== T·∫†O PROMPT ==========
route_prompt_faq = ChatPromptTemplate.from_messages([
    ("system", router_system_faq),
    ("human", "{question}")
])

# ========== COMBINE PROMPT V·ªöI STRUCTURED LLM ==========
question_router_faq = route_prompt_faq | structured_llm_router_faq

print("‚úì FAQ question router created successfully!")

def route_question_faq(state):
    """Route c√¢u h·ªèi ban ƒë·∫ßu v√† l∆∞u snapshot c·ªßa chat_history"""
    print("---PH√ÇN LU·ªíNG C√ÇU H·ªéI (V·ªöI NG·ªÆ C·∫¢NH)---")

    question = state["question"]
    chat_history = get_full_chat_history()  # Load from memory

    # ‚úÖ L∆∞u c√¢u h·ªèi g·ªëc
    if "original_question" not in state or not state.get("original_question"):
        print(f"  üíæ L∆∞u c√¢u h·ªèi g·ªëc: {question}")
        state["original_question"] = question

    # ‚úÖ L∆∞u snapshot c·ªßa chat_history TR∆Ø·ªöC KHI v√†o FAQ path
    if "original_chat_history" not in state or not state.get("original_chat_history"):
        print(f"  üíæ L∆∞u snapshot chat_history ({len(chat_history)} k√Ω t·ª±)")
        state["original_chat_history"] = chat_history

    print(f"L·ªãch s·ª≠ h·ªôi tho·∫°i:\n{chat_history}\n")
    print(f"C√¢u h·ªèi hi·ªán t·∫°i: {question}")

    # G·ªçi LLM router
    source = question_router_faq.invoke({
        "question": question,
        "chat_history": chat_history
    })

    datasource = source.get("datasource") if isinstance(source, dict) else getattr(source, "datasource", None)

    print(f"---PH√ÇN LU·ªíNG T·ªöI: {datasource.upper() if datasource else 'UNKNOWN'}---")

    if datasource == 'vectorstore_faq':
        return "vectorstore_faq"
    elif datasource == 'chitchat':
        return "chitchat"


print("‚úÖ route_question_faq v·ªõi chat_history snapshot")


from typing import List, TypedDict
from langgraph.graph import StateGraph, END


print("‚úì State defined")

# ========== NODE FUNCTIONS ==========

def retrieve_faq_node(state):
    """Retrieve FAQ documents"""
    print("\n" + "="*80)
    print("üìö RETRIEVE FAQ")
    print("="*80)

    question = state["question"]
    print(f"  Question: {question}")

    # Use your existing retrieve_faq_top1 function
    documents = retrieve_faq_top1(question, score_threshold=0.6)

    print(f"  Documents found: {len(documents)}")
    print("="*80 + "\n")

    state["documents"] = documents
    return state




def generate_faq_node(state):
    """Generate answer from FAQ documents"""
    print("\n" + "="*80)
    print("üí¨ GENERATE FAQ ANSWER")
    print("="*80)

    question = state["question"]
    documents = state["documents"]

    if not documents:
        print("  ‚ö†Ô∏è  No documents")
        state["generation"] = "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong FAQ."
        return state

    doc = documents[0]
    faq_question = doc.metadata.get("C√¢u_h·ªèi", "")
    faq_answer = doc.page_content

    print(f"  FAQ: {faq_question[:60]}...")

    prompt = ChatPromptTemplate.from_messages([
        ("system", """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t EPR.
D·ª±a v√†o FAQ ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi."""),
        ("human", """FAQ: {faq_question}
Tr·∫£ l·ªùi: {faq_answer}

C√¢u h·ªèi: {user_question}

Tr·∫£ l·ªùi:""")
    ])

    chain = prompt | llm | StrOutputParser()

    generation = chain.invoke({
        "faq_question": faq_question,
        "faq_answer": faq_answer,
        "user_question": question
    })

    print(f"  Answer: {generation[:80]}...")
    print("="*80 + "\n")

    state["generation"] = generation
    return state


def new_round_router(state):
    """
    Reset state and restore chat_history from snapshot
    """
    print("\n" + "="*80)
    print("üîÅ NEW ROUND: RESETTING STATE")
    print("="*80)

    # ‚úÖ Restore chat_history from snapshot
    original_chat_history = state.get("original_chat_history", "")
    current_chat_history = state.get("chat_history", "")

    # Prefer original snapshot
    chat_history_to_use = original_chat_history if original_chat_history else current_chat_history

    # Restore original question
    original_question = state.get("original_question", state.get("question", ""))

    print(f"  üìå Restoring original question: {original_question}")

    if original_chat_history:
        print(f"  üí¨ Restoring chat history from snapshot ({len(original_chat_history)} chars)")
        print(f"     (Ignoring modified chat history from FAQ path)")
    elif current_chat_history:
        print(f"  üí¨ Using current chat history ({len(current_chat_history)} chars)")
    else:
        print(f"  ‚ö†Ô∏è  No chat history")

    print("="*80 + "\n")

    return {
        **state,
        "question": original_question,
        "original_question": original_question,
        "chat_history": chat_history_to_use,  # ‚úÖ Use clean snapshot
        "original_chat_history": original_chat_history,  # ‚úÖ Keep snapshot
        "retries": 0,
        "generation_retries": 0,
        "documents": [],
        "generation": "",
    }

print("‚úÖ new_round_router ready")



# ========== DECISION FUNCTIONS ==========

def decide_after_retrieve_faq(state):
    """
    Decision function after retrieve_faq_node

    Check if documents were retrieved:
    - If yes (has docs) ‚Üí go to "generate_faq"
    - If no (no docs) ‚Üí go to "new_round_router"

    Returns:
        str: "generate_faq" or "new_round_router"
    """
    documents = state.get("documents", [])

    print(f"\nüîÄ DECISION AFTER RETRIEVE FAQ")
    print(f"   Documents: {len(documents)}")

    if documents:
        print(f"   ‚û°Ô∏è  HAS DOCS ‚Üí generate_faq")
        return "generate_faq"
    else:
        print(f"   ‚û°Ô∏è  NO DOCS ‚Üí new_round_router")
        return "new_round_router"






data={"meta":[
{
  "ƒêi·ªÅu": "ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh",
  "Ch∆∞∆°ng": "Ch∆∞∆°ng I. NH·ªÆNG QUY ƒê·ªäNH CHUNG",
  "M·ª•c": "",
  "Pages": "2",
  "Text": "Ngh·ªã ƒë·ªãnh n√†y quy ƒë·ªãnh chi ti·∫øt kho·∫£n 4 ƒêi·ªÅu 9; kho·∫£n 5 ƒêi·ªÅu 13; kho·∫£n 4 ƒêi·ªÅu 14; kho·∫£n 4 ƒêi·ªÅu 15; kho·∫£n 3 ƒêi·ªÅu 20; kho·∫£n 4 ƒêi·ªÅu 21; kho·∫£n 4 ƒêi·ªÅu 23; kho·∫£n 2 ƒêi·ªÅu 24; kho·∫£n 3 ƒêi·ªÅu 25; kho·∫£n 7 ƒêi·ªÅu 28; kho·∫£n 7 ƒêi·ªÅu 33; kho·∫£n 7 ƒêi·ªÅu 37; kho·∫£n 6 ƒêi·ªÅu 43; kho·∫£n 6 ƒêi·ªÅu 44; kho·∫£n 5 ƒêi·ªÅu 46; kho·∫£n 8 ƒêi·ªÅu 49; kho·∫£n 6 ƒêi·ªÅu 51; kho·∫£n 4 ƒêi·ªÅu 52; kho·∫£n 4 ƒêi·ªÅu 53; kho·∫£n 5 ƒêi·ªÅu 54; kho·∫£n 5 ƒêi·ªÅu 55; kho·∫£n 7 ƒêi·ªÅu 56; kho·∫£n 3 ƒêi·ªÅu 59; kho·∫£n 5 ƒêi·ªÅu 61; kho·∫£n 1 ƒêi·ªÅu 63; kho·∫£n 7 ƒêi·ªÅu 65; kho·∫£n 7 ƒêi·ªÅu 67; ƒëi·ªÉm d kho·∫£n 2 ƒêi·ªÅu 69; kho·∫£n 2 ƒêi·ªÅu 70; kho·∫£n 3 ƒêi·ªÅu 71; kho·∫£n 8 ƒêi·ªÅu 72; kho·∫£n 7 ƒêi·ªÅu 73; kho·∫£n 4 ƒêi·ªÅu 78; kho·∫£n 3, kho·∫£n 4 ƒêi·ªÅu 79; kho·∫£n 3 ƒêi·ªÅu 80; kho·∫£n 5 ƒêi·ªÅu 85; kho·∫£n 1 ƒêi·ªÅu 86; kho·∫£n 1 ƒêi·ªÅu 105; kho·∫£n 4 ƒêi·ªÅu 110; kho·∫£n 7 ƒêi·ªÅu 111; kho·∫£n 7 ƒêi·ªÅu 112; kho·∫£n 4 ƒêi·ªÅu 114; kho·∫£n 3 ƒêi·ªÅu 115; ƒëi·ªÉm a kho·∫£n 2 ƒêi·ªÅu 116; kho·∫£n 7 ƒêi·ªÅu 121; kho·∫£n 4 ƒêi·ªÅu 131; kho·∫£n 4 ƒêi·ªÅu 132; kho·∫£n 4 ƒêi·ªÅu 135; kho·∫£n 5 ƒêi·ªÅu 137; kho·∫£n 5 ƒêi·ªÅu 138; kho·∫£n 2 ƒêi·ªÅu 140; kho·∫£n 5 ƒêi·ªÅu 141; kho·∫£n 4 ƒêi·ªÅu 142; kho·∫£n 3 ƒêi·ªÅu 143; kho·∫£n 5 ƒêi·ªÅu 144; kho·∫£n 4 ƒêi·ªÅu 145; kho·∫£n 2 ƒêi·ªÅu 146; kho·∫£n 7 ƒêi·ªÅu 148; kho·∫£n 5 ƒêi·ªÅu 149; kho·∫£n 5 ƒêi·ªÅu 150; kho·∫£n 3 ƒêi·ªÅu 151; kho·∫£n 4 ƒêi·ªÅu 158; kho·∫£n 6 ƒêi·ªÅu 160; kho·∫£n 4 ƒêi·ªÅu 167; kho·∫£n 6 ƒêi·ªÅu 171 Lu·∫≠t B·∫£o v·ªá m√¥i tr∆∞·ªùng v·ªÅ b·∫£o v·ªá c√°c th√†nh ph·∫ßn m√¥i tr∆∞·ªùng; ph√¢n v√πng m√¥i tr∆∞·ªùng, ƒë√°nh gi√° m√¥i tr∆∞·ªùng chi·∫øn l∆∞·ª£c, ƒë√°nh gi√° t√°c ƒë·ªông m√¥i tr∆∞·ªùng; gi·∫•y ph√©p m√¥i tr∆∞·ªùng, ƒëƒÉng k√Ω m√¥i tr∆∞·ªùng; b·∫£o v·ªá m√¥i tr∆∞·ªùng trong ho·∫°t ƒë·ªông s·∫£n xu·∫•t, kinh doanh, d·ªãch v·ª•, ƒë√¥ th·ªã, n√¥ng th√¥n v√† m·ªôt s·ªë lƒ©nh v·ª±c; qu·∫£n l√Ω ch·∫•t th·∫£i; tr√°ch nhi·ªám t√†i ch·∫ø, x·ª≠ l√Ω s·∫£n ph·∫©m, bao b√¨ c·ªßa t·ªï ch·ª©c, c√° nh√¢n s·∫£n xu·∫•t, nh·∫≠p kh·∫©u; quan tr·∫Øc m√¥i tr∆∞·ªùng; h·ªá th·ªëng th√¥ng tin, c∆° s·ªü d·ªØ li·ªáu v·ªÅ m√¥i tr∆∞·ªùng; ph√≤ng ng·ª´a, ·ª©ng ph√≥ s·ª± c·ªë m√¥i tr∆∞·ªùng, b·ªìi th∆∞·ªùng thi·ªát h·∫°i v·ªÅ m√¥i tr∆∞·ªùng; c√¥ng c·ª• kinh t·∫ø v√† ngu·ªìn l·ª±c b·∫£o v·ªá m√¥i tr∆∞·ªùng; qu·∫£n l√Ω nh√† n∆∞·ªõc, ki·ªÉm tra, thanh tra v√† cung c·∫•p d·ªãch v·ª• c√¥ng tr·ª±c tuy·∫øn v·ªÅ b·∫£o v·ªá m√¥i tr∆∞·ªùng."
},
{
  "ƒêi·ªÅu": "ƒêi·ªÅu 2. ƒê·ªëi t∆∞·ª£ng √°p d·ª•ng",
  "Ch∆∞∆°ng": "Ch∆∞∆°ng I. NH·ªÆNG QUY ƒê·ªäNH CHUNG",
  "M·ª•c": "",
  "Pages": "2",
  "Text": "Ngh·ªã ƒë·ªãnh n√†y √°p d·ª•ng ƒë·ªëi v·ªõi c∆° quan, t·ªï ch·ª©c, c·ªông ƒë·ªìng d√¢n c∆∞, h·ªô gia ƒë√¨nh v√† c√° nh√¢n c√≥ ho·∫°t ƒë·ªông li√™n quan ƒë·∫øn c√°c n·ªôi dung quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 1 Ngh·ªã ƒë·ªãnh n√†y tr√™n l√£nh th·ªï n∆∞·ªõc C·ªông h√≤a x√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam, bao g·ªìm ƒë·∫•t li·ªÅn, h·∫£i ƒë·∫£o, v√πng bi·ªÉn, l√≤ng ƒë·∫•t v√† v√πng tr·ªùi."
},
{
  "ƒêi·ªÅu": "ƒêi·ªÅu 3. Gi·∫£i th√≠ch t·ª´ ng·ªØ",
  "Ch∆∞∆°ng": "Ch∆∞∆°ng I. NH·ªÆNG QUY ƒê·ªäNH CHUNG",
  "M·ª•c": "",
  "Pages": "2,3,4,5,6",
  "Text": """Trong Ngh·ªã ƒë·ªãnh n√†y, c√°c t·ª´ ng·ªØ d∆∞·ªõi ƒë√¢y ƒë∆∞·ª£c hi·ªÉu nh∆∞ sau:
  1. H·ªá th·ªëng thu gom, tho√°t n∆∞·ªõc m∆∞a c·ªßa c∆° s·ªü s·∫£n xu·∫•t, kinh doanh, d·ªãch v·ª• g·ªìm m·∫°ng l∆∞·ªõi thu gom, tho√°t n∆∞·ªõc (ƒë∆∞·ªùng ·ªëng, h·ªë ga, c·ªëng, k√™nh, m∆∞∆°ng, h·ªì ƒëi·ªÅu h√≤a), c√°c tr·∫°m b∆°m tho√°t n∆∞·ªõc m∆∞a v√† c√°c c√¥ng tr√¨nh ph·ª• tr·ª£ kh√°c nh·∫±m m·ª•c ƒë√≠ch thu gom, chuy·ªÉn t·∫£i, ti√™u tho√°t n∆∞·ªõc m∆∞a, ch·ªëng ng·∫≠p √∫ng.
  2. H·ªá th·ªëng thu gom, x·ª≠ l√Ω, tho√°t n∆∞·ªõc th·∫£i c·ªßa c∆° s·ªü s·∫£n xu·∫•t, kinh doanh, d·ªãch v·ª• g·ªìm m·∫°ng l∆∞·ªõi thu gom n∆∞·ªõc th·∫£i (ƒë∆∞·ªùng ·ªëng, h·ªë ga, c·ªëng),c√°c tr·∫°m b∆°m n∆∞·ªõc th·∫£i, c√°c c√¥ng tr√¨nh x·ª≠ l√Ω n∆∞·ªõc th·∫£i v√† c√°c c√¥ng tr√¨nh ph·ª• tr·ª£ nh·∫±m m·ª•c ƒë√≠ch thu gom,x·ª≠ l√Ω n∆∞·ªõc th·∫£i v√† tho√°t n∆∞·ªõc th·∫£i sau x·ª≠ l√Ω v√†o m√¥i tr∆∞·ªùng ti·∫øp nh·∫≠n.
  3. C√¥ng tr√¨nh, thi·∫øt b·ªã x·ª≠ l√Ω ch·∫•t th·∫£i t·∫°i ch·ªó l√† c√°c c√¥ng tr√¨nh, thi·∫øt b·ªã ƒë∆∞·ª£c s·∫£n xu·∫•t, l·∫Øp r√°p s·∫µn ho·∫∑c ƒë∆∞·ª£c x√¢y d·ª±ng t·∫°i ch·ªó ƒë·ªÉ x·ª≠ l√Ω n∆∞·ªõc th·∫£i, kh√≠ th·∫£i c·ªßa c∆° s·ªü s·∫£n xu·∫•t, kinh doanh, d·ªãch v·ª• quy m√¥ h·ªô gia ƒë√¨nh; c√¥ng vi√™n,
      khu vui ch∆°i, gi·∫£i tr√≠, khu kinh doanh, d·ªãch v·ª• t·∫≠p trung, ch·ª£, nh√† ga, b·∫øn xe, b·∫øn t√†u, b·∫øn c·∫£ng, b·∫øn ph√† v√† khu v·ª±c c√¥ng c·ªông kh√°c; h·ªô gia ƒë√¨nh, c√° nh√¢n c√≥ ph√°t sinh n∆∞·ªõc th·∫£i, kh√≠ th·∫£i ph·∫£i x·ª≠ l√Ω theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t v·ªÅ b·∫£o v·ªá m√¥i tr∆∞·ªùng.
  4. N∆∞·ªõc trao ƒë·ªïi nhi·ªát l√† n∆∞·ªõc ph·ª•c v·ª• m·ª•c ƒë√≠ch gi·∫£i nhi·ªát (n∆∞·ªõc l√†m m√°t) ho·∫∑c gia nhi·ªát cho thi·∫øt b·ªã, m√°y m√≥c trong qu√° tr√¨nh s·∫£n xu·∫•t, kh√¥ng ti·∫øp x√∫c tr·ª±c ti·∫øp v·ªõi nguy√™n li·ªáu, v·∫≠t li·ªáu, nhi√™n li·ªáu, h√≥a ch·∫•t s·ª≠ d·ª•ng trong c√°c c√¥ng ƒëo·∫°n s·∫£n xu·∫•t.
  5. T·ª± x·ª≠ l√Ω ch·∫•t th·∫£i l√† ho·∫°t ƒë·ªông x·ª≠ l√Ω ch·∫•t th·∫£i do ch·ªß ngu·ªìn th·∫£i th·ª±c hi·ªán trong khu√¥n vi√™n c∆° s·ªü ph√°t sinh ch·∫•t th·∫£i b·∫±ng c√°c h·∫°ng m·ª•c,
  d√¢y chuy·ªÅn s·∫£n xu·∫•t ho·∫∑c c√¥ng tr√¨nh b·∫£o v·ªá m√¥i tr∆∞·ªùng ƒë√°p ·ª©ng y√™u c·∫ßu v·ªÅ b·∫£o v·ªá m√¥i tr∆∞·ªùng.
  6. T√°i s·ª≠ d·ª•ng ch·∫•t th·∫£i l√† vi·ªác s·ª≠ d·ª•ng l·∫°i ch·∫•t th·∫£i m·ªôt c√°ch tr·ª±c ti·∫øp ho·∫∑c s·ª≠ d·ª•ng sau khi ƒë√£ s∆° ch·∫ø. S∆° ch·∫ø ch·∫•t th·∫£i l√† vi·ªác s·ª≠ d·ª•ng c√°c bi·ªán ph√°p
  k·ªπ thu·∫≠t c∆° - l√Ω ƒë∆°n thu·∫ßn nh·∫±m thay ƒë·ªïi t√≠nh ch·∫•t v·∫≠t l√Ω nh∆∞ k√≠ch th∆∞·ªõc, ƒë·ªô ·∫©m, nhi·ªát ƒë·ªô ƒë·ªÉ t·∫°o ƒëi·ªÅu ki·ªán thu·∫≠n l·ª£i cho vi·ªác ph√¢n lo·∫°i, l∆∞u gi·ªØ, v·∫≠n chuy·ªÉn, t√°i s·ª≠ d·ª•ng, t√°i ch·∫ø, ƒë·ªìng x·ª≠ l√Ω, x·ª≠ l√Ω nh·∫±m ph·ªëi tr·ªôn ho·∫∑c t√°ch ri√™ng c√°c th√†nh ph·∫ßn c·ªßa ch·∫•t th·∫£i cho ph√π h·ª£p v·ªõi c√°c quy tr√¨nh qu·∫£n l√Ω kh√°c nhau.
  7. T√°i ch·∫ø ch·∫•t th·∫£i l√† qu√° tr√¨nh s·ª≠ d·ª•ng c√°c gi·∫£i ph√°p c√¥ng ngh·ªá, k·ªπ thu·∫≠t ƒë·ªÉ thu l·∫°i c√°c th√†nh ph·∫ßn c√≥ gi√° tr·ªã t·ª´ ch·∫•t th·∫£i.
  8. X·ª≠ l√Ω ch·∫•t th·∫£i l√† qu√° tr√¨nh s·ª≠ d·ª•ng c√°c gi·∫£i ph√°p c√¥ng ngh·ªá, k·ªπ thu·∫≠t (kh√°c v·ªõi s∆° ch·∫ø) ƒë·ªÉ l√†m gi·∫£m, lo·∫°i b·ªè, c√¥ l·∫≠p, c√°ch ly, thi√™u ƒë·ªët, ti√™u h·ªßy, ch√¥n l·∫•p ch·∫•t th·∫£i v√† c√°c y·∫øu t·ªë c√≥ h·∫°i trong ch·∫•t th·∫£i.
  9. N∆∞·ªõc th·∫£i l√† n∆∞·ªõc ƒë√£ b·ªã thay ƒë·ªïi ƒë·∫∑c ƒëi·ªÉm, t√≠nh ch·∫•t ƒë∆∞·ª£c th·∫£i ra t·ª´ ho·∫°t ƒë·ªông s·∫£n xu·∫•t, kinh doanh, d·ªãch v·ª•, sinh ho·∫°t ho·∫∑c ho·∫°t ƒë·ªông kh√°c.
  10. Ch·∫•t th·∫£i r·∫Øn th√¥ng th∆∞·ªùng l√† ch·∫•t th·∫£i r·∫Øn kh√¥ng thu·ªôc danh m·ª•c ch·∫•t th·∫£i nguy h·∫°i v√† kh√¥ng thu·ªôc danh m·ª•c ch·∫•t th·∫£i c√¥ng nghi·ªáp ph·∫£i ki·ªÉm so√°t c√≥ y·∫øu t·ªë nguy h·∫°i v∆∞·ª£t ng∆∞·ª°ng ch·∫•t th·∫£i nguy h·∫°i.
  11. Ch·∫•t th·∫£i r·∫Øn sinh ho·∫°t (c√≤n g·ªçi l√† r√°c th·∫£i sinh ho·∫°t) l√† ch·∫•t th·∫£i r·∫Øn
  ph√°t sinh trong sinh ho·∫°t th∆∞·ªùng ng√†y c·ªßa con ng∆∞·ªùi.
  12. Ch·∫•t th·∫£i c√¥ng nghi·ªáp l√† ch·∫•t th·∫£i ph√°t sinh t·ª´ ho·∫°t ƒë·ªông s·∫£n xu·∫•t,
  kinh doanh, d·ªãch v·ª•, trong ƒë√≥ bao g·ªìm ch·∫•t th·∫£i nguy h·∫°i, ch·∫•t th·∫£i c√¥ng
  nghi·ªáp ph·∫£i ki·ªÉm so√°t v√† ch·∫•t th·∫£i r·∫Øn c√¥ng nghi·ªáp th√¥ng th∆∞·ªùng.
  13. Vi nh·ª±a trong s·∫£n ph·∫©m, h√†ng h√≥a l√† c√°c h·∫°t nh·ª±a r·∫Øn, kh√¥ng tan
  trong n∆∞·ªõc c√≥ ƒë∆∞·ªùng k√≠nh nh·ªè h∆°n 05 mm v·ªõi th√†nh ph·∫ßn ch√≠nh l√† polyme
  t·ªïng h·ª£p ho·∫∑c b√°n t·ªïng h·ª£p, ƒë∆∞·ª£c ph·ªëi tr·ªôn c√≥ ch·ªß ƒë√≠ch trong c√°c s·∫£n ph·∫©m,
  h√†ng h√≥a bao g·ªìm: kem ƒë√°nh rƒÉng, b·ªôt gi·∫∑t, x√† ph√≤ng, m·ªπ ph·∫©m, d·∫ßu g·ªôi ƒë·∫ßu,
  s·ªØa t·∫Øm, s·ªØa r·ª≠a m·∫∑t v√† c√°c s·∫£n ph·∫©m t·∫©y da kh√°c.
  14. S·∫£n ph·∫©m nh·ª±a s·ª≠ d·ª•ng m·ªôt l·∫ßn l√† c√°c s·∫£n ph·∫©m (tr·ª´ s·∫£n ph·∫©m g·∫Øn
  k√®m kh√¥ng th·ªÉ thay th·∫ø) bao g·ªìm khay, h·ªôp ch·ª©a ƒë·ª±ng th·ª±c ph·∫©m, b√°t, ƒë≈©a,
  ly, c·ªëc, dao, th√¨a, dƒ©a, ·ªëng h√∫t, d·ª•ng c·ª• ƒÉn u·ªëng kh√°c c√≥ th√†nh ph·∫ßn nh·ª±a
  ƒë∆∞·ª£c thi·∫øt k·∫ø v√† ƒë∆∞a ra th·ªã tr∆∞·ªùng v·ªõi ch·ªß ƒë√≠ch ƒë·ªÉ s·ª≠ d·ª•ng m·ªôt l·∫ßn tr∆∞·ªõc khi
  th·∫£i b·ªè ra m√¥i tr∆∞·ªùng.
  15. Bao b√¨ nh·ª±a kh√≥ ph√¢n h·ªßy sinh h·ªçc l√† bao b√¨ c√≥ th√†nh ph·∫ßn ch√≠nh l√†
  polyme c√≥ ngu·ªìn g·ªëc t·ª´ d·∫ßu m·ªè nh∆∞ nh·ª±a Polyme Etylen (PE), Polypropylen
  (PP), Polyme Styren (PS), Polyme Vinyl Clorua (PVC), Polyethylene
  Terephthalate (PET) v√† th∆∞·ªùng kh√≥ ph√¢n h·ªßy, l√¢u ph√¢n h·ªßy trong m√¥i tr∆∞·ªùng
  th·∫£i b·ªè (m√¥i tr∆∞·ªùng n∆∞·ªõc, m√¥i tr∆∞·ªùng ƒë·∫•t ho·∫∑c t·∫°i b√£i ch√¥n l·∫•p ch·∫•t th·∫£i r·∫Øn).
  16. Khu b·∫£o t·ªìn thi√™n nhi√™n bao g·ªìm v∆∞·ªùn qu·ªëc gia, khu d·ª± tr·ªØ thi√™n
  nhi√™n, khu b·∫£o t·ªìn lo√†i - sinh c·∫£nh v√† khu b·∫£o v·ªá c·∫£nh quan ƒë∆∞·ª£c x√°c l·∫≠p theo
  quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t v·ªÅ ƒëa d·∫°ng sinh h·ªçc, l√¢m nghi·ªáp v√† th·ªßy s·∫£n.
  17. H√†ng ho√° m√¥i tr∆∞·ªùng l√† c√¥ng ngh·ªá, thi·∫øt b·ªã, s·∫£n ph·∫©m ƒë∆∞·ª£c s·ª≠ d·ª•ng
  ƒë·ªÉ b·∫£o v·ªá m√¥i tr∆∞·ªùng.
  18. H·ªá th·ªëng th√¥ng tin m√¥i tr∆∞·ªùng l√† m·ªôt h·ªá th·ªëng ƒë·ªìng b·ªô theo m·ªôt
  ki·∫øn tr√∫c t·ªïng th·ªÉ bao g·ªìm con ng∆∞·ªùi, m√°y m√≥c thi·∫øt b·ªã, k·ªπ thu·∫≠t, d·ªØ li·ªáu v√†
  c√°c ch∆∞∆°ng tr√¨nh l√†m nhi·ªám v·ª• thu nh·∫≠n, x·ª≠ l√Ω, l∆∞u tr·ªØ v√† ph√¢n ph·ªëi th√¥ng tin
  v·ªÅ m√¥i tr∆∞·ªùng cho ng∆∞·ªùi s·ª≠ d·ª•ng trong m·ªôt m√¥i tr∆∞·ªùng nh·∫•t ƒë·ªãnh.
  19. H·∫°n ng·∫°ch x·∫£ n∆∞·ªõc th·∫£i l√† t·∫£i l∆∞·ª£ng c·ªßa t·ª´ng th√¥ng s·ªë √¥ nhi·ªÖm c√≥
  th·ªÉ ti·∫øp t·ª•c x·∫£ v√†o m√¥i tr∆∞·ªùng n∆∞·ªõc.
  20. Ngu·ªìn √¥ nhi·ªÖm ƒëi·ªÉm l√† ngu·ªìn th·∫£i tr·ª±c ti·∫øp ch·∫•t √¥ nhi·ªÖm v√†o m√¥i
  tr∆∞·ªùng ph·∫£i ƒë∆∞·ª£c x·ª≠ l√Ω v√† c√≥ t√≠nh ch·∫•t ƒë∆°n l·∫ª, c√≥ v·ªã tr√≠ x√°c ƒë·ªãnh.
  21. Ngu·ªìn √¥ nhi·ªÖm di·ªán l√† ngu·ªìn th·∫£i ch·∫•t √¥ nhi·ªÖm v√†o m√¥i tr∆∞·ªùng, c√≥
  t√≠nh ch·∫•t ph√¢n t√°n, kh√¥ng c√≥ v·ªã tr√≠ x√°c ƒë·ªãnh.
  22. C∆° s·ªü th·ª±c hi·ªán d·ªãch v·ª• x·ª≠ l√Ω ch·∫•t th·∫£i l√† c∆° s·ªü c√≥ ho·∫°t ƒë·ªông x·ª≠ l√Ω
  ch·∫•t th·∫£i (bao g·ªìm c·∫£ ho·∫°t ƒë·ªông t√°i ch·∫ø, ƒë·ªìng x·ª≠ l√Ω ch·∫•t th·∫£i) cho c√°c h·ªô gia
  ƒë√¨nh, c√° nh√¢n, c∆° quan, t·ªï ch·ª©c, c∆° s·ªü s·∫£n xu·∫•t, kinh doanh, d·ªãch v·ª•, khu s·∫£n
  xu·∫•t, kinh doanh, d·ªãch v·ª• t·∫≠p trung, c·ª•m c√¥ng nghi·ªáp.
  23. N∆∞·ªõc th·∫£i ph·∫£i x·ª≠ l√Ω l√† n∆∞·ªõc th·∫£i n·∫øu kh√¥ng x·ª≠ l√Ω th√¨ kh√¥ng ƒë√°p
  ·ª©ng quy chu·∫©n k·ªπ thu·∫≠t m√¥i tr∆∞·ªùng, quy chu·∫©n k·ªπ thu·∫≠t, h∆∞·ªõng d·∫´n k·ªπ thu·∫≠t,quy ƒë·ªãnh ƒë·ªÉ t√°i s·ª≠ d·ª•ng khi ƒë√°p ·ª©ng y√™u c·∫ßu v·ªÅ b·∫£o v·ªá m√¥i tr∆∞·ªùng ho·∫∑c quy ƒë·ªãnh c·ªßa ch·ªß ƒë·∫ßu t∆∞ x√¢y d·ª±ng v√† kinh doanh h·∫° t·∫ßng khu s·∫£n xu·∫•t, kinh doanh, d·ªãch v·ª• t·∫≠p trung, c·ª•m c√¥ng nghi·ªáp, h·ªá th·ªëng x·ª≠ l√Ω n∆∞·ªõc th·∫£i t·∫≠p trung c·ªßa khu ƒë√¥ th·ªã, khu d√¢n c∆∞ t·∫≠p trung.
  24. Ngu·ªìn ph√°t sinh n∆∞·ªõc th·∫£i l√† h·ªá th·ªëng, c√¥ng tr√¨nh, m√°y m√≥c, thi·∫øt b·ªã, c√¥ng ƒëo·∫°n ho·∫∑c ho·∫°t ƒë·ªông c√≥ ph√°t sinh n∆∞·ªõc th·∫£i. Ngu·ªìn ph√°t sinh n∆∞·ªõc th·∫£i c√≥ th·ªÉ bao g·ªìm nhi·ªÅu h·ªá th·ªëng, c√¥ng tr√¨nh, m√°y m√≥c, thi·∫øt b·ªã, c√¥ng ƒëo·∫°n ho·∫∑c ho·∫°t ƒë·ªông c√≥ ph√°t sinh n∆∞·ªõc th·∫£i c√πng t√≠nh ch·∫•t v√† c√πng khu v·ª±c.
  25. D√≤ng n∆∞·ªõc th·∫£i l√† n∆∞·ªõc th·∫£i sau x·ª≠ l√Ω ho·∫∑c ph·∫£i ƒë∆∞·ª£c ki·ªÉm so√°t tr∆∞·ªõc khi x·∫£ ra ngu·ªìn ti·∫øp nh·∫≠n n∆∞·ªõc th·∫£i t·∫°i m·ªôt v·ªã tr√≠ x·∫£ th·∫£i x√°c ƒë·ªãnh.
  26. Ngu·ªìn ti·∫øp nh·∫≠n n∆∞·ªõc th·∫£i (c√≤n g·ªçi l√† ngu·ªìn n∆∞·ªõc ti·∫øp nh·∫≠n) l√† c√°c d·∫°ng t√≠ch t·ª• n∆∞·ªõc t·ª± nhi√™n, nh√¢n t·∫°o c√≥ m·ª•c ƒë√≠ch s·ª≠ d·ª•ng x√°c ƒë·ªãnh do c∆° quan nh√† n∆∞·ªõc c√≥ th·∫©m quy·ªÅn quy ƒë·ªãnh. C√°c d·∫°ng t√≠ch t·ª• n∆∞·ªõc t·ª± nhi√™n bao g·ªìm s√¥ng, su·ªëi, k√™nh, m∆∞∆°ng, r·∫°ch, h·ªì, ao, ƒë·∫ßm, ph√° v√† c√°c d·∫°ng t√≠ch t·ª• n∆∞·ªõc kh√°c ƒë∆∞·ª£c h√¨nh th√†nh t·ª± nhi√™n. C√°c d·∫°ng t√≠ch t·ª• n∆∞·ªõc nh√¢n t·∫°o, bao g·ªìm: H·ªì ch·ª©a th·ªßy ƒëi·ªán, th·ªßy l·ª£i, s√¥ng, k√™nh, m∆∞∆°ng, r·∫°ch, h·ªì, ao, ƒë·∫ßm v√† c√°c d·∫°ng t√≠ch t·ª• n∆∞·ªõc kh√°c do con ng∆∞·ªùi t·∫°o ra.
  Tr∆∞·ªùng h·ª£p ngu·ªìn n∆∞·ªõc t·∫°i v·ªã tr√≠ x·∫£ n∆∞·ªõc th·∫£i ch∆∞a ƒë∆∞·ª£c c∆° quan nh√† n∆∞·ªõc c√≥ th·∫©m quy·ªÅn x√°c ƒë·ªãnh m·ª•c ƒë√≠ch s·ª≠ d·ª•ng th√¨ ngu·ªìn ti·∫øp nh·∫≠n n∆∞·ªõc th·∫£i l√† ngu·ªìn n∆∞·ªõc li·ªÅn th√¥ng g·∫ßn nh·∫•t ƒë√£ ƒë∆∞·ª£c x√°c ƒë·ªãnh m·ª•c ƒë√≠ch s·ª≠ d·ª•ng.
  27. B·ª•i, kh√≠ th·∫£i ph·∫£i x·ª≠ l√Ω l√† b·ª•i, kh√≠ th·∫£i n·∫øu kh√¥ng x·ª≠ l√Ω th√¨ kh√¥ng ƒë√°p ·ª©ng quy chu·∫©n k·ªπ thu·∫≠t m√¥i tr∆∞·ªùng.
  28. Ngu·ªìn ph√°t sinh b·ª•i, kh√≠ th·∫£i (sau ƒë√¢y g·ªçi chung l√† ngu·ªìn ph√°t sinh kh√≠ th·∫£i) l√† h·ªá th·ªëng, c√¥ng tr√¨nh, m√°y m√≥c, thi·∫øt b·ªã, c√¥ng ƒëo·∫°n ho·∫∑c ho·∫°t ƒë·ªông c√≥ ph√°t sinh b·ª•i, kh√≠ th·∫£i v√† c√≥ v·ªã tr√≠ x√°c ƒë·ªãnh. Tr∆∞·ªùng h·ª£p nhi·ªÅu h·ªá th·ªëng, c√¥ng tr√¨nh, m√°y m√≥c, thi·∫øt b·ªã t·∫°i c√πng m·ªôt khu v·ª±c c√≥ ph√°t sinh b·ª•i, kh√≠ th·∫£i c√≥ c√πng t√≠nh ch·∫•t v√† ƒë∆∞·ª£c thu gom, x·ª≠ l√Ω chung t·∫°i m·ªôt h·ªá th·ªëng x·ª≠ l√Ω kh√≠ th·∫£i th√¨ ƒë∆∞·ª£c coi l√† m·ªôt ngu·ªìn kh√≠ th·∫£i.

  29.D√≤ng kh√≠ th·∫£i l√† kh√≠ th·∫£i sau khi x·ª≠ l√Ω ƒë∆∞·ª£c x·∫£ v√†o m√¥i tr∆∞·ªùng kh√¥ng kh√≠ th√¥ng qua ·ªëng kh√≥i, ·ªëng th·∫£i.
  30.Ho·∫°t ƒë·ªông s·∫£n xu·∫•t, kinh doanh, d·ªãch v·ª• l√† ho·∫°t ƒë·ªông c·ªßa t·ªï ch·ª©c, c√° nh√¢n th·ª±c hi·ªán ƒë·ªÉ s·∫£n xu·∫•t, kinh doanh, d·ªãch v·ª•, kh√¥ng bao g·ªìm ho·∫°t ƒë·ªông d·ªãch v·ª• h√†nh ch√≠nh c√¥ng khi xem x√©t c·∫•p gi·∫•y ph√©p m√¥i tr∆∞·ªùng.
  31.D·ª± √°n c√≥ s·ª≠ d·ª•ng ƒë·∫•t, ƒë·∫•t c√≥ m·∫∑t n∆∞·ªõc l√† d·ª± √°n ƒë∆∞·ª£c giao ƒë·∫•t, cho thu√™ ƒë·∫•t theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t v·ªÅ ƒë·∫•t ƒëai ho·∫∑c d·ª± √°n ƒë∆∞·ª£c tri·ªÉn khai tr√™n ƒë·∫•t, ƒë·∫•t c√≥ m·∫∑t n∆∞·ªõc theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t c√≥ li√™n quan.
  32.B√°o c√°o ƒë√°nh gi√° t√°c ƒë·ªông m√¥i tr∆∞·ªùng ƒë√£ ƒë∆∞·ª£c ph√™ duy·ªát k·∫øt qu·∫£ th·∫©m ƒë·ªãnh l√†:
  a) B√°o c√°o ƒë√°nh gi√° t√°c ƒë·ªông m√¥i tr∆∞·ªùng ƒë√£ ƒë∆∞·ª£c c∆° quan c√≥ th·∫©m quy·ªÅn ra quy·∫øt ƒë·ªãnh ph√™ duy·ªát k·∫øt qu·∫£ th·∫©m ƒë·ªãnh, tr·ª´ tr∆∞·ªùng h·ª£p ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i ƒëi·ªÉm b kho·∫£n n√†y;
  b) B√°o c√°o ƒë√°nh gi√° t√°c ƒë·ªông m√¥i tr∆∞·ªùng ƒë√£ ƒë∆∞·ª£c ch·ªânh s·ª≠a, b·ªï sung theo n·ªôi dung, y√™u c·∫ßu v·ªÅ b·∫£o v·ªá m√¥i tr∆∞·ªùng ƒë∆∞·ª£c n√™u trong quy·∫øt ƒë·ªãnh ph√™ duy·ªát k·∫øt qu·∫£ th·∫©m ƒë·ªãnh b√°o c√°o ƒë√°nh gi√° t√°c ƒë·ªông m√¥i tr∆∞·ªùng theo quy ƒë·ªãnh t·∫°i kho·∫£n 1 ƒêi·ªÅu 37 Lu·∫≠t B·∫£o v·ªá m√¥i tr∆∞·ªùng."""
    },
  {
  "ƒêi·ªÅu": "ƒêi·ªÅu 4. N·ªôi dung k·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t",
  "Ch∆∞∆°ng": "Ch∆∞∆°ng II B·∫¢O V·ªÜ C√ÅC TH√ÄNH PH·∫¶N M√îI TR∆Ø·ªúNG V√Ä DI S·∫¢N THI√äN NHI√äN",
  "M·ª•c": "M·ª•c 1 B·∫¢O V·ªÜ M√îI TR∆Ø·ªúNG N∆Ø·ªöC",
  "Pages": "6,7,8,9",
  "Text": """N·ªôi dung ch√≠nh c·ªßa k·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng n∆∞·ªõc m·∫∑t ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i kho·∫£n 2 ƒêi·ªÅu 9 Lu·∫≠t B·∫£o v·ªá m√¥i tr∆∞·ªùng. M·ªôt s·ªë n·ªôi dung ƒë∆∞·ª£c quy ƒë·ªãnh chi ti·∫øt nh∆∞ sau:
        1. V·ªÅ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t; x√°c ƒë·ªãnh v√πng b·∫£o h·ªô
v·ªá sinh khu v·ª±c l·∫•y n∆∞·ªõc sinh ho·∫°t, h√†nh lang b·∫£o v·ªá ngu·ªìn n∆∞·ªõc m·∫∑t; x√°c
ƒë·ªãnh khu v·ª±c sinh th·ªßy:
a) Hi·ªán tr·∫°ng, di·ªÖn bi·∫øn ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t ƒë·ªëi v·ªõi s√¥ng,
h·ªì giai ƒëo·∫°n t·ªëi thi·ªÉu 03 nƒÉm g·∫ßn nh·∫•t;
b) T·ªïng h·ª£p hi·ªán tr·∫°ng c√°c v√πng b·∫£o h·ªô v·ªá sinh khu v·ª±c l·∫•y n∆∞·ªõc sinh
ho·∫°t, h√†nh lang b·∫£o v·ªá ngu·ªìn n∆∞·ªõc m·∫∑t, ngu·ªìn sinh th·ªßy ƒë√£ ƒë∆∞·ª£c x√°c ƒë·ªãnh
theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t v·ªÅ t√†i nguy√™n n∆∞·ªõc.
2. V·ªÅ lo·∫°i v√† t·ªïng l∆∞·ª£ng ch·∫•t √¥ nhi·ªÖm th·∫£i v√†o m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t:
a) K·∫øt qu·∫£ t·ªïng h·ª£p, ƒë√°nh gi√° t·ªïng t·∫£i l∆∞·ª£ng c·ªßa t·ª´ng ch·∫•t √¥ nhi·ªÖm
ƒë∆∞·ª£c l·ª±a ch·ªçn ƒë·ªÉ ƒë√°nh gi√° kh·∫£ nƒÉng ch·ªãu t·∫£i ƒë·ªëi v·ªõi m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t t·ª´
c√°c ngu·ªìn √¥ nhi·ªÖm ƒëi·ªÉm, ngu·ªìn √¥ nhi·ªÖm di·ªán ƒë√£ ƒë∆∞·ª£c ƒëi·ªÅu tra, ƒë√°nh gi√° theo
quy ƒë·ªãnh t·∫°i ƒëi·ªÉm b kho·∫£n 2 ƒêi·ªÅu 9 Lu·∫≠t B·∫£o v·ªá m√¥i tr∆∞·ªùng;
b) D·ª± b√°o t√¨nh h√¨nh ph√°t sinh t·∫£i l∆∞·ª£ng √¥ nhi·ªÖm t·ª´ c√°c ngu·ªìn √¥ nhi·ªÖm
ƒëi·ªÉm, ngu·ªìn √¥ nhi·ªÖm di·ªán trong th·ªùi k·ª≥ c·ªßa k·∫ø ho·∫°ch.
3. V·ªÅ ƒë√°nh gi√° kh·∫£ nƒÉng ch·ªãu t·∫£i, ph√¢n v√πng x·∫£ th·∫£i, h·∫°n ng·∫°ch x·∫£ n∆∞·ªõc
th·∫£i:
a) T·ªïng h·ª£p k·∫øt qu·∫£ ƒë√°nh gi√° kh·∫£ nƒÉng ch·ªãu t·∫£i c·ªßa m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t
tr√™n c∆° s·ªü c√°c k·∫øt qu·∫£ ƒë√£ c√≥ trong v√≤ng t·ªëi ƒëa 03 nƒÉm g·∫ßn nh·∫•t v√† k·∫øt qu·∫£ ƒëi·ªÅu
tra, ƒë√°nh gi√° b·ªï sung; x√°c ƒë·ªãnh l·ªô tr√¨nh ƒë√°nh gi√° kh·∫£ nƒÉng ch·ªãu t·∫£i c·ªßa m√¥i
tr∆∞·ªùng n∆∞·ªõc m·∫∑t trong giai ƒëo·∫°n th·ª±c hi·ªán k·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i
tr∆∞·ªùng n∆∞·ªõc m·∫∑t;
b) Ph√¢n v√πng x·∫£ th·∫£i theo m·ª•c ƒë√≠ch b·∫£o v·ªá v√† c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng m√¥i
tr∆∞·ªùng n∆∞·ªõc m·∫∑t tr√™n c∆° s·ªü k·∫øt qu·∫£ ƒë√°nh gi√° kh·∫£ nƒÉng ch·ªãu t·∫£i c·ªßa m√¥i tr∆∞·ªùng
n∆∞·ªõc m·∫∑t v√† ph√¢n v√πng m√¥i tr∆∞·ªùng (n·∫øu c√≥);
c) X√°c ƒë·ªãnh h·∫°n ng·∫°ch x·∫£ n∆∞·ªõc th·∫£i ƒë·ªëi v·ªõi t·ª´ng ƒëo·∫°n s√¥ng, h·ªì tr√™n c∆°
s·ªü k·∫øt qu·∫£ ƒë√°nh gi√° kh·∫£ nƒÉng ch·ªãu t·∫£i c·ªßa m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t v√† vi·ªác ph√¢n
v√πng x·∫£ th·∫£i.
4. D·ª± b√°o xu h∆∞·ªõng di·ªÖn bi·∫øn ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t tr√™n c∆°
s·ªü c√°c n·ªôi dung sau:
a) D·ª± b√°o t√¨nh h√¨nh ph√°t sinh t·∫£i l∆∞·ª£ng √¥ nhi·ªÖm t·ª´ c√°c ngu·ªìn √¥ nhi·ªÖm
ƒëi·ªÉm, √¥ nhi·ªÖm di·ªán trong giai ƒëo·∫°n 05 nƒÉm ti·∫øp theo;
b) K·∫øt qu·∫£ th·ª±c hi·ªán c√°c n·ªôi dung quy ƒë·ªãnh t·∫°i c√°c kho·∫£n 1, 2 v√† 3
ƒêi·ªÅu n√†y.
5. V·ªÅ c√°c m·ª•c ti√™u, ch·ªâ ti√™u c·ªßa k·∫ø ho·∫°ch:
a) M·ª•c ti√™u, ch·ªâ ti√™u v·ªÅ ch·∫•t l∆∞·ª£ng n∆∞·ªõc m·∫∑t c·∫ßn ƒë·∫°t ƒë∆∞·ª£c cho giai ƒëo·∫°n
05 nƒÉm ƒë·ªëi v·ªõi t·ª´ng ƒëo·∫°n s√¥ng, h·ªì cƒÉn c·ª© nhu c·∫ßu th·ª±c ti·ªÖn v·ªÅ ph√°t tri·ªÉn kinh
t·∫ø - x√£ h·ªôi, b·∫£o v·ªá m√¥i tr∆∞·ªùng; m·ª•c ti√™u ch·∫•t l∆∞·ª£ng n∆∞·ªõc c·ªßa s√¥ng, h·ªì n·ªôi t·ªânh
ph·∫£i ph√π h·ª£p v·ªõi m·ª•c ti√™u ch·∫•t l∆∞·ª£ng n∆∞·ªõc c·ªßa s√¥ng, h·ªì li√™n t·ªânh;
b) M·ª•c ti√™u v√† l·ªô tr√¨nh gi·∫£m x·∫£ th·∫£i v√†o c√°c ƒëo·∫°n s√¥ng, h·ªì kh√¥ng c√≤n
kh·∫£ nƒÉng ch·ªãu t·∫£i nh·∫±m m·ª•c ti√™u c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng n∆∞·ªõc, c·ª• th·ªÉ: t·ªïng t·∫£i
l∆∞·ª£ng √¥ nhi·ªÖm c·∫ßn gi·∫£m ƒë·ªëi v·ªõi t·ª´ng th√¥ng s·ªë √¥ nhi·ªÖm m√† m√¥i tr∆∞·ªùng n∆∞·ªõc
m·∫∑t kh√¥ng c√≤n kh·∫£ nƒÉng ch·ªãu t·∫£i; ph√¢n b·ªï t·∫£i l∆∞·ª£ng c·∫ßn gi·∫£m theo nh√≥m
ngu·ªìn √¥ nhi·ªÖm v√† l·ªô tr√¨nh th·ª±c hi·ªán.
6. V·ªÅ bi·ªán ph√°p ph√≤ng ng·ª´a v√† gi·∫£m thi·ªÉu √¥ nhi·ªÖm m√¥i tr∆∞·ªùng n∆∞·ªõc
m·∫∑t; gi·∫£i ph√°p h·ª£p t√°c, chia s·∫ª th√¥ng tin v√† qu·∫£n l√Ω √¥ nhi·ªÖm n∆∞·ªõc m·∫∑t xuy√™n
bi√™n gi·ªõi:
a) C√°c bi·ªán ph√°p quy ƒë·ªãnh t·∫°i kho·∫£n 2 ƒêi·ªÅu 7 Lu·∫≠t B·∫£o v·ªá m√¥i tr∆∞·ªùng
ƒë·ªëi v·ªõi ƒëo·∫°n s√¥ng, h·ªì kh√¥ng c√≤n kh·∫£ nƒÉng ch·ªãu t·∫£i;
b) C√°c bi·ªán ph√°p, gi·∫£i ph√°p b·∫£o v·ªá c√°c v√πng b·∫£o h·ªô v·ªá sinh khu v·ª±c l·∫•y
n∆∞·ªõc sinh ho·∫°t, h√†nh lang b·∫£o v·ªá ngu·ªìn n∆∞·ªõc m·∫∑t, ngu·ªìn sinh th·ªßy theo quy
ƒë·ªãnh c·ªßa ph√°p lu·∫≠t v·ªÅ t√†i nguy√™n n∆∞·ªõc;
c)13 C√°c bi·ªán ph√°p, gi·∫£i ph√°p v·ªÅ c∆° ch·∫ø, ch√≠nh s√°ch ƒë·ªÉ th·ª±c hi·ªán l·ªô tr√¨nh
quy ƒë·ªãnh t·∫°i kho·∫£n 5 ƒêi·ªÅu n√†y;
d) C√°c bi·ªán ph√°p, gi·∫£i ph√°p ki·ªÉm so√°t c√°c ngu·ªìn x·∫£ th·∫£i v√†o m√¥i tr∆∞·ªùng
n∆∞·ªõc m·∫∑t;
ƒë) Thi·∫øt l·∫≠p h·ªá th·ªëng quan tr·∫Øc, c·∫£nh b√°o di·ªÖn bi·∫øn ch·∫•t l∆∞·ª£ng m√¥i
tr∆∞·ªùng n∆∞·ªõc m·∫∑t, bao g·ªìm c·∫£ ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t xuy√™n bi√™n
gi·ªõi, ph√π h·ª£p v·ªõi quy ho·∫°ch t·ªïng th·ªÉ quan tr·∫Øc m√¥i tr∆∞·ªùng qu·ªëc gia v√† n·ªôi
dung quan tr·∫Øc m√¥i tr∆∞·ªùng trong quy ho·∫°ch v√πng, quy ho·∫°ch t·ªânh;
e) C√°c bi·ªán ph√°p, gi·∫£i ph√°p h·ª£p t√°c, chia s·∫ª th√¥ng tin v·ªÅ ch·∫•t l∆∞·ª£ng m√¥i
tr∆∞·ªùng n∆∞·ªõc m·∫∑t xuy√™n bi√™n gi·ªõi;
g) C√°c bi·ªán ph√°p, gi·∫£i ph√°p kh√°c.
7. V·ªÅ gi·∫£i ph√°p b·∫£o v·ªá, c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t:
a) C√°c gi·∫£i ph√°p v·ªÅ khoa h·ªçc, c√¥ng ngh·ªá x·ª≠ l√Ω, c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng m√¥i
tr∆∞·ªùng n∆∞·ªõc m·∫∑t;
b) C√°c gi·∫£i ph√°p v·ªÅ c∆° ch·∫ø, ch√≠nh s√°ch;
c) C√°c gi·∫£i ph√°p v·ªÅ t·ªï ch·ª©c, huy ƒë·ªông s·ª± tham gia c·ªßa c∆° quan, t·ªï
ch·ª©c, c·ªông ƒë·ªìng;
d) C√°c gi·∫£i ph√°p c√¥ng tr√¨nh, phi c√¥ng tr√¨nh kh√°c.
8. T·ªï ch·ª©c th·ª±c hi·ªán:
a) Ph√¢n c√¥ng tr√°ch nhi·ªám ƒë·ªëi v·ªõi c∆° quan ch·ªß tr√¨ v√† c√°c c∆° quan ph·ªëi
h·ª£p th·ª±c hi·ªán k·∫ø ho·∫°ch;
b) C∆° ch·∫ø gi√°m s√°t, b√°o c√°o, ƒë√¥n ƒë·ªëc th·ª±c hi·ªán;
c) Danh m·ª•c c√°c d·ª± √°n, nhi·ªám v·ª• ∆∞u ti√™n ƒë·ªÉ th·ª±c hi·ªán c√°c m·ª•c ti√™u c·ªßa
k·∫ø ho·∫°ch;
d) C∆° ch·∫ø ph√¢n b·ªï ngu·ªìn l·ª±c th·ª±c hi·ªán.
"""
  },
{ "ƒêi·ªÅu": "ƒêi·ªÅu 5. Tr√¨nh t·ª±, th·ªß t·ª•c ban h√†nh k·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t",
  "Ch∆∞∆°ng": "Ch∆∞∆°ng II B·∫¢O V·ªÜ C√ÅC TH√ÄNH PH·∫¶N M√îI TR∆Ø·ªúNG V√Ä DI S·∫¢N THI√äN NHI√äN",
  "M·ª•c": "M·ª•c 1 B·∫¢O V·ªÜ M√îI TR∆Ø·ªúNG N∆Ø·ªöC",
  "Pages": "9,10",
  "Text": """1. K·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t ƒë·ªëi v·ªõi c√°c s√¥ng,
h·ªì li√™n t·ªânh c√≥ vai tr√≤ quan tr·ªçng v·ªõi ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi, b·∫£o v·ªá m√¥i
tr∆∞·ªùng ƒë∆∞·ª£c ban h√†nh ƒë·ªëi v·ªõi t·ª´ng s√¥ng, h·ªì li√™n t·ªânh theo quy ƒë·ªãnh sau:
a) B·ªô T√†i nguy√™n v√† M√¥i tr∆∞·ªùng ch·ªß tr√¨, ph·ªëi h·ª£p v·ªõi c√°c b·ªô, c∆° quan
ngang b·ªô, ·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh c√≥ li√™n quan l·∫≠p, ph√™ duy·ªát, tri·ªÉn khai ƒë·ªÅ
√°n ƒëi·ªÅu tra, ƒë√°nh gi√°, x√¢y d·ª±ng d·ª± th·∫£o k·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i
tr∆∞·ªùng n∆∞·ªõc m·∫∑t ƒë·ªëi v·ªõi t·ª´ng s√¥ng, h·ªì li√™n t·ªânh;
b) B·ªô T√†i nguy√™n v√† M√¥i tr∆∞·ªùng g·ª≠i d·ª± th·∫£o k·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t ƒë·ªëi v·ªõi t·ª´ng s√¥ng, h·ªì li√™n t·ªânh ƒë·∫øn ·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh v√† c√°c b·ªô, c∆° quan ngang b·ªô c√≥ li√™n quan ƒë·ªÉ l·∫•y √Ω ki·∫øn b·∫±ng vƒÉn b·∫£n; nghi√™n c·ª©u, ti·∫øp thu, gi·∫£i tr√¨nh c√°c √Ω ki·∫øn g√≥p √Ω, ho√†n thi·ªán d·ª± th·∫£o k·∫ø ho·∫°ch, tr√¨nh Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß xem x√©t, ban h√†nh. H·ªì s∆° tr√¨nh Th·ªß t∆∞·ªõng
Ch√≠nh ph·ªß bao g·ªìm: t·ªù tr√¨nh; d·ª± th·∫£o k·∫ø ho·∫°ch; d·ª± th·∫£o quy·∫øt ƒë·ªãnh ban h√†nh k·∫ø ho·∫°ch; b√°o c√°o gi·∫£i tr√¨nh, ti·∫øp thu c√°c √Ω ki·∫øn g√≥p √Ω; vƒÉn b·∫£n g√≥p √Ω c·ªßa c√°c c∆° quan c√≥ li√™n quan;
c) CƒÉn c·ª© y√™u c·∫ßu qu·∫£n l√Ω nh√† n∆∞·ªõc v√† ƒë·ªÅ xu·∫•t c·ªßa ·ª¶y ban nh√¢n d√¢n
c·∫•p t·ªânh, B·ªô T√†i nguy√™n v√† M√¥i tr∆∞·ªùng xem x√©t, quy·∫øt ƒë·ªãnh vi·ªác giao nhi·ªám
v·ª• x√¢y d·ª±ng k·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng n∆∞·ªõc m·∫∑t ƒë·ªëi v·ªõi t·ª´ng s√¥ng, h·ªì li√™n
t·ªânh cho ·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh ch·ªß tr√¨, ph·ªëi h·ª£p v·ªõi c√°c ƒë·ªãa ph∆∞∆°ng, c∆°
quan c√≥ li√™n quan th·ª±c hi·ªán.
·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh ƒë∆∞·ª£c giao nhi·ªám v·ª• ch·ªß tr√¨ th·ª±c hi·ªán tr√°ch
nhi·ªám c·ªßa B·ªô T√†i nguy√™n v√† M√¥i tr∆∞·ªùng trong vi·ªác x√¢y d·ª±ng, l·∫•y √Ω ki·∫øn v√†
ho√†n thi·ªán d·ª± th·∫£o k·∫ø ho·∫°ch theo quy ƒë·ªãnh t·∫°i ƒëi·ªÉm a v√† ƒëi·ªÉm b kho·∫£n n√†y;
g·ª≠i h·ªì s∆° theo quy ƒë·ªãnh t·∫°i ƒëi·ªÉm b kho·∫£n n√†y ƒë·∫øn B·ªô T√†i nguy√™n v√† M√¥i
tr∆∞·ªùng ƒë·ªÉ xem x√©t, tr√¨nh Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß ban h√†nh.
2. K·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t ƒë·ªëi v·ªõi s√¥ng, h·ªì
n·ªôi t·ªânh c√≥ vai tr√≤ quan tr·ªçng v·ªõi ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi, b·∫£o v·ªá m√¥i
tr∆∞·ªùng ƒë∆∞·ª£c x√¢y d·ª±ng chung cho to√†n b·ªô s√¥ng, h·ªì n·ªôi t·ªânh ho·∫∑c ri√™ng cho
t·ª´ng s√¥ng, h·ªì n·ªôi t·ªânh v√† theo quy ƒë·ªãnh sau:
a) C∆° quan chuy√™n m√¥n v·ªÅ b·∫£o v·ªá m√¥i tr∆∞·ªùng c·∫•p t·ªânh ch·ªß tr√¨, ph·ªëi h·ª£p
v·ªõi c√°c s·ªü, ban, ng√†nh, ·ª¶y ban nh√¢n d√¢n c·∫•p huy·ªán c√≥ li√™n quan l·∫≠p, ph√™
duy·ªát v√† th·ª±c hi·ªán ƒë·ªÅ √°n ƒëi·ªÅu tra, ƒë√°nh gi√°, x√¢y d·ª±ng d·ª± th·∫£o k·∫ø ho·∫°ch qu·∫£n
l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t s√¥ng, h·ªì n·ªôi t·ªânh;
b) C∆° quan chuy√™n m√¥n v·ªÅ b·∫£o v·ªá m√¥i tr∆∞·ªùng c·∫•p t·ªânh g·ª≠i d·ª± th·∫£o k·∫ø
ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t s√¥ng, h·ªì n·ªôi t·ªânh ƒë·∫øn c√°c ·ª¶y
ban nh√¢n d√¢n c·∫•p huy·ªán, c√°c s·ªü, ban, ng√†nh li√™n quan v√† c∆° quan chuy√™n m√¥n
v·ªÅ b·∫£o v·ªá m√¥i tr∆∞·ªùng c·∫•p t·ªânh c·ªßa c√°c t·ªânh, th√†nh ph·ªë tr·ª±c thu·ªôc trung ∆∞∆°ng
gi√°p ranh ƒë·ªÉ l·∫•y √Ω ki·∫øn b·∫±ng vƒÉn b·∫£n; nghi√™n c·ª©u, ti·∫øp thu, gi·∫£i tr√¨nh c√°c √Ω
ki·∫øn g√≥p √Ω, ho√†n thi·ªán d·ª± th·∫£o k·∫ø ho·∫°ch, tr√¨nh ·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh xem
x√©t, ban h√†nh. H·ªì s∆° tr√¨nh ·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh bao g·ªìm: t·ªù tr√¨nh; d·ª±
th·∫£o k·∫ø ho·∫°ch; d·ª± th·∫£o quy·∫øt ƒë·ªãnh ban h√†nh k·∫ø ho·∫°ch; b√°o c√°o gi·∫£i tr√¨nh, ti·∫øp
thu c√°c √Ω ki·∫øn g√≥p √Ω; vƒÉn b·∫£n g√≥p √Ω c·ªßa c√°c c∆° quan c√≥ li√™n quan.
3. Vi·ªác x√°c ƒë·ªãnh s√¥ng, h·ªì c√≥ vai tr√≤ quan tr·ªçng v·ªõi ph√°t tri·ªÉn kinh t·∫ø -
x√£ h·ªôi, b·∫£o v·ªá m√¥i tr∆∞·ªùng ƒë∆∞·ª£c cƒÉn c·ª© v√†o hi·ªán tr·∫°ng ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng
n∆∞·ªõc m·∫∑t, hi·ªán tr·∫°ng ngu·ªìn th·∫£i, nhu c·∫ßu s·ª≠ d·ª•ng ngu·ªìn n∆∞·ªõc cho c√°c m·ª•c
ƒë√≠ch ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi, m·ª•c ti√™u b·∫£o v·ªá v√† c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng m√¥i
tr∆∞·ªùng n∆∞·ªõc m·∫∑t v√† c√°c y√™u c·∫ßu qu·∫£n l√Ω nh√† n∆∞·ªõc v·ªÅ b·∫£o v·ªá m√¥i tr∆∞·ªùng kh√°c.
4. K·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t ƒë·ªëi v·ªõi c√°c s√¥ng,
h·ªì li√™n t·ªânh ph·∫£i ph√π h·ª£p v·ªõi quy ho·∫°ch b·∫£o v·ªá m√¥i tr∆∞·ªùng qu·ªëc gia. Tr∆∞·ªùng
h·ª£p quy ho·∫°ch b·∫£o v·ªá m√¥i tr∆∞·ªùng qu·ªëc gia ch∆∞a ƒë∆∞·ª£c ban h√†nh, k·∫ø ho·∫°ch
qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t ƒë·ªëi v·ªõi c√°c s√¥ng, h·ªì li√™n t·ªânh ph·∫£i
ph√π h·ª£p v·ªõi y√™u c·∫ßu qu·∫£n l√Ω nh√† n∆∞·ªõc v√† ph·∫£i ƒë∆∞·ª£c r√† so√°t, c·∫≠p nh·∫≠t ph√π h·ª£p
v·ªõi quy ho·∫°ch b·∫£o v·ªá m√¥i tr∆∞·ªùng qu·ªëc gia khi ƒë∆∞·ª£c ban h√†nh.
5. K·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t ƒë·ªëi v·ªõi c√°c s√¥ng,
h·ªì n·ªôi t·ªânh ph·∫£i ph√π h·ª£p v·ªõi quy ho·∫°ch b·∫£o v·ªá m√¥i tr∆∞·ªùng qu·ªëc gia, n·ªôi dung
b·∫£o v·ªá m√¥i tr∆∞·ªùng trong quy ho·∫°ch v√πng, quy ho·∫°ch t·ªânh. Tr∆∞·ªùng h·ª£p quy
ho·∫°ch b·∫£o v·ªá m√¥i tr∆∞·ªùng qu·ªëc gia, n·ªôi dung b·∫£o v·ªá m√¥i tr∆∞·ªùng trong quy
ho·∫°ch v√πng, quy ho·∫°ch t·ªânh ch∆∞a ƒë∆∞·ª£c ban h√†nh, k·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng
m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t ƒë·ªëi v·ªõi c√°c s√¥ng, h·ªì n·ªôi t·ªânh ph·∫£i ph√π h·ª£p v·ªõi y√™u c·∫ßu
qu·∫£n l√Ω nh√† n∆∞·ªõc v√† ph·∫£i ƒë∆∞·ª£c r√† so√°t, c·∫≠p nh·∫≠t ph√π h·ª£p v·ªõi quy ho·∫°ch b·∫£o
v·ªá m√¥i tr∆∞·ªùng qu·ªëc gia, quy ho·∫°ch v√πng, quy ho·∫°ch t·ªânh khi ƒë∆∞·ª£c ban h√†nh.
6. K·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng n∆∞·ªõc m·∫∑t quy ƒë·ªãnh t·∫°i kho·∫£n 1
v√† kho·∫£n 2 ƒêi·ªÅu n√†y ph·∫£i ƒë∆∞·ª£c x√¢y d·ª±ng ph√π h·ª£p v·ªõi k·∫ø ho·∫°ch ph√°t tri·ªÉn
kinh t·∫ø - x√£ h·ªôi 05 nƒÉm. Tr∆∞·ªõc ng√†y 30 th√°ng 6 nƒÉm th·ª© t∆∞ c·ªßa k·∫ø ho·∫°ch ƒë·∫ßu
t∆∞ c√¥ng trung h·∫°n giai ƒëo·∫°n tr∆∞·ªõc, c∆° quan ph√™ duy·ªát k·∫ø ho·∫°ch ch·ªâ ƒë·∫°o t·ªï
ch·ª©c t·ªïng k·∫øt, ƒë√°nh gi√° vi·ªác th·ª±c hi·ªán k·∫ø ho·∫°ch k·ª≥ tr∆∞·ªõc, x√¢y d·ª±ng, ph√™ duy·ªát
k·∫ø ho·∫°ch cho giai ƒëo·∫°n ti·∫øp theo ƒë·ªÉ l√†m c∆° s·ªü ƒë·ªÅ xu·∫•t k·∫ø ho·∫°ch ƒë·∫ßu t∆∞ c√¥ng
trung h·∫°n."""
},

{ "ƒêi·ªÅu": "ƒêi·ªÅu 6. N·ªôi dung k·∫ø ho·∫°ch qu·ªëc gia v·ªÅ qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠",
  "Ch∆∞∆°ng": "Ch∆∞∆°ng II B·∫¢O V·ªÜ C√ÅC TH√ÄNH PH·∫¶N M√îI TR∆Ø·ªúNG V√Ä DI S·∫¢N THI√äN NHI√äN",
  "M·ª•c": "M·ª•c 2 B·∫¢O V·ªÜ M√îI TR∆Ø·ªúNG KH√îNG KH√ç",
  "Pages": "10,11,12",
  "Text": """N·ªôi dung ch√≠nh c·ªßa k·∫ø ho·∫°ch qu·ªëc gia v·ªÅ qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i
tr∆∞·ªùng kh√¥ng kh√≠ ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i kho·∫£n 3 ƒêi·ªÅu 13 Lu·∫≠t B·∫£o v·ªá m√¥i tr∆∞·ªùng.
M·ªôt s·ªë n·ªôi dung ƒë∆∞·ª£c quy ƒë·ªãnh chi ti·∫øt nh∆∞ sau:
1. V·ªÅ ƒë√°nh gi√° c√¥ng t√°c qu·∫£n l√Ω, ki·ªÉm so√°t √¥ nhi·ªÖm kh√¥ng kh√≠ c·∫•p qu·ªëc
gia; nh·∫≠n ƒë·ªãnh c√°c nguy√™n nh√¢n ch√≠nh g√¢y √¥ nhi·ªÖm m√¥i tr∆∞·ªùng kh√¥ng kh√≠:
a) Hi·ªán tr·∫°ng, di·ªÖn bi·∫øn ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ qu·ªëc gia
trong giai ƒëo·∫°n t·ªëi thi·ªÉu 03 nƒÉm g·∫ßn nh·∫•t; t·ªïng l∆∞·ª£ng ph√°t th·∫£i g√¢y √¥ nhi·ªÖm
m√¥i tr∆∞·ªùng kh√¥ng kh√≠ v√† ph√¢n b·ªë ph√°t th·∫£i theo kh√¥ng gian t·ª´ c√°c ngu·ªìn √¥
nhi·ªÖm ƒëi·ªÉm, ngu·ªìn √¥ nhi·ªÖm di ƒë·ªông, ngu·ªìn √¥ nhi·ªÖm di·ªán; ·∫£nh h∆∞·ªüng c·ªßa √¥
nhi·ªÖm m√¥i tr∆∞·ªùng kh√¥ng kh√≠ t·ªõi s·ª©c kh·ªèe c·ªông ƒë·ªìng;
b) K·∫øt qu·∫£ th·ª±c hi·ªán c√°c ch∆∞∆°ng tr√¨nh quan tr·∫Øc ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng
kh√¥ng kh√≠, c√°c tr·∫°m quan tr·∫Øc t·ª± ƒë·ªông, li√™n t·ª•c ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng
kh√≠ v√† kh√≠ th·∫£i c√¥ng nghi·ªáp; vi·ªác s·ª≠ d·ª•ng s·ªë li·ªáu quan tr·∫Øc ph·ª•c v·ª• c√¥ng t√°c
ƒë√°nh gi√° di·ªÖn bi·∫øn v√† qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ trong giai
ƒëo·∫°n t·ªëi thi·ªÉu 03 nƒÉm g·∫ßn nh·∫•t;
c) Hi·ªán tr·∫°ng c√¥ng t√°c qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ c·∫•p
qu·ªëc gia giai ƒëo·∫°n t·ªëi thi·ªÉu 03 nƒÉm g·∫ßn nh·∫•t; c√°c v·∫•n ƒë·ªÅ b·∫•t c·∫≠p, t·ªìn t·∫°i trong
c√¥ng t√°c qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠;
d) Nh·∫≠n ƒë·ªãnh c√°c nguy√™n nh√¢n ch√≠nh g√¢y √¥ nhi·ªÖm m√¥i tr∆∞·ªùng kh√¥ng kh√≠.
2. M·ª•c ti√™u qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠:
a) M·ª•c ti√™u t·ªïng th·ªÉ: tƒÉng c∆∞·ªùng hi·ªáu l·ª±c, hi·ªáu qu·∫£ qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng
m√¥i tr∆∞·ªùng kh√¥ng kh√≠ ph√π h·ª£p v·ªõi k·∫ø ho·∫°ch ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi, b·∫£o v·ªá
m√¥i tr∆∞·ªùng theo k·ª≥ k·∫ø ho·∫°ch;
b) M·ª•c ti√™u c·ª• th·ªÉ: ƒë·ªãnh l∆∞·ª£ng c√°c ch·ªâ ti√™u nh·∫±m gi·∫£m thi·ªÉu t·ªïng l∆∞·ª£ng
kh√≠ th·∫£i ph√°t sinh t·ª´ c√°c ngu·ªìn th·∫£i ch√≠nh; c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng
kh√¥ng kh√≠.
3. Nhi·ªám v·ª• v√† gi·∫£i ph√°p qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠:
a) V·ªÅ c∆° ch·∫ø, ch√≠nh s√°ch;
b) V·ªÅ khoa h·ªçc, c√¥ng ngh·ªá nh·∫±m c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng
kh√¥ng kh√≠;
c) V·ªÅ qu·∫£n l√Ω, ki·ªÉm so√°t ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠.
4. Ch∆∞∆°ng tr√¨nh, d·ª± √°n ∆∞u ti√™n ƒë·ªÉ th·ª±c hi·ªán c√°c nhi·ªám v·ª•, gi·∫£i ph√°p
quy ƒë·ªãnh t·∫°i kho·∫£n 3 ƒêi·ªÅu n√†y.
5. Quy ch·∫ø ph·ªëi h·ª£p, bi·ªán ph√°p qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng
kh√≠ li√™n v√πng, li√™n t·ªânh ph·∫£i th·ªÉ hi·ªán ƒë·∫ßy ƒë·ªß c√°c n·ªôi dung, bi·ªán ph√°p ph·ªëi h·ª£p
x·ª≠ l√Ω, qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠; tr√°ch nhi·ªám c·ªßa c√°c c∆° quan,
t·ªï ch·ª©c c√≥ li√™n quan trong c√¥ng t√°c qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠
li√™n v√πng, li√™n t·ªânh, thu th·∫≠p v√† b√°o c√°o, c√¥ng b·ªë th√¥ng tin trong tr∆∞·ªùng h·ª£p
ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ b·ªã √¥ nhi·ªÖm.
6. T·ªï ch·ª©c th·ª±c hi·ªán k·∫ø ho·∫°ch qu·ªëc gia v·ªÅ qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i
tr∆∞·ªùng kh√¥ng kh√≠, bao g·ªìm:
a) Ph√¢n c√¥ng tr√°ch nhi·ªám c·ªßa c∆° quan ch·ªß tr√¨ v√† c√°c c∆° quan ph·ªëi h·ª£p
trong vi·ªác th·ª±c hi·ªán k·∫ø ho·∫°ch;
b) C∆° ch·∫ø gi√°m s√°t, b√°o c√°o, ƒë√¥n ƒë·ªëc th·ª±c hi·ªán;
c) Danh m·ª•c c√°c ch∆∞∆°ng tr√¨nh, d·ª± √°n ∆∞u ti√™n ƒë·ªÉ th·ª±c hi·ªán c√°c nhi·ªám
v·ª•, gi·∫£i ph√°p c·ªßa k·∫ø ho·∫°ch;
d) C∆° ch·∫ø ph√¢n b·ªï ngu·ªìn l·ª±c th·ª±c hi·ªán."""
},
 {"ƒêi·ªÅu": "ƒêi·ªÅu 7. Tr√¨nh t·ª±, th·ªß t·ª•c ban h√†nh k·∫ø ho·∫°ch qu·ªëc gia v·ªÅ qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠",
  "Ch∆∞∆°ng": "Ch∆∞∆°ng II B·∫¢O V·ªÜ C√ÅC TH√ÄNH PH·∫¶N M√îI TR∆Ø·ªúNG V√Ä DI S·∫¢N THI√äN NHI√äN",
  "M·ª•c": "M·ª•c 2 B·∫¢O V·ªÜ M√îI TR∆Ø·ªúNG KH√îNG KH√ç",
  "Pages": "12",
  "Text": """1. K·∫ø ho·∫°ch qu·ªëc gia v·ªÅ qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ ƒë∆∞·ª£c
ban h√†nh theo quy ƒë·ªãnh sau:
a) B·ªô T√†i nguy√™n v√† M√¥i tr∆∞·ªùng ch·ªß tr√¨, ph·ªëi h·ª£p v·ªõi c√°c b·ªô, c∆° quan
ngang b·ªô, ·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh c√≥ li√™n quan t·ªï ch·ª©c l·∫≠p, ph√™ duy·ªát, tri·ªÉn
khai ƒë·ªÅ √°n ƒëi·ªÅu tra, ƒë√°nh gi√°, x√¢y d·ª±ng d·ª± th·∫£o k·∫ø ho·∫°ch qu·ªëc gia v·ªÅ qu·∫£n l√Ω
ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠;
b) B·ªô T√†i nguy√™n v√† M√¥i tr∆∞·ªùng g·ª≠i d·ª± th·∫£o k·∫ø ho·∫°ch qu·ªëc gia v·ªÅ qu·∫£n
l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ ƒë·∫øn ·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh v√† c√°c b·ªô,
c∆° quan ngang b·ªô c√≥ li√™n quan ƒë·ªÉ l·∫•y √Ω ki·∫øn g√≥p √Ω b·∫±ng vƒÉn b·∫£n; nghi√™n c·ª©u,
ti·∫øp thu, gi·∫£i tr√¨nh c√°c √Ω ki·∫øn g√≥p √Ω, ho√†n thi·ªán d·ª± th·∫£o k·∫ø ho·∫°ch, tr√¨nh Th·ªß
t∆∞·ªõng Ch√≠nh ph·ªß xem x√©t, ban h√†nh. H·ªì s∆° tr√¨nh Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß bao
g·ªìm: t·ªù tr√¨nh, d·ª± th·∫£o k·∫ø ho·∫°ch, d·ª± th·∫£o quy·∫øt ƒë·ªãnh ban h√†nh k·∫ø ho·∫°ch; b√°o
c√°o t·ªïng h·ª£p, gi·∫£i tr√¨nh ti·∫øp thu d·ª± th·∫£o k·∫ø ho·∫°ch; vƒÉn b·∫£n g√≥p √Ω c·ªßa c√°c c∆°
quan c√≥ li√™n quan.
2. K·∫ø ho·∫°ch qu·ªëc gia v·ªÅ qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ ph·∫£i
ph√π h·ª£p v·ªõi quy ho·∫°ch b·∫£o v·ªá m√¥i tr∆∞·ªùng qu·ªëc gia. Tr∆∞·ªùng h·ª£p quy ho·∫°ch
b·∫£o v·ªá m√¥i tr∆∞·ªùng qu·ªëc gia ch∆∞a ƒë∆∞·ª£c ban h√†nh, k·∫ø ho·∫°ch qu·ªëc gia v·ªÅ qu·∫£n
l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ ph·∫£i ph√π h·ª£p v·ªõi y√™u c·∫ßu qu·∫£n l√Ω nh√†
n∆∞·ªõc v·ªÅ b·∫£o v·ªá m√¥i tr∆∞·ªùng v√† ph·∫£i ƒë∆∞·ª£c r√† so√°t, c·∫≠p nh·∫≠t ph√π h·ª£p v·ªõi quy
ho·∫°ch b·∫£o v·ªá m√¥i tr∆∞·ªùng qu·ªëc gia khi ƒë∆∞·ª£c ban h√†nh.
3. K·∫ø ho·∫°ch qu·ªëc gia v·ªÅ qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ ƒë∆∞·ª£c
x√¢y d·ª±ng ph√π h·ª£p v·ªõi k·∫ø ho·∫°ch ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi 05 nƒÉm. Tr∆∞·ªõc ng√†y
30 th√°ng 6 nƒÉm th·ª© t∆∞ c·ªßa k·∫ø ho·∫°ch ƒë·∫ßu t∆∞ c√¥ng trung h·∫°n giai ƒëo·∫°n tr∆∞·ªõc, c∆°
quan ph√™ duy·ªát k·∫ø ho·∫°ch ch·ªâ ƒë·∫°o t·ªï ch·ª©c t·ªïng k·∫øt, ƒë√°nh gi√° vi·ªác th·ª±c hi·ªán k·∫ø
ho·∫°ch k·ª≥ tr∆∞·ªõc, x√¢y d·ª±ng, ph√™ duy·ªát k·∫ø ho·∫°ch cho giai ƒëo·∫°n ti·∫øp theo ƒë·ªÉ l√†m
c∆° s·ªü ƒë·ªÅ xu·∫•t k·∫ø ho·∫°ch ƒë·∫ßu t∆∞ c√¥ng trung h·∫°n."""
},
{"ƒêi·ªÅu": "ƒêi·ªÅu 8. N·ªôi dung k·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ c·∫•p t·ªânh",
  "Ch∆∞∆°ng": "Ch∆∞∆°ng II B·∫¢O V·ªÜ C√ÅC TH√ÄNH PH·∫¶N M√îI TR∆Ø·ªúNG V√Ä DI S·∫¢N THI√äN NHI√äN",
  "M·ª•c": "M·ª•c 2 B·∫¢O V·ªÜ M√îI TR∆Ø·ªúNG KH√îNG KH√ç",
  "Pages": "12,13",
  "Text": """N·ªôi dung ch√≠nh c·ªßa k·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠
c·∫•p t·ªânh ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i kho·∫£n 4 ƒêi·ªÅu 13 Lu·∫≠t B·∫£o v·ªá m√¥i tr∆∞·ªùng. M·ªôt s·ªë
n·ªôi dung ƒë∆∞·ª£c quy ƒë·ªãnh chi ti·∫øt nh∆∞ sau:
1. V·ªÅ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ ·ªü ƒë·ªãa ph∆∞∆°ng: hi·ªán
tr·∫°ng ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ khu v·ª±c ƒë√¥ th·ªã, n√¥ng th√¥n v√† c√°c khu
v·ª±c kh√°c.
2. V·ªÅ ƒë√°nh gi√° c√¥ng t√°c qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠; quan
tr·∫Øc m√¥i tr∆∞·ªùng kh√¥ng kh√≠; x√°c ƒë·ªãnh v√† ƒë√°nh gi√° c√°c ngu·ªìn ph√°t th·∫£i kh√≠ th·∫£i
ch√≠nh; ki·ªÉm k√™ ph√°t th·∫£i; m√¥ h√¨nh h√≥a ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠; th·ª±c
tr·∫°ng v√† hi·ªáu qu·∫£ c·ªßa c√°c gi·∫£i ph√°p qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ ƒëang th·ª±c
hi·ªán; hi·ªán tr·∫°ng c√°c ch∆∞∆°ng tr√¨nh, h·ªá th·ªëng quan tr·∫Øc; t·ªïng h·ª£p, x√°c ƒë·ªãnh,
ƒë√°nh gi√° c√°c ngu·ªìn ph√°t th·∫£i ch√≠nh (ngu·ªìn √¥ nhi·ªÖm ƒëi·ªÉm, ngu·ªìn √¥ nhi·ªÖm di
ƒë·ªông, ngu·ªìn √¥ nhi·ªÖm di·ªán); th·ª±c hi·ªán ki·ªÉm k√™ c√°c ngu·ªìn ph√°t th·∫£i ch√≠nh v√†
m√¥ h√¨nh h√≥a ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠.
3. Ph√¢n t√≠ch, nh·∫≠n ƒë·ªãnh nguy√™n nh√¢n g√¢y √¥ nhi·ªÖm m√¥i tr∆∞·ªùng kh√¥ng
kh√≠: nguy√™n nh√¢n kh√°ch quan t·ª´ c√°c y·∫øu t·ªë kh√≠ t∆∞·ª£ng, th·ªùi ti·∫øt, kh√≠ h·∫≠u theo
m√πa, c√°c v·∫•n ƒë·ªÅ √¥ nhi·ªÖm li√™n t·ªânh, xuy√™n bi√™n gi·ªõi (n·∫øu c√≥); nguy√™n nh√¢n ch·ªß
quan t·ª´ ho·∫°t ƒë·ªông ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi l√†m ph√°t sinh c√°c ngu·ªìn kh√≠ th·∫£i
g√¢y √¥ nhi·ªÖm kh√¥ng kh√≠ (ngu·ªìn √¥ nhi·ªÖm ƒëi·ªÉm, ngu·ªìn √¥ nhi·ªÖm di ƒë·ªông,
ngu·ªìn √¥ nhi·ªÖm di·ªán).
4 V·ªÅ ƒë√°nh gi√° ·∫£nh h∆∞·ªüng c·ªßa √¥ nhi·ªÖm kh√¥ng kh√≠ ƒë·∫øn s·ª©c kh·ªèe c·ªông ƒë·ªìng:
th√¥ng tin, s·ªë li·ªáu v·ªÅ s·ªë ca b·ªánh do ·∫£nh h∆∞·ªüng c·ªßa √¥ nhi·ªÖm kh√¥ng kh√≠ (n·∫øu c√≥); k·∫øt
qu·∫£ ƒë√°nh gi√° ·∫£nh h∆∞·ªüng c·ªßa √¥ nhi·ªÖm kh√¥ng kh√≠ t·ªõi s·ª©c kh·ªèe ng∆∞·ªùi d√¢n t·∫°i ƒë·ªãa
ph∆∞∆°ng.
5. M·ª•c ti√™u v√† ph·∫°m vi qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠: hi·ªán
tr·∫°ng v√† di·ªÖn bi·∫øn ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠, hi·ªán tr·∫°ng c√¥ng t√°c qu·∫£n
l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ ·ªü ƒë·ªãa ph∆∞∆°ng.
6. Nhi·ªám v·ª• v√† gi·∫£i ph√°p qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠:
a) V·ªÅ c∆° ch·∫ø, ch√≠nh s√°ch;
b) V·ªÅ khoa h·ªçc, c√¥ng ngh·ªá nh·∫±m c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng
kh√¥ng kh√≠;
c) V·ªÅ qu·∫£n l√Ω, ki·ªÉm so√°t ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠.
7. T·ªï ch·ª©c th·ª±c hi·ªán k·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng
kh√≠ c·∫•p t·ªânh, bao g·ªìm:
a) Ph√¢n c√¥ng tr√°ch nhi·ªám c·ªßa c∆° quan ch·ªß tr√¨ v√† c√°c c∆° quan ph·ªëi h·ª£p
trong vi·ªác th·ª±c hi·ªán k·∫ø ho·∫°ch;
b) C∆° ch·∫ø gi√°m s√°t, b√°o c√°o, ƒë√¥n ƒë·ªëc th·ª±c hi·ªán;
c) C∆° ch·∫ø ph√¢n b·ªï ngu·ªìn l·ª±c th·ª±c hi·ªán.
8. ·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh t·ªï ch·ª©c x√¢y d·ª±ng k·∫ø ho·∫°ch qu·∫£n l√Ω ch·∫•t
l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠ c·∫•p t·ªânh theo h∆∞·ªõng d·∫´n k·ªπ thu·∫≠t c·ªßa B·ªô T√†i
nguy√™n v√† M√¥i tr∆∞·ªùng."""
    },

    ]
}

import json
import re
from typing import Dict, Any, Optional


def extract_number(prefix: str, text: str) -> Optional[int]:
    """Generic function to extract number or Roman numeral after prefix (e.g., ƒêi·ªÅu 1, Ch∆∞∆°ng II, M·ª•c 3)"""
    match = re.search(fr'{prefix}\s+([IVXLCDM\d]+)', text, re.IGNORECASE)
    if not match:
        return None

    value = match.group(1).strip().upper()

    # Roman numeral lookup (extend as needed)
    roman_to_int = {
        'I': 1, 'II': 2, 'III': 3, 'IV': 4, 'V': 5,
        'VI': 6, 'VII': 7, 'VIII': 8, 'IX': 9, 'X': 10,
        'XI': 11, 'XII': 12, 'XIII': 13, 'XIV': 14, 'XV': 15,
        'XVI': 16, 'XVII': 17, 'XVIII': 18, 'XIX': 19, 'XX': 20
    }

    # Prioritize Roman numeral check
    if value in roman_to_int:
        return roman_to_int[value]

    # Otherwise, try integer conversion
    if value.isdigit():
        return int(value)

    # Fallback if neither
    return None

def extract_content(prefix: str, text: str) -> str:
    """
    Extract clean content after 'Ch∆∞∆°ng X' or 'M·ª•c 1' etc.
    Removes the prefix and number even if there is no dot.
    """
    # Remove prefix and number part (e.g., 'Ch∆∞∆°ng II', 'M·ª•c 1', 'ƒêi·ªÅu 3')
    content = re.sub(fr'^{prefix}\s+[IVXLCDM\d]+\.?\s*', '', text.strip(), flags=re.IGNORECASE)
    return content.strip()

def transform_data(input_data: Any) -> Dict[str, Any]:
    """Transform JSON data by splitting ƒêi·ªÅu, Ch∆∞∆°ng, and M·ª•c"""
    # Handle different input types
    if isinstance(input_data, str):
        try:
            data = json.loads(input_data)
        except json.JSONDecodeError:
            with open(input_data, 'r', encoding='utf-8') as f:
                data = json.load(f)
    else:
        data = input_data

    # Transform the meta array
    if 'meta' in data and isinstance(data['meta'], list):
        transformed_meta = []

        for item in data['meta']:
            transformed_item = {}

            # --- ƒêi·ªÅu ---
            if 'ƒêi·ªÅu' in item:
                transformed_item['ƒêi·ªÅu'] = extract_number('ƒêi·ªÅu', item['ƒêi·ªÅu'])
                transformed_item['ƒêi·ªÅu_Content'] = extract_content('ƒêi·ªÅu', item['ƒêi·ªÅu'])

            # --- Ch∆∞∆°ng ---
            if 'Ch∆∞∆°ng' in item:
                transformed_item['Ch∆∞∆°ng'] = extract_number('Ch∆∞∆°ng', item['Ch∆∞∆°ng'])
                transformed_item['Ch∆∞∆°ng_Content'] = extract_content('Ch∆∞∆°ng', item['Ch∆∞∆°ng'])

            # --- M·ª•c ---
            if 'M·ª•c' in item:
                transformed_item['M·ª•c'] = extract_number('M·ª•c', item['M·ª•c'])
                transformed_item['M·ª•c_Content'] = extract_content('M·ª•c', item['M·ª•c'])

            # Other fields
            if 'Pages' in item:
                transformed_item['Pages'] = item['Pages']
            if 'Text' in item:
                transformed_item['Text'] = item['Text']

            transformed_meta.append(transformed_item)

        data['meta'] = transformed_meta

    return data


def save_transformed_data(output_path: str, transformed_data: Dict[str, Any], indent: int = 2):
    """Save transformed data to JSON file"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(transformed_data, f, ensure_ascii=False, indent=indent)


# Transform the legal data
print("üìä Transforming legal document data...")
transformed_data = transform_data(data)
print(f"‚úÖ Transformed {len(transformed_data.get('meta', []))} legal articles")

# Example usage (for standalone testing)
# if __name__ == "__main__":
#     print("=== TRANSFORMED JSON ===")
#     print(json.dumps(transformed_data, ensure_ascii=False, indent=2))
#
#     if transformed_data.get('meta'):
#         first = transformed_data['meta'][0]
#         print("\n=== EXAMPLE ACCESS ===")
#         print(f"ƒêi·ªÅu: {first.get('ƒêi·ªÅu')} - {first.get('ƒêi·ªÅu_Content')}")
#         print(f"Ch∆∞∆°ng: {first.get('Ch∆∞∆°ng')} - {first.get('Ch∆∞∆°ng_Content')}")
#         print(f"M·ª•c: {first.get('M·ª•c')} - {first.get('M·ª•c_Content')}")


from langchain_core.documents import Document
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# --- Prompt template ---
query_str = """H√£y cung c·∫•p m·ªôt b·∫£n t√≥m t·∫Øt chi ti·∫øt b·∫±ng ti·∫øng Vi·ªát v·ªÅ quy ƒë·ªãnh ph√°p lu·∫≠t Vi·ªát Nam n√†y, bao g·ªìm:
- C√°c y√™u c·∫ßu ho·∫∑c quy ƒë·ªãnh ph√°p l√Ω ch√≠nh ƒë∆∞·ª£c n√™u ra
- Nh·ªØng c√° nh√¢n, t·ªï ch·ª©c ho·∫∑c ƒë·ªëi t∆∞·ª£ng n√†o ch·ªãu s·ª± ƒëi·ªÅu ch·ªânh c·ªßa quy ƒë·ªãnh n√†y
- C√°c nghƒ©a v·ª•, quy·ªÅn ho·∫∑c th·ªß t·ª•c quan tr·ªçng ƒë∆∞·ª£c quy ƒë·ªãnh
- C√°c ƒëi·ªÅu ki·ªán, ngo·∫°i l·ªá ho·∫∑c y√™u c·∫ßu c·ª• th·ªÉ ƒë√°ng ch√∫ √Ω (n·∫øu c√≥)
- M·ª•c ƒë√≠ch ho·∫∑c ph·∫°m vi t·ªïng th·ªÉ c·ªßa quy ƒë·ªãnh ph√°p lu·∫≠t n√†y

Vui l√≤ng tr√¨nh b√†y b·∫£n t√≥m t·∫Øt th√†nh 3-4 ƒëo·∫°n vƒÉn b·∫±ng ti·∫øng Vi·ªát, b·∫£o ƒë·∫£m v·ª´a ƒë·∫ßy ƒë·ªß v·ª´a d·ªÖ ƒë·ªçc.
"""

# --- Convert transformed JSON into Document objects ---
docs = []
for item in transformed_data["meta"]:
    metadata = {
        "ƒêi·ªÅu": item.get("ƒêi·ªÅu", ""),
        "ƒêi·ªÅu_Name": item.get("ƒêi·ªÅu_Content", ""),
        "Ch∆∞∆°ng": item.get("Ch∆∞∆°ng", ""),
        "Ch∆∞∆°ng_Name": (item.get("Ch∆∞∆°ng_Content", "")).lower(),
        "M·ª•c": item.get("M·ª•c", ""),
        "M·ª•c_Name": (item.get("M·ª•c_Content", "")).lower(),
        "Pages": item.get("Pages", "")
    }

    doc = Document(
        page_content=item.get("Text", ""),
        metadata=metadata
    )
    docs.append(doc)

# --- Chain definition ---
chain = (
    {"doc": lambda x: x.page_content}
    | ChatPromptTemplate.from_template(f"{query_str}\n\nN·ªôi dung vƒÉn b·∫£n:\n\n{{doc}}")
    | ChatOpenAI(model="gpt-3.5-turbo", temperature=0.4, max_retries=1)
    | StrOutputParser()
)

# --- Batch process documents ---
print("üìÑ Generating document summaries...")
summaries = chain.batch(docs, {"max_concurrency": 5})
print(f"‚úÖ Generated {len(summaries)} document summaries")

# --- Display results (commented out for cleaner import) ---
# for doc, summary in zip(docs, summaries):
#     print(f"\n{'='*80}")
#     print(f"Ch∆∞∆°ng: {doc.metadata['Ch∆∞∆°ng']} - {doc.metadata['Ch∆∞∆°ng_Name']}")
#     print(f"M·ª•c: {doc.metadata['M·ª•c']} - {doc.metadata['M·ª•c_Name']}")
#     print(f"ƒêi·ªÅu: {doc.metadata['ƒêi·ªÅu']} - {doc.metadata['ƒêi·ªÅu_Name']}")
#     print(f"\nüìò T√≥m t·∫Øt chi ti·∫øt:\n{summary}")
#     print('='*80)


from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
import uuid

# --- Step 1: Normalize metadata field names ---
def normalize_metadata(meta: dict):
    rename_map = {
        "Ch∆∞∆°ng": "Chuong",
        "Ch∆∞∆°ng_Name": "Chuong_Name",
        "M·ª•c": "Muc",
        "M·ª•c_Name": "Muc_Name",
        "ƒêi·ªÅu": "Dieu",
        "ƒêi·ªÅu_Name": "Dieu_Name",
    }
    new_meta = {}
    for k, v in meta.items():
        new_key = rename_map.get(k, k)
        new_meta[new_key] = v
    return new_meta


# --- Step 2: Prepare summarized documents ---
vector_docs = []
for doc, summary in zip(docs, summaries):
    vector_doc = Document(
        page_content=summary,  # D√πng summary l√†m n·ªôi dung
        metadata=normalize_metadata(doc.metadata)  # ‚úÖ d√πng metadata ƒë√£ chu·∫©n h√≥a
    )
    vector_docs.append(vector_doc)


# --- Step 3: Initialize LLMs ---
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
llm_creative = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)


# --- Step 4: Initialize embeddings ---
# embeddings = OpenAIEmbeddings()

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# --- Step 5: Create Qdrant vector store (Cloud or Local) ---
if USE_QDRANT_CLOUD and QDRANT_CLOUD_URL and QDRANT_API_KEY:
    # Use Qdrant Cloud for law collection
    print("üì° Using Qdrant Cloud for law collection...")
    vectorstore_fix = QdrantVectorStore.from_existing_collection(
        embedding=embeddings,
        collection_name="law_collection",
        url=QDRANT_CLOUD_URL,
        api_key=QDRANT_API_KEY,
    )
    print("‚úÖ Connected to law_collection on Qdrant Cloud")
else:
    # Use local/in-memory Qdrant for law collection
    print("üìç Using local Qdrant for law collection...")
    vectorstore_fix = QdrantVectorStore.from_documents(
        documents=vector_docs,
        embedding=embeddings,
        collection_name="legal_documents",
        location=":memory:"  # In-memory mode (no server needed)
    )
    print("‚úÖ Qdrant vector store created successfully with", len(vector_docs), "documents.")

from langchain.chains.query_constructor.ir import Comparator, Operator
from langchain.retrievers.self_query.qdrant import QdrantTranslator

# --- M√¥ t·∫£ t·ªïng qu√°t v·ªÅ c·∫•u tr√∫c ---
mo_ta_van_ban = """VƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam c√≥ c·∫•u tr√∫c ph√¢n c·∫•p:
- ƒêI·ªÄU (Dieu): Quy ƒë·ªãnh chi ti·∫øt (v√≠ d·ª•: "ƒêi·ªÅu 9. Ph·∫°m vi ƒëi·ªÅu ch·ªânh")
- CH∆Ø∆†NG (Chuong): Ph·∫°m vi r·ªông nh·∫•t - LU√îN d√πng S·ªê LA M√É (v√≠ d·ª•: "Ch∆∞∆°ng I", "Ch∆∞∆°ng II", "Ch∆∞∆°ng III", "Ch∆∞∆°ng IV"...)
- M·ª§C (Muc): Ch·ªß ƒë·ªÅ c·ª• th·ªÉ - d√πng s·ªë ·∫¢ R·∫≠p (v√≠ d·ª•: "M·ª•c 1", "M·ª•c 2", "M·ª•c 3"...)

‚ö†Ô∏è QUAN TR·ªåNG - ƒê·ªãnh d·∫°ng Ch∆∞∆°ng:
- Ch∆∞∆°ng LU√îN d√πng S·ªê LA M√É: I, II, III, IV, V, VI, VII, VIII, IX, X, XI, XII, XIII
- V√ç D·ª§ CHUY·ªÇN ƒê·ªîI:
  * "ch∆∞∆°ng 1" ho·∫∑c "Ch∆∞∆°ng 1" ‚Üí "Ch∆∞∆°ng I"
  * "ch∆∞∆°ng 2" ho·∫∑c "Ch∆∞∆°ng 2" ‚Üí "Ch∆∞∆°ng II"
  * "ch∆∞∆°ng 3" ho·∫∑c "Ch∆∞∆°ng 3" ‚Üí "Ch∆∞∆°ng III"
  * "ch∆∞∆°ng 10" ho·∫∑c "Ch∆∞∆°ng 10" ‚Üí "Ch∆∞∆°ng X"
- Vi·∫øt hoa ch·ªØ 'C': "Ch∆∞∆°ng" (KH√îNG ph·∫£i "ch∆∞∆°ng")

Khi t√¨m ki·∫øm:
- S·ªê ƒêI·ªÄU (v√≠ d·ª•: "ƒêi·ªÅu 9") ‚Üí d√πng Dieu_Number v·ªõi eq: eq("Dieu_Number", 9)
- CH∆Ø∆†NG (v√≠ d·ª•: "ch∆∞∆°ng 2", "Ch∆∞∆°ng II") ‚Üí chuy·ªÉn sang S·ªê LA M√É V√Ä vi·∫øt hoa, d√πng LIKE: like("Chuong", "Ch∆∞∆°ng II")
- M·ª§C (v√≠ d·ª•: "m·ª•c 2", "M·ª•c 2") ‚Üí vi·∫øt hoa ch·ªØ 'M', d√πng LIKE: like("Muc", "M·ª•c 2")
- K·∫øt h·ª£p M·ª§C v√† CH∆Ø∆†NG ‚Üí d√πng AND: and(like("Muc", "M·ª•c 2"), like("Chuong", "Ch∆∞∆°ng II"))
"""

metadata_fields = [
    AttributeInfo(
        name="Dieu_Number",
        description="S·ªë ƒëi·ªÅu (integer, v√≠ d·ª•: 9 cho ƒêi·ªÅu 9)",
        type="integer",
    ),
    AttributeInfo(
        name="Dieu",
        description="T√™n ƒë·∫ßy ƒë·ªß c·ªßa ƒëi·ªÅu (v√≠ d·ª•: 'ƒêi·ªÅu 9. Ph·∫°m vi ƒëi·ªÅu ch·ªânh')",
        type="string",
    ),
    AttributeInfo(
        name="Chuong",
        description="T√™n ch∆∞∆°ng (v√≠ d·ª•: 'Ch∆∞∆°ng I. NH·ªÆNG QUY ƒê·ªäNH CHUNG')",
        type="string",
    ),
    AttributeInfo(
        name="Muc",
        description="T√™n m·ª•c (v√≠ d·ª•: 'M·ª•c 1 B·∫¢O V·ªÜ M√îI TR∆Ø·ªúNG N∆Ø·ªöC')",
        type="string",
    ),
]

# --- Kh·ªüi t·∫°o LLM ---
llm_query = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# --- T·∫°o prompt constructor v·ªõi allowed_operators ---
prompt_truy_van_phap_luat = get_query_constructor_prompt(
    mo_ta_van_ban,
    metadata_fields,
    allowed_comparators=[
        Comparator.EQ,
        Comparator.LT,
        Comparator.LTE,
        Comparator.GT,
        Comparator.GTE,
        Comparator.LIKE,
    ],
    allowed_operators=[Operator.AND, Operator.OR],  # Enable AND and OR
    examples=[
        # T√¨m theo s·ªë ƒëi·ªÅu
        ("ƒêi·ªÅu 6 quy ƒë·ªãnh g√¨?", {"query": "n·ªôi dung ƒëi·ªÅu 6", "filter": 'eq("Dieu_Number", 6)'}),
        ("Cho t√¥i h·ªèi v·ªÅ ƒêi·ªÅu 9?", {"query": "v·ªÅ ƒëi·ªÅu 9", "filter": 'eq("Dieu_Number", 9)'}),

        # T√¨m theo ch∆∞∆°ng (chuy·ªÉn ƒë·ªïi sang s·ªë La M√£)
        ("Ch∆∞∆°ng 1 quy ƒë·ªãnh g√¨?", {"query": "ch∆∞∆°ng 1", "filter": 'like("Chuong", "Ch∆∞∆°ng I")'}),
        ("Ch∆∞∆°ng 2 quy ƒë·ªãnh g√¨?", {"query": "ch∆∞∆°ng 2", "filter": 'like("Chuong", "Ch∆∞∆°ng II")'}),
        ("ch∆∞∆°ng II quy ƒë·ªãnh g√¨?", {"query": "ch∆∞∆°ng II", "filter": 'like("Chuong", "Ch∆∞∆°ng II")'}),
        ("Ch∆∞∆°ng III v·ªÅ g√¨?", {"query": "ch∆∞∆°ng III", "filter": 'like("Chuong", "Ch∆∞∆°ng III")'}),

        # T√¨m theo m·ª•c (vi·∫øt hoa ch·ªØ M)
        ("m·ª•c 1 v·ªÅ g√¨?", {"query": "m·ª•c 1", "filter": 'like("Muc", "M·ª•c 1")'}),
        ("M·ª•c 2 v·ªÅ g√¨?", {"query": "m·ª•c 2", "filter": 'like("Muc", "M·ª•c 2")'}),

        # K·∫øt h·ª£p m·ª•c v√† ch∆∞∆°ng (chuy·ªÉn ƒë·ªïi s·ªë sang La M√£, vi·∫øt hoa)
        ("M·ª•c 2 c·ªßa ch∆∞∆°ng 2 quy ƒë·ªãnh g√¨?", {"query": "m·ª•c 2 ch∆∞∆°ng 2", "filter": 'and(like("Muc", "M·ª•c 2"), like("Chuong", "Ch∆∞∆°ng II"))'}),
        ("Cho t√¥i h·ªèi v·ªÅ M·ª•c 1 c·ªßa ch∆∞∆°ng 1?", {"query": "m·ª•c 1 ch∆∞∆°ng 1", "filter": 'and(like("Muc", "M·ª•c 1"), like("Chuong", "Ch∆∞∆°ng I"))'}),
        ("M·ª•c 3 Ch∆∞∆°ng IV quy ƒë·ªãnh g√¨?", {"query": "m·ª•c 3 ch∆∞∆°ng IV", "filter": 'and(like("Muc", "M·ª•c 3"), like("Chuong", "Ch∆∞∆°ng IV"))'}),

        # T√¨m theo n·ªôi dung
        ("Quy ƒë·ªãnh v·ªÅ m√¥i tr∆∞·ªùng kh√¥ng kh√≠", {"query": "m√¥i tr∆∞·ªùng kh√¥ng kh√≠", "filter": 'like("Dieu", "m√¥i tr∆∞·ªùng kh√¥ng kh√≠")'}),
        ("Ch∆∞∆°ng n√†o v·ªÅ b·∫£o v·ªá m√¥i tr∆∞·ªùng", {"query": "b·∫£o v·ªá m√¥i tr∆∞·ªùng", "filter": 'like("Chuong", "b·∫£o v·ªá m√¥i tr∆∞·ªùng")'}),

        # Nhi·ªÅu ƒëi·ªÅu
        ("ƒêi·ªÅu 5 ho·∫∑c ƒêi·ªÅu 6", {"query": "ƒëi·ªÅu 5 ƒëi·ªÅu 6", "filter": 'or(eq("Dieu_Number", 5), eq("Dieu_Number", 6))'}),

        # Kh√¥ng c√≥ filter c·ª• th·ªÉ
        ("Tr√°ch nhi·ªám c·ªßa t·ªï ch·ª©c s·∫£n xu·∫•t", {"query": "tr√°ch nhi·ªám t·ªï ch·ª©c s·∫£n xu·∫•t", "filter": None}),
    ],
)

# --- Kh·ªüi t·∫°o parser ---
parser_phap_luat = StructuredQueryOutputParser.from_components(
    allowed_comparators=[
        Comparator.EQ,
        Comparator.LT,
        Comparator.LTE,
        Comparator.GT,
        Comparator.GTE,
        Comparator.LIKE,
    ],
    allowed_operators=[Operator.AND, Operator.OR],  # Enable AND and OR
)

# --- K·∫øt h·ª£p prompt v√† LLM ---
llm_constructor_phap_luat = prompt_truy_van_phap_luat | llm_query | parser_phap_luat

# --- T·∫°o SelfQueryRetriever ---
retriever_phap_luat = SelfQueryRetriever(
    query_constructor=llm_constructor_phap_luat,
    vectorstore=vectorstore_fix,
    structured_query_translator=QdrantTranslator(metadata_key="metadata"),
    verbose=True,
    search_kwargs={"k": 5}
)

print("‚úÖ SelfQueryRetriever ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!")

from langchain.retrievers.self_query.qdrant import QdrantTranslator
from qdrant_client.models import Filter

class FallbackLegalRetriever:
    """
    Retriever with fallback: if filtered search returns nothing, try without filter
    """

    def __init__(self, vectorstore, query_constructor, k=5):
        self.vectorstore = vectorstore
        self.query_constructor = query_constructor
        self.k = k
        # ‚úÖ Create translator to convert LangChain filters to Qdrant format
        self.translator = QdrantTranslator(metadata_key="metadata")

    def invoke(self, query: str):
        """Get documents with fallback strategy"""
        print(f"\n{'='*80}")
        print(f"üîç FALLBACK RETRIEVER")
        print(f"{'='*80}")
        print(f"Query: {query}")

        # Step 1: Construct structured query
        structured_query = self.query_constructor.invoke({"query": query})

        print(f"Structured query:")
        print(f"  Query: {structured_query.query}")
        print(f"  Filter: {structured_query.filter}")

        # Step 2: Try with filter first
        if structured_query.filter:
            print(f"\nüîç Searching WITH filter...")

            # ‚úÖ Translate LangChain filter to Qdrant filter
            try:
                result = self.translator.visit_structured_query(structured_query)

                # Extract filter from the result (it returns a tuple/dict)
                if isinstance(result, tuple):
                    # Result is (query, filter_dict)
                    _, filter_dict = result
                    qdrant_filter = filter_dict.get('filter') if isinstance(filter_dict, dict) else filter_dict
                elif isinstance(result, dict):
                    # Result is {'filter': Filter(...)}
                    qdrant_filter = result.get('filter', result)
                else:
                    # Result is directly the filter
                    qdrant_filter = result

                print(f"   Using Qdrant filter: {qdrant_filter}")

                docs = self.vectorstore.similarity_search(
                    structured_query.query,
                    k=self.k,
                    filter=qdrant_filter  # ‚úÖ Use translated filter
                )

                if docs:
                    print(f"‚úÖ Found {len(docs)} documents with filter")
                    print(f"{'='*80}\n")
                    return docs
                else:
                    print(f"‚ö†Ô∏è  No results with filter, trying without filter...")
            except Exception as e:
                print(f"‚ö†Ô∏è  Error with filter: {e}")
                print(f"   Trying without filter...")

        # Step 3: Fallback to search without filter
        print(f"\nüîç Searching WITHOUT filter...")
        docs = self.vectorstore.similarity_search(
            structured_query.query,
            k=self.k
        )

        print(f"‚úÖ Found {len(docs)} documents without filter")
        print(f"{'='*80}\n")

        return docs


# ‚úÖ Create fallback retriever
fallback_retriever = FallbackLegalRetriever(
    vectorstore=vectorstore_fix,
    query_constructor=llm_constructor_phap_luat,
    k=5
)

print("‚úÖ Fallback retriever created!")

# Test (commented out for module import)
# query = "N√≥i r√µ c√°c ƒëi·ªÅu v·ªÅ t√°i ch·∫ø trong Lu·∫≠t B·∫£o v·ªá m√¥i tr∆∞·ªùng ra?"
# results = fallback_retriever.invoke(query)
# print(f"\nüìä RESULTS: {len(results)} documents")
# for i, doc in enumerate(results, 1):
#     print(f"\n{i}. ƒêi·ªÅu {doc.metadata.get('Dieu')}: {doc.metadata.get('Dieu_Name')}")
#     print(f"   Content: {doc.page_content[:100]}...")


from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate

# Data model
class GradeDocuments(BaseModel):
    """ƒê√°nh gi√° nh·ªã ph√¢n v·ªÅ m·ª©c ƒë·ªô li√™n quan c·ªßa t√†i li·ªáu ƒë√£ truy xu·∫•t."""

    binary_score: str = Field(
        description="T√†i li·ªáu c√≥ li√™n quan ƒë·∫øn c√¢u h·ªèi hay kh√¥ng, 'c√≥' ho·∫∑c 'kh√¥ng'"
    )

# LLM with function call
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_llm_grader = llm.with_structured_output(GradeDocuments)

# Enhanced Prompt
system = """B·∫°n l√† b·ªô ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan c·ªßa t√†i li·ªáu ƒë∆∞·ª£c truy xu·∫•t ƒë·ªëi v·ªõi c√¢u h·ªèi ng∆∞·ªùi d√πng.

üéØ M·ª§C TI√äU:
X√°c ƒë·ªãnh xem t√†i li·ªáu c√≥ th·ªÉ GI√öP TR·∫¢ L·ªúI c√¢u h·ªèi hay kh√¥ng (k·ªÉ c·∫£ khi c√¢u tr·∫£ l·ªùi l√† "KH√îNG").

üìã QUY T·∫ÆC ƒê√ÅNH GI√Å:

‚úÖ ƒê√ÅNH GI√Å "C√ì" (t√†i li·ªáu LI√äN QUAN) KHI:

1. **C√¢u h·ªèi v·ªÅ ƒêi·ªÅu/Ch∆∞∆°ng/M·ª•c c·ª• th·ªÉ**
   - C√¢u h·ªèi: "ƒêi·ªÅu 7 c√≥ n√≥i v·ªÅ X kh√¥ng?"
   - T√†i li·ªáu: Ch·ª©a th√¥ng tin v·ªÅ ƒêi·ªÅu 7
   - ‚Üí "C√ì" (d√π t√†i li·ªáu kh√¥ng nh·∫Øc ƒë·∫øn X, v√¨ c√≥ th·ªÉ tr·∫£ l·ªùi "KH√îNG")

2. **C√¢u h·ªèi v·ªÅ ch·ªß ƒë·ªÅ**
   - C√¢u h·ªèi: "Quy ƒë·ªãnh v·ªÅ t√°i ch·∫ø l√† g√¨?"
   - T√†i li·ªáu: Ch·ª©a th√¥ng tin v·ªÅ t√°i ch·∫ø
   - ‚Üí "C√ì"

3. **T·ª´ kh√≥a ho·∫∑c ng·ªØ nghƒ©a li√™n quan**
   - C√¢u h·ªèi: "Tr√°ch nhi·ªám c·ªßa nh√† s·∫£n xu·∫•t?"
   - T√†i li·ªáu: N√≥i v·ªÅ tr√°ch nhi·ªám s·∫£n xu·∫•t, EPR
   - ‚Üí "C√ì"

‚ùå ƒê√ÅNH GI√Å "KH√îNG" (t√†i li·ªáu KH√îNG LI√äN QUAN) CH·ªà KHI:

1. **Sai ho√†n to√†n ƒêi·ªÅu/Ch∆∞∆°ng/M·ª•c**
   - C√¢u h·ªèi: "ƒêi·ªÅu 7 n√≥i g√¨?"
   - T√†i li·ªáu: Ch·ªâ v·ªÅ ƒêi·ªÅu 99
   - ‚Üí "KH√îNG"

2. **Ch·ªß ƒë·ªÅ ho√†n to√†n kh√°c**
   - C√¢u h·ªèi: "Quy ƒë·ªãnh v·ªÅ t√°i ch·∫ø?"
   - T√†i li·ªáu: Ch·ªâ v·ªÅ x√¢y d·ª±ng, y t·∫ø, kh√¥ng li√™n quan m√¥i tr∆∞·ªùng
   - ‚Üí "KH√îNG"

üîç TR∆Ø·ªúNG H·ª¢P ƒê·∫∂C BI·ªÜT:

**C√¢u h·ªèi d·∫°ng "ƒêi·ªÅu X c√≥ n√≥i v·ªÅ Y kh√¥ng?"**
- N·∫øu t√†i li·ªáu C√ì ƒêi·ªÅu X ‚Üí "C√ì" (v√¨ c√≥ th·ªÉ tr·∫£ l·ªùi "c√≥" ho·∫∑c "kh√¥ng")
- N·∫øu t√†i li·ªáu KH√îNG C√ì ƒêi·ªÅu X ‚Üí "KH√îNG"

V√ç D·ª§:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

C√¢u h·ªèi: "ƒêi·ªÅu 7 c√≥ n√≥i v·ªÅ l·ªëp xe kh√¥ng?"
T√†i li·ªáu: [Metadata: ƒêi·ªÅu 7, N·ªôi dung: Quy ƒë·ªãnh v·ªÅ ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠...]
‚Üí "C√ì" ‚úÖ (V√¨ c√≥ ƒêi·ªÅu 7, c√≥ th·ªÉ tr·∫£ l·ªùi "KH√îNG, ƒêi·ªÅu 7 kh√¥ng n√≥i v·ªÅ l·ªëp xe")

C√¢u h·ªèi: "ƒêi·ªÅu 7 c√≥ n√≥i v·ªÅ l·ªëp xe kh√¥ng?"
T√†i li·ªáu: [Metadata: ƒêi·ªÅu 99, N·ªôi dung: Quy ƒë·ªãnh v·ªÅ...]
‚Üí "KH√îNG" ‚ùå (V√¨ t√†i li·ªáu kh√¥ng ph·∫£i ƒêi·ªÅu 7)

C√¢u h·ªèi: "Quy ƒë·ªãnh v·ªÅ t√°i ch·∫ø?"
T√†i li·ªáu: [N·ªôi dung: Tr√°ch nhi·ªám t√°i ch·∫ø s·∫£n ph·∫©m...]
‚Üí "C√ì" ‚úÖ

C√¢u h·ªèi: "Quy ƒë·ªãnh v·ªÅ t√°i ch·∫ø?"
T√†i li·ªáu: [N·ªôi dung: Quy ƒë·ªãnh v·ªÅ x√¢y d·ª±ng nh√† ·ªü...]
‚Üí "KH√îNG" ‚ùå

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚öñÔ∏è NGUY√äN T·∫ÆC:
M·ª•c ti√™u l√† GI·ªÆ L·∫†I t√†i li·ªáu c√≥ th·ªÉ gi√∫p tr·∫£ l·ªùi (k·ªÉ c·∫£ tr·∫£ l·ªùi "kh√¥ng").
Ch·ªâ lo·∫°i b·ªè t√†i li·ªáu HO√ÄN TO√ÄN KH√îNG LI√äN QUAN.

H√£y ƒë∆∞a ra ƒëi·ªÉm nh·ªã ph√¢n: 'c√≥' ho·∫∑c 'kh√¥ng'"""

grade_prompt = ChatPromptTemplate.from_messages([
    ("system", system),
    ("human", """T√†i li·ªáu ƒë√£ truy xu·∫•t:
{document}

C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:
{question}

T√†i li·ªáu n√†y c√≥ li√™n quan ƒë·∫øn c√¢u h·ªèi kh√¥ng? ('c√≥' ho·∫∑c 'kh√¥ng')"""),
])

retrieval_grader = grade_prompt | structured_llm_grader



def grade_documents(state):
    """
    X√°c ƒë·ªãnh xem c√°c t√†i li·ªáu ƒë√£ truy xu·∫•t c√≥ li√™n quan ƒë·∫øn c√¢u h·ªèi hay kh√¥ng.

    Args:
        state (dict): Tr·∫°ng th√°i hi·ªán t·∫°i c·ªßa ƒë·ªì th·ªã

    Returns:
        state (dict): C·∫≠p nh·∫≠t kh√≥a documents ch·ªâ v·ªõi c√°c t√†i li·ªáu li√™n quan ƒë√£ ƒë∆∞·ª£c l·ªçc
    """

    print("---CHECK DOCUMENT RELEVANCE TO QUESTION---")
    question = state["question"]
    documents = state["documents"]

    # Score each doc
    filtered_docs = []
    for d in documents:
        # Combine metadata with content
        doc_txt_with_metadata = f"""
Metadata:
- ƒêi·ªÅu {d.metadata.get('Dieu', 'N/A')}: {d.metadata.get('Dieu_Name', '')}
- Ch∆∞∆°ng {d.metadata.get('Chuong', 'N/A')}: {d.metadata.get('Chuong_Name', '')}
- M·ª•c {d.metadata.get('Muc', 'N/A')}: {d.metadata.get('Muc_Name', '')}

N·ªôi dung:
{d.page_content}
"""

        score = retrieval_grader.invoke({"question": question, "document": doc_txt_with_metadata})
        grade = score.binary_score
        if grade == "c√≥":
            print("---GRADE: DOCUMENT RELEVANT---")
            filtered_docs.append(d)
        else:
            print("---GRADE: DOCUMENT NOT RELEVANT---")
            continue

    return {"documents": filtered_docs, "question": question}

# ========== ROUTE QUERY MODEL FOR LEGAL DOCUMENTS ==========
class LegalRouteQuery(BaseModel):
    """Ph√¢n lo·∫°i c√¢u h·ªèi ng∆∞·ªùi d√πng t·ªõi ngu·ªìn d·ªØ li·ªáu ph√π h·ª£p"""
    datasource: Literal["vectorstore","chitchat"] = Field(
        ...,
        description="vectorstore (vƒÉn b·∫£n ph√°p lu·∫≠t), chitchat (giao ti·∫øp th√¢n thi·ªán)"
    )

# ========== INITIALIZE LLM ROUTER ==========
llm_router = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
structured_llm_router = llm_router.with_structured_output(LegalRouteQuery)

# ========== SYSTEM PROMPT ==========
router_system = """B·∫°n l√† m·ªôt chuy√™n gia ph√¢n lo·∫°i c√¢u h·ªèi ng∆∞·ªùi d√πng t·ªõi ngu·ªìn d·ªØ li·ªáu ph√π h·ª£p.

B·∫°n c√≥ quy·ªÅn truy c·∫≠p 2 ngu·ªìn:
1. **vectorstore** - VƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam (lu·∫≠t, ngh·ªã ƒë·ªãnh, ƒëi·ªÅu kho·∫£n)
q. **chitchat** - Giao ti·∫øp th√¢n thi·ªán, h·ªèi thƒÉm, c·∫£m ∆°n, ch√†o h·ªèi

## QUY T·∫ÆC ∆ØU TI√äN QUAN TR·ªåNG (ki·ªÉm tra theo th·ª© t·ª±):

### 1. Chuy·ªÉn t·ªõi **chitchat** n·∫øu:
- L·ªùi ch√†o: "Xin ch√†o", "Ch√†o b·∫°n", "Hi", "Good morning"
- Gi·ªõi thi·ªáu: "T√¥i t√™n l√†...", "M√¨nh l√†..."
- C·∫£m ∆°n: "C·∫£m ∆°n", "Thanks"
- T·∫°m bi·ªát: "T·∫°m bi·ªát", "Bye", "Goodbye"
- H·ªèi thƒÉm / n√≥i chuy·ªán th√¢n thi·ªán: "B·∫°n c√≥ kh·ªèe kh√¥ng?", "H√¥m nay th·∫ø n√†o?"
- C√°c c√¢u h·ªèi v·ªÅ tr·ª£ l√Ω: "B·∫°n nh·ªõ t√¥i kh√¥ng?", "T√™n t√¥i l√† g√¨?"
- **L∆∞u √Ω:** N·∫øu c√¢u b·∫Øt ƒë·∫ßu b·∫±ng l·ªùi ch√†o, lu√¥n l√† chitchat, ngay c·∫£ khi c√≥ nh·∫Øc ƒë·∫øn lu·∫≠t.

### 2. Chuy·ªÉn t·ªõi **vectorstore** n·∫øu kh√¥ng ph·∫£i chitchat v√†:
- H·ªèi v·ªÅ lu·∫≠t / ƒëi·ªÅu kho·∫£n c·ª• th·ªÉ: "ƒêi·ªÅu ki·ªán c·∫•p gi·∫•y ph√©p m√¥i tr∆∞·ªùng", "Quy·ªÅn v√† nghƒ©a v·ª• c·ªßa t·ªï ch·ª©c s·∫£n xu·∫•t"
- Tra c·ª©u n·ªôi dung ƒêi·ªÅu / M·ª•c / Ch∆∞∆°ng
- So s√°nh quy ƒë·ªãnh: "So s√°nh ƒêi·ªÅu 5 v√† ƒêi·ªÅu 6 c·ªßa Lu·∫≠t BVMT"
- Ph·∫°m vi √°p d·ª•ng: "Ph·∫°m vi √°p d·ª•ng c·ªßa Lu·∫≠t BVMT l√† g√¨?"
- Y√™u c·∫ßu t√≥m t·∫Øt ho·∫∑c gi·∫£i th√≠ch vƒÉn b·∫£n ph√°p lu·∫≠t


## V√≠ d·ª•:

"Xin ch√†o! T√¥i mu·ªën h·ªèi v·ªÅ lu·∫≠t m√¥i tr∆∞·ªùng" ‚Üí **chitchat** (l·ªùi ch√†o + c√¢u h·ªèi)
"C·∫£m ∆°n b·∫°n ƒë√£ gi√∫p t√¥i!" ‚Üí **chitchat**
"ƒêi·ªÅu ki·ªán c·∫•p gi·∫•y ph√©p m√¥i tr∆∞·ªùng l√† g√¨?" ‚Üí **vectorstore**
"Quy·ªÅn v√† nghƒ©a v·ª• c·ªßa doanh nghi·ªáp v·ªÅ ch·∫•t th·∫£i?" ‚Üí **vectorstore**



## C√¢u h·ªèi hi·ªán t·∫°i:
{question}

Ph√¢n lo·∫°i c√¢u h·ªèi d·ª±a tr√™n quy t·∫Øc ∆∞u ti√™n tr√™n."""

# ========== CREATE ROUTER PROMPT ==========
route_prompt = ChatPromptTemplate.from_messages([
    ("system", router_system),
    ("human", "{question}")
])

# ========== COMBINE PROMPT WITH STRUCTURED LLM ==========
question_router = route_prompt | structured_llm_router

print("‚úì Legal question router created successfully!")

def route_question_law(state):
    """Ph√¢n lu·ªìng c√¢u h·ªèi v·ªõi x·ª≠ l√Ω ng·ªØ c·∫£nh c·∫£i ti·∫øn"""
    print("---PH√ÇN LU·ªíNG C√ÇU H·ªéI (V·ªöI NG·ªÆ C·∫¢NH)---")

    question = state["question"]
    # L·∫•y l·ªãch s·ª≠ h·ªôi tho·∫°i ƒë·∫ßy ƒë·ªß
    chat_history = get_full_chat_history()  # h√†m b·∫°n ƒë√£ ƒë·ªãnh nghƒ©a ƒë·ªÉ load memory

    print(f"L·ªãch s·ª≠ h·ªôi tho·∫°i:\n{chat_history}\n")
    print(f"C√¢u h·ªèi hi·ªán t·∫°i: {question}")

    # G·ªçi LLM router ƒë·ªÉ quy·∫øt ƒë·ªãnh ngu·ªìn d·ªØ li·ªáu
    source = question_router.invoke({
        "question": question,
        "chat_history": chat_history
    })

    # L·∫•y datasource
    if isinstance(source, dict):
        datasource = source.get("datasource")
    else:
        datasource = getattr(source, "datasource", None)

    print(f"---PH√ÇN LU·ªíNG T·ªöI: {datasource.upper() if datasource else 'UNKNOWN'}---")

    # Map datasource sang c√°c h√†m c·ªßa pipeline ph√°p lu·∫≠t
    if datasource == 'vectorstore':
        return "vectorstore"  # Truy xu·∫•t ƒêi·ªÅu ‚Äì M·ª•c ‚Äì Ch∆∞∆°ng
    # elif datasource == 'websearch':
    #     return "websearch"  # T√¨m ki·∫øm tr√™n web ph√°p lu·∫≠t
    elif datasource == 'chitchat':
        return "chitchat"  # Tr√≤ chuy·ªán th√¢n thi·ªán"

def retrieve(state):
    print("---RETRIEVING LAW---")

    question = state["question"]
    original_question = state.get("original_question", question)

    try:
        # documents = retriever_phap_luat.invoke(question)
        documents = fallback_retriever.invoke(question)
    except Exception as e:
        print(f"  ‚ö†Ô∏è L·ªói khi retrieve v·ªõi filter: {e}")
        print(f"  üîÑ Fallback: semantic search kh√¥ng filter")

        # Fallback to simple semantic search
        documents = vectorstore_fix.similarity_search(question, k=5)

    print(f"  üìä T√¨m th·∫•y {len(documents)} t√†i li·ªáu")

    if documents:
        for i, doc in enumerate(documents, 1):
            print(f"  üìÑ Doc {i}: {doc.page_content[:150]}...")

    return {
        **state,
        "documents": documents,
        "original_question": original_question
    }

### Generate

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# Custom detailed prompt for Vietnamese legal RAG
prompt_template = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t EPR (Extended Producer Responsibility - Tr√°ch nhi·ªám m·ªü r·ªông c·ªßa nh√† s·∫£n xu·∫•t) t·∫°i Vi·ªát Nam.

NHI·ªÜM V·ª§ C·ª¶A B·∫†N:
1. Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a HO√ÄN TO√ÄN tr√™n c√°c vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi
2. Tr√≠ch d·∫´n c·ª• th·ªÉ s·ªë ƒêi·ªÅu, Ch∆∞∆°ng, M·ª•c khi tr·∫£ l·ªùi
3. Gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu b·∫±ng ti·∫øng Vi·ªát
4. N·∫øu th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu, h√£y n√≥i r√µ "Th√¥ng tin n√†y kh√¥ng c√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p"

QUY T·∫ÆC TR·∫¢ L·ªúI:
- KH√îNG b·ªãa ƒë·∫∑t ho·∫∑c th√™m th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu
- KH√îNG suy di·ªÖn ra ngo√†i ph·∫°m vi c·ªßa t√†i li·ªáu
- Lu√¥n tr√≠ch d·∫´n ngu·ªìn (ƒêi·ªÅu, Ch∆∞∆°ng, M·ª•c) khi c√≥ th·ªÉ
- KH√îNG s·ª≠ d·ª•ng c·ª•m t·ª´ "T√†i li·ªáu 1", "T√†i li·ªáu 2" - CH·ªà d√πng "ƒêi·ªÅu X", "Ch∆∞∆°ng Y", "M·ª•c Z"
- S·ª≠ d·ª•ng ng√¥n ng·ªØ ph√°p l√Ω ch√≠nh x√°c nh∆∞ng d·ªÖ hi·ªÉu
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch nh∆∞ng ƒë·∫ßy ƒë·ªß th√¥ng tin

ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI M·∫™U:
"Theo ƒêi·ªÅu X (T√™n ƒëi·ªÅu), [n·ªôi dung ch√≠nh]. C·ª• th·ªÉ, [gi·∫£i th√≠ch chi ti·∫øt]..."

N·∫øu c√≥ nhi·ªÅu ƒëi·ªÅu li√™n quan:
"V·ªÅ v·∫•n ƒë·ªÅ n√†y:
- Theo ƒêi·ªÅu X (T√™n ƒëi·ªÅu): [n·ªôi dung]
- Theo ƒêi·ªÅu Y (T√™n ƒëi·ªÅu): [n·ªôi dung]"

ƒê·∫∂C BI·ªÜT CH√ö √ù:
- N·∫øu c√¢u h·ªèi d·∫°ng "ƒêi·ªÅu X c√≥ n√≥i v·ªÅ Y kh√¥ng?":
  * N·∫øu t√†i li·ªáu c√≥ ƒêi·ªÅu X nh∆∞ng KH√îNG ƒë·ªÅ c·∫≠p Y ‚Üí Tr·∫£ l·ªùi r√µ r√†ng: "KH√îNG, ƒêi·ªÅu X kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn Y. ƒêi·ªÅu X quy ƒë·ªãnh v·ªÅ..."
  * N·∫øu t√†i li·ªáu c√≥ ƒêi·ªÅu X v√† C√ì ƒë·ªÅ c·∫≠p Y ‚Üí Tr·∫£ l·ªùi: "C√ì, ƒêi·ªÅu X c√≥ quy ƒë·ªãnh v·ªÅ Y. C·ª• th·ªÉ..."
  * KH√îNG n√≥i "kh√¥ng t√¨m th·∫•y trong c∆° s·ªü d·ªØ li·ªáu" n·∫øu ƒë√£ c√≥ t√†i li·ªáu v·ªÅ ƒêi·ªÅu X

V√ç D·ª§:
C√¢u h·ªèi: "ƒêi·ªÅu 7 c√≥ n√≥i v·ªÅ l·ªëp xe kh√¥ng?"
T√†i li·ªáu: [ƒêi·ªÅu 7: Quy ƒë·ªãnh v·ªÅ qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠...]
‚úÖ ƒê√∫ng: "KH√îNG, ƒêi·ªÅu 7 kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn l·ªëp xe. ƒêi·ªÅu 7 quy ƒë·ªãnh v·ªÅ qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠..."
‚ùå Sai: "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong c∆° s·ªü d·ªØ li·ªáu"

C√¢u h·ªèi: "ƒêi·ªÅu 7 quy ƒë·ªãnh g√¨?"
‚úÖ ƒê√öNG: "ƒêi·ªÅu 7 quy ƒë·ªãnh v·ªÅ tr√¨nh t·ª±, th·ªß t·ª•c ban h√†nh k·∫ø ho·∫°ch qu·ªëc gia v·ªÅ qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng m√¥i tr∆∞·ªùng kh√¥ng kh√≠..."
‚ùå SAI: "KH√îNG, ƒêi·ªÅu 7 kh√¥ng n√≥i v·ªÅ l·ªëp xe..." (ƒê√¢y l√† tr·∫£ l·ªùi c√¢u h·ªèi kh√°c!)

===============================================
T√ÄI LI·ªÜU PH√ÅP LU·∫¨T THAM KH·∫¢O:

{context}

===============================================
C√ÇU H·ªéI: {question}

TR·∫¢ L·ªúI:"""


prompt = ChatPromptTemplate.from_template(prompt_template)

# LLM
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)


def format_docs(docs, max_docs: int = 5, max_tokens_per_doc: int = 800):
    """
    Format documents with metadata for LLM context with token limits

    Args:
        docs: List of documents to format
        max_docs: Maximum number of documents to include (default: 5)
        max_tokens_per_doc: Maximum tokens per document content (default: 800)

    Returns:
        Formatted string with document content
    """
    if not docs:
        return "Kh√¥ng c√≥ t√†i li·ªáu li√™n quan."

    # Limit number of documents
    docs_to_use = docs[:max_docs]

    formatted_parts = []
    for i, doc in enumerate(docs_to_use, 1):
        metadata = doc.metadata

        # Build citation label from metadata
        citation_parts = []
        if metadata.get('Dieu'):
            citation_parts.append(f"ƒêi·ªÅu {metadata.get('Dieu')}")
        if metadata.get('Muc'):
            citation_parts.append(f"M·ª•c {metadata.get('Muc')}")
        if metadata.get('Chuong'):
            citation_parts.append(f"Ch∆∞∆°ng {metadata.get('Chuong')}")

        # Create citation label
        if citation_parts:
            citation = ", ".join(citation_parts)
        else:
            citation = f"T√†i li·ªáu {i}"

        # Truncate document content to fit token limit
        content = truncate_text(doc.page_content, max_tokens=max_tokens_per_doc)

        # Include metadata in the formatted output
        doc_with_meta = f"""[{citation}]
T√™n ƒêi·ªÅu: {metadata.get('Dieu_Name', 'N/A')}
T√™n Ch∆∞∆°ng: {metadata.get('Chuong_Name', 'N/A')}
T√™n M·ª•c: {metadata.get('Muc_Name', 'N/A')}

N·ªôi dung:
{content}
"""
        formatted_parts.append(doc_with_meta)

    return "\n\n---\n\n".join(formatted_parts)
# Chain
rag_chain = prompt | llm | StrOutputParser()

# Generate function
def generate(state):
    """Generate answer using RAG with detailed prompt"""
    print("---GENERATE---")
    question = state["question"]
    documents = state["documents"]
    retries = state.get("retries", 0)

    if not documents:
        print("   ‚ö†Ô∏è No documents available")


    else:
        # Format documents with metadata
        context = format_docs(documents)
        print(f"   üìÑ Generating from {len(documents)} documents")

        # Generate answer
        generation = rag_chain.invoke({"context": context, "question": question})

    return {
        "documents": documents,
        "question": question,
        "generation": generation,
        "retries": retries
    }

def decide_to_generate(state):
    """
    Determines whether to generate an answer, or re-generate a question.
    Implements retry logic - allows up to 3 query transformations before giving up.

    Args:
        state (dict): The current graph state

    Returns:
        str: Binary decision for next node to call
    """

    print("---ASSESS GRADED DOCUMENTS---")
    question = state["question"]
    filtered_documents = state["documents"]
    retries = state.get("retries", 0)  # Get current retries from state
    max_retries = 3

    print(f"   Current retries: {retries}/{max_retries}")
    print(f"   Filtered documents: {len(filtered_documents)}")

    if not filtered_documents:
        # All documents have been filtered check_relevance

        if retries < max_retries:
            # Still have retries left - transform query
            print(f"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY (Attempt {retries + 1}/{max_retries})---")
            state["retries"] = retries + 1  # Increment retries
            return "transform_query"
        else:
            # Max retries reached - give up
            print(f"---DECISION: MAX RETRIES ({max_retries}) REACHED, GENERATING ANSWER WITH NO RELEVANT DOCUMENTS---")
            return "web_search"
    else:
        # We have relevant documents, so generate answer
        print(f"---DECISION: GENERATE WITH {len(filtered_documents)} RELEVANT DOCUMENTS---")
        return "generate"

### Web Search - Return Links Only

from langchain_community.tools.tavily_search import TavilySearchResults
import os

# Initialize web search tool with error handling
try:
    tavily_api_key = os.getenv("TAVILY_API_KEY")
    if not tavily_api_key or tavily_api_key == "your-tavily-api-key-here":
        print("‚ö†Ô∏è WARNING: TAVILY_API_KEY not configured. Web search will not work.")
        print("   Please set TAVILY_API_KEY in your .env file to enable web search.")
        web_search_tool = None
    else:
        web_search_tool = TavilySearchResults(k=3)
        print("‚úÖ Tavily web search tool initialized successfully!")
except Exception as e:
    print(f"‚ö†Ô∏è WARNING: Failed to initialize Tavily web search: {e}")
    web_search_tool = None

def web_search(state):
    """
    Perform web search and store results in web_urls
    Does NOT generate final response - that's done by generate_web

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Updates web_urls with search results
    """
    print("---WEB SEARCH FOR ADDITIONAL RESOURCES---")
    question = state["question"]

    # Check if web search tool is available
    if web_search_tool is None:
        print("   ‚ö†Ô∏è Web search tool not available (TAVILY_API_KEY not configured)")
        links_text = f"""C√¢u h·ªèi "{question}" kh√¥ng t√¨m th·∫•y trong c∆° s·ªü d·ªØ li·ªáu ph√°p lu·∫≠t EPR.

‚ö†Ô∏è TH√îNG B√ÅO:
T√≠nh nƒÉng t√¨m ki·∫øm web ch∆∞a ƒë∆∞·ª£c k√≠ch ho·∫°t. ƒê·ªÉ s·ª≠ d·ª•ng t√≠nh nƒÉng n√†y:
1. ƒêƒÉng k√Ω t√†i kho·∫£n t·∫°i https://tavily.com
2. L·∫•y API key
3. Th√™m TAVILY_API_KEY v√†o file .env

üí° G·ª¢I √ù:
- Th·ª≠ ƒë·∫∑t c√¢u h·ªèi kh√°c ho·∫∑c c·ª• th·ªÉ h∆°n
- Ki·ªÉm tra ch√≠nh t·∫£ v√† t·ª´ kh√≥a
- Li√™n h·ªá chuy√™n gia ph√°p l√Ω ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n tr·ª±c ti·∫øp
"""
        return {
            "question": question,
            "web_urls": links_text,
        }

    try:
        # Perform web search
        print(f"   üîç Searching web for: {question}")
        search_results = web_search_tool.invoke({"query": question})

        # Format results as links
        if search_results:
            links_text = f"""C√¢u h·ªèi "{question}" kh√¥ng t√¨m th·∫•y trong c∆° s·ªü d·ªØ li·ªáu ph√°p lu·∫≠t EPR.

üìö C√ÅC NGU·ªíN THAM KH·∫¢O T·ª™ WEB:

"""
            for i, result in enumerate(search_results, 1):
                title = result.get("title", "Kh√¥ng c√≥ ti√™u ƒë·ªÅ")
                url = result.get("url", "")
                snippet = result.get("content", "")[:200] + "..." if result.get("content") else ""

                links_text += f"{i}. {title}\n"
                links_text += f"   üîó {url}\n"
                if snippet:
                    links_text += f"   üìù {snippet}\n"
                links_text += "\n"

            links_text += """
‚ö†Ô∏è L∆ØU √ù:
- C√°c ngu·ªìn tr√™n t·ª´ Internet, ch∆∞a ƒë∆∞·ª£c ki·ªÉm ch·ª©ng
- Vui l√≤ng x√°c minh ƒë·ªô ch√≠nh x√°c t·ª´ c∆° quan c√≥ th·∫©m quy·ªÅn
- ƒê·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c, li√™n h·ªá lu·∫≠t s∆∞ chuy√™n ng√†nh
"""
            print(f"   ‚úÖ Found {len(search_results)} web results")
        else:
            links_text = f"Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ t√¨m ki·∫øm web v·ªÅ '{question}'."
            print(f"   ‚ö†Ô∏è  No web results found")

    except Exception as e:
        print(f"   ‚ùå Web search error: {e}")
        import traceback
        print(f"   Error details: {traceback.format_exc()}")
        links_text = f"""Kh√¥ng th·ªÉ th·ª±c hi·ªán t√¨m ki·∫øm web cho c√¢u h·ªèi "{question}".

‚ùå L·ªñI: {str(e)}

üí° G·ª¢I √ù:
- Ki·ªÉm tra k·∫øt n·ªëi Internet
- Ki·ªÉm tra TAVILY_API_KEY trong file .env
- Th·ª≠ l·∫°i sau v√†i ph√∫t
- Li√™n h·ªá qu·∫£n tr·ªã vi√™n n·∫øu l·ªói v·∫´n ti·∫øp di·ªÖn
"""

    return {
        "question": question,
        "web_urls": links_text,
    }

### Generate Web - Separate Function for Web Search Results

def generate_web(state):
    """
    Generate response from web search results

    Args:
        state (dict): The current graph state

    Returns:
        dict: Updated state with web search results as generation
    """
    print("---GENERATE WEB RESPONSE---")

    question = state["question"]
    web_urls = state.get("web_urls", "")

    if web_urls:
        print(f"   üåê Formatting web search results")
        generation = web_urls
    else:
        print(f"   ‚ö†Ô∏è  No web URLs found")
        generation = f"Xin l·ªói, kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ '{question}'"

    print(f"   ‚úÖ Generated web response")

    return {
        "question": question,
        "generation": generation,
        "web_urls": web_urls
    }

### Hallucination Grader - Ki·ªÉm tra ·∫£o gi√°c

from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate

# Data model
class GradeHallucinations(BaseModel):
    """ƒê√°nh gi√° nh·ªã ph√¢n xem c√¢u tr·∫£ l·ªùi c√≥ d·ª±a tr√™n t√†i li·ªáu hay kh√¥ng."""

    binary_score: str = Field(
        description="C√¢u tr·∫£ l·ªùi c√≥ d·ª±a tr√™n t√†i li·ªáu kh√¥ng, 'c√≥' ho·∫∑c 'kh√¥ng'"
    )

# LLM with function call
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
structured_llm_grader = llm.with_structured_output(GradeHallucinations)

# Prompt
system = """B·∫°n l√† chuy√™n gia ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi AI trong lƒ©nh v·ª±c ph√°p lu·∫≠t EPR Vi·ªát Nam.

üéØ M·ª§C TI√äU:
X√°c ƒë·ªãnh xem c√¢u tr·∫£ l·ªùi c·ªßa AI c√≥ HO√ÄN TO√ÄN d·ª±a tr√™n c√°c t√†i li·ªáu ph√°p lu·∫≠t ƒë∆∞·ª£c cung c·∫•p hay kh√¥ng.

üìã TI√äU CH√ç ƒê√ÅNH GI√Å 'C√ì' (c√¢u tr·∫£ l·ªùi t·ªët):
‚úì M·ªçi th√¥ng tin trong c√¢u tr·∫£ l·ªùi ƒë·ªÅu c√≥ trong t√†i li·ªáu
‚úì S·ªë ƒêi·ªÅu, Ch∆∞∆°ng, M·ª•c ƒë∆∞·ª£c tr√≠ch d·∫´n CH√çNH X√ÅC kh·ªõp v·ªõi t√†i li·ªáu
‚úì C√¢u tr·∫£ l·ªùi c√≥ th·ªÉ t√≥m t·∫Øt ho·∫∑c di·ªÖn gi·∫£i t√†i li·ªáu
‚úì Ng√¥n ng·ªØ kh√°c nhau nh∆∞ng √Ω nghƒ©a gi·ªëng t√†i li·ªáu

‚ùå TI√äU CH√ç ƒê√ÅNH GI√Å 'KH√îNG' (c√¢u tr·∫£ l·ªùi c√≥ v·∫•n ƒë·ªÅ):
‚úó C√¢u tr·∫£ l·ªùi c√≥ th√¥ng tin KH√îNG C√ì trong t√†i li·ªáu
‚úó S·ªë ƒêi·ªÅu, Ch∆∞∆°ng, M·ª•c SAI ho·∫∑c kh√¥ng kh·ªõp
‚úó C√¢u tr·∫£ l·ªùi th√™m chi ti·∫øt kh√¥ng c√≥ trong t√†i li·ªáu
‚úó C√¢u tr·∫£ l·ªùi ƒë∆∞a ra √Ω ki·∫øn c√° nh√¢n kh√¥ng c√≥ c∆° s·ªü
‚úó C√¢u tr·∫£ l·ªùi suy lu·∫≠n th√¥ng tin kh√¥ng ƒë∆∞·ª£c t√†i li·ªáu h·ªó tr·ª£

üîç ƒê·∫∂C BI·ªÜT CH√ö √ù:
- Ki·ªÉm tra k·ªπ c√°c con s·ªë: s·ªë ƒêi·ªÅu, Kho·∫£n, M·ª•c, Ch∆∞∆°ng, nƒÉm
- Ki·ªÉm tra t√™n ch√≠nh x√°c c·ªßa c√°c ƒëi·ªÅu lu·∫≠t
- Kh√¥ng ch·∫•p nh·∫≠n th√¥ng tin "g·∫ßn ƒë√∫ng" ho·∫∑c "c√≥ th·ªÉ suy ra"

‚öñÔ∏è K·∫æT LU·∫¨N:
Tr·∫£ l·ªùi 'c√≥' ch·ªâ khi c√¢u tr·∫£ l·ªùi HO√ÄN TO√ÄN d·ª±a tr√™n t√†i li·ªáu.
Tr·∫£ l·ªùi 'kh√¥ng' n·∫øu c√≥ B·∫§T K·ª≤ th√¥ng tin n√†o kh√¥ng ƒë∆∞·ª£c t√†i li·ªáu h·ªó tr·ª£.

H√£y ƒë∆∞a ra ƒë√°nh gi√°: 'c√≥' ho·∫∑c 'kh√¥ng'"""

hallucination_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "T√†i li·ªáu ph√°p lu·∫≠t: \n\n {documents} \n\n C√¢u tr·∫£ l·ªùi c·ªßa AI: {generation}"),
    ]
)

hallucination_grader = hallucination_prompt | structured_llm_grader

print("‚úÖ Hallucination grader ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!")

### Answer Grader - ƒê√°nh gi√° c√¢u tr·∫£ l·ªùi c√≥ gi·∫£i quy·∫øt c√¢u h·ªèi kh√¥ng

from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate

# Data model
class GradeAnswer(BaseModel):
    """ƒê√°nh gi√° nh·ªã ph√¢n xem c√¢u tr·∫£ l·ªùi c√≥ gi·∫£i quy·∫øt ƒë∆∞·ª£c c√¢u h·ªèi hay kh√¥ng."""

    binary_score: str = Field(
        description="C√¢u tr·∫£ l·ªùi c√≥ gi·∫£i quy·∫øt c√¢u h·ªèi kh√¥ng, 'c√≥' ho·∫∑c 'kh√¥ng'"
    )

# LLM with function call
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
structured_llm_grader = llm.with_structured_output(GradeAnswer)

# Prompt
system = """B·∫°n l√† b·ªô ƒë√°nh gi√° xem c√¢u tr·∫£ l·ªùi c·ªßa AI c√≥ gi·∫£i quy·∫øt/tr·∫£ l·ªùi ƒë∆∞·ª£c c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng hay kh√¥ng.

NHI·ªÜM V·ª§:
ƒê√°nh gi√° xem c√¢u tr·∫£ l·ªùi c√≥ GI·∫¢I QUY·∫æT TR·ª∞C TI·∫æP c√¢u h·ªèi hay kh√¥ng.

QUY T·∫ÆC ƒê√ÅNH GI√Å 'C√ì':
‚úì C√¢u tr·∫£ l·ªùi cung c·∫•p th√¥ng tin m√† ng∆∞·ªùi d√πng ƒëang t√¨m ki·∫øm
‚úì C√¢u tr·∫£ l·ªùi tr·∫£ l·ªùi ƒë√∫ng tr·ªçng t√¢m c√¢u h·ªèi
‚úì Ng∆∞·ªùi d√πng c√≥ th·ªÉ hi·ªÉu v√† s·ª≠ d·ª•ng ƒë∆∞·ª£c th√¥ng tin trong c√¢u tr·∫£ l·ªùi
‚úì C√¢u tr·∫£ l·ªùi c√≥ th·ªÉ d√†i ho·∫∑c ng·∫Øn, nh∆∞ng ph·∫£i ƒê√öNG TR·ªåNG T√ÇM

QUY T·∫ÆC ƒê√ÅNH GI√Å 'KH√îNG':
‚úó C√¢u tr·∫£ l·ªùi kh√¥ng li√™n quan ƒë·∫øn c√¢u h·ªèi
‚úó C√¢u tr·∫£ l·ªùi n√© tr√°nh ho·∫∑c kh√¥ng tr·∫£ l·ªùi tr·ª±c ti·∫øp
‚úó C√¢u tr·∫£ l·ªùi qu√° chung chung, kh√¥ng cung c·∫•p th√¥ng tin c·ª• th·ªÉ
‚úó C√¢u tr·∫£ l·ªùi n√≥i "kh√¥ng c√≥ th√¥ng tin" khi ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi c·ª• th·ªÉ

H√£y ƒë∆∞a ra ƒë√°nh gi√°: 'c√≥' ho·∫∑c 'kh√¥ng'"""

answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: \n\n {question} \n\n C√¢u tr·∫£ l·ªùi c·ªßa AI: {generation}"),
    ]
)

answer_grader = answer_prompt | structured_llm_grader

print("‚úÖ Answer grader ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!")

def grade_generation_v_documents_and_question(state):
    """Grade generation quality"""
    print("---KI·ªÇM TRA CH·∫§T L∆Ø·ª¢NG C√ÇU TR·∫¢ L·ªúI---")
    
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]
    generation_retries = int(state.get("generation_retries") or 0)
    max_generation_retries = 3
    
    print(f"   Generation retries: {generation_retries}/{max_generation_retries}")
    
    
    # Determine grade_result and new_generation_retries
    grade_result = "useful"  # default
    new_generation_retries = generation_retries  # default
    
    if not documents:
        print("   ‚ö†Ô∏è  Kh√¥ng c√≥ t√†i li·ªáu, b·ªè qua grading")
        grade_result = "useful"
    else:
        formatted_docs = format_docs(documents)
        
        print("---B∆Ø·ªöC 1: KI·ªÇM TRA ·∫¢O GI√ÅC---")
        hallucination_score = hallucination_grader.invoke({
            "documents": formatted_docs,
            "generation": generation
        })
        hallucination_grade = hallucination_score.binary_score

        if hallucination_grade == "c√≥":
            print("   ‚úÖ PASS: D·ª±a tr√™n t√†i li·ªáu")
            
            print("---B∆Ø·ªöC 2: KI·ªÇM TRA C√ÇU TR·∫¢ L·ªúI---")
            answer_score = answer_grader.invoke({
                "question": question,
                "generation": generation
            })
            answer_grade = answer_score.binary_score
            
            if answer_grade == "c√≥":
                print("   ‚úÖ PASS: Gi·∫£i quy·∫øt c√¢u h·ªèi")
                print("---QUY·∫æT ƒê·ªäNH: USEFUL---")
                grade_result = "useful"
                # Don't increment
            else:
                print("   ‚ùå FAIL: Kh√¥ng gi·∫£i quy·∫øt c√¢u h·ªèi")
                
                if generation_retries < max_generation_retries:
                    print(f"---T·∫†O L·∫†I: L·∫ßn {generation_retries + 1}/{max_generation_retries}---")
                    grade_result = "not useful"
                    new_generation_retries = generation_retries + 1  # ‚úÖ INCREMENT
                else:
                    print(f"---H·∫æT L·∫¶N TH·ª¨: CHUY·ªÇN WEB SEARCH---")
                    grade_result = "web_search"
        else:
            print("   ‚ùå FAIL: C√≥ ·∫£o gi√°c")
            
            if generation_retries < max_generation_retries:
                print(f"---T·∫†O L·∫†I: L·∫ßn {generation_retries + 1}/{max_generation_retries}---")
                grade_result = "not supported"
                new_generation_retries = generation_retries + 1  # ‚úÖ INCREMENT
            else:
                print(f"---H·∫æT L·∫¶N TH·ª¨: CHUY·ªÇN WEB SEARCH---")
                grade_result = "web_search"
    
    print(f"\nüîç RETURNING STATE:")
    print(f"   grade_result: {grade_result}")
    print(f"   generation_retries: {new_generation_retries} (was {generation_retries})")
    
    # ‚úÖ Return ALL state fields
    return {
        "question": state.get("question"),
        "original_question": state.get("original_question"),
        "chat_history": state.get("chat_history", ""),
        "generation": state.get("generation"),
        "documents": state.get("documents", []),
        "retries": state.get("retries", 0),
        "generation_retries": new_generation_retries,  # ‚úÖ Updated value
        "grade_result": grade_result,  # ‚úÖ Updated value
        "hallucination_detected": hallucination_grade == "kh√¥ng" if documents else False
    }

# 

def decide_after_grade_generation(state):
    """Decide next step"""
    print(f"\n{'='*80}")
    print(f"üîç ROUTING FUNCTION - FULL DEBUG")
    print(f"{'='*80}")
    
    # Print EVERYTHING
    print("Full state received:")
    for key, val in state.items():
        if key not in ["documents", "chat_history"]:
            print(f"  {key}: {repr(val)}")
    
    grade_result = state.get("grade_result", "useful")
    
    print(f"\nExtracted:")
    print(f"  grade_result: {repr(grade_result)}")
    print(f"  Type: {type(grade_result)}")
    print(f"  Is 'not supported': {grade_result == 'not supported'}")
    print(f"  Is 'useful': {grade_result == 'useful'}")
    
    print(f"\nüîÄ ROUTING DECISION: {grade_result}")
    print(f"{'='*80}\n")
    
    if grade_result == "not supported":
        print("  ‚Üí Routing to 'not supported' (regenerate)")
        return "not supported"
    elif grade_result == "useful":
        print("  ‚Üí Routing to 'useful' (END)")
        return "useful"
    elif grade_result == "not useful":
        print("  ‚Üí Routing to 'not useful' (transform)")
        return "not useful"
    elif grade_result == "web_search":
        print("  ‚Üí Routing to 'web_search'")
        return "web_search"
    else:
        print(f"  ‚Üí Unknown value, defaulting to 'useful'")
        return "useful"

from langgraph.graph import END, StateGraph
from typing import TypedDict, List

class GraphState(TypedDict):
    question: str
    generation: str
    documents: List[str]
    
    original_question: str
    chat_history: str
    retries: int
    generation_retries: int
    grade_result: str 
    hallucination_detected: bool 
    web_urls: str

# ========== BUILD WORKFLOW ==========

workflow = StateGraph(GraphState)

# Add initial routing node (no transformation - just routes)
def initial_route_node(state):
    """Initial routing node - passes question through without transformation"""
    print("---INITIAL ROUTING NODE---")
    question = state["question"]
    chat_history = get_full_chat_history()

    # Save original question and chat history at the very beginning
    if "original_question" not in state or not state.get("original_question"):
        print(f"  üíæ Saving original question: {question}")
        state["original_question"] = question

    if "original_chat_history" not in state or not state.get("original_chat_history"):
        print(f"  üíæ Saving chat history snapshot ({len(chat_history)} chars)")
        state["original_chat_history"] = chat_history

    # Just return state without transformation
    return {
        **state,
        "question": question,
        "chat_history": chat_history
    }

# Add nodes
workflow.add_node("initial_route", initial_route_node)
workflow.add_node("retrieve_faq", retrieve_faq_node)

workflow.add_node("generate_faq", generate_faq_node)
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate)

workflow.add_node("transform_query1", transform_query)
workflow.add_node("transform_query2", transform_query)
workflow.add_node("transform_query3", transform_query)

workflow.add_node("grade_documents", grade_documents)
workflow.add_node("grade_generation", grade_generation_v_documents_and_question)

workflow.add_node("chitchat1", chitchat)
workflow.add_node("chitchat2", chitchat)
workflow.add_node("web_search1", web_search) # web search
workflow.add_node("generate_web1", generate_web) # generatae

workflow.add_node("web_search2", web_search) # web search
workflow.add_node("generate_web2", generate_web) # generatae

workflow.add_node("new_round_router", new_round_router)

# Set entry point with routing BEFORE transformation
workflow.set_entry_point("initial_route")
workflow.add_conditional_edges(
    "initial_route",
    route_question_faq,
    {
        "vectorstore_faq": "transform_query1",  # Transform only for FAQ path
        "chitchat": "chitchat1",  # No transformation for chitchat
    },
)

# After transforming FAQ queries, retrieve
workflow.add_edge("transform_query1", "retrieve_faq")

# Add edges
workflow.add_edge("chitchat1", END)

# Conditional edges from grade_faq_documents
workflow.add_conditional_edges(
    "retrieve_faq",
    decide_after_retrieve_faq,
    {
        "generate_faq": "generate_faq",
        "new_round_router": "new_round_router",
    },
)

# Transform query loops back to retrieve
workflow.add_edge("generate_faq", END)

workflow.add_edge("new_round_router", "transform_query2")

workflow.add_conditional_edges(
    "transform_query2",
    route_question_law,
    {
        "vectorstore": "retrieve",
        "chitchat": "chitchat2",
    },
)

workflow.add_edge("chitchat2", END)
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "transform_query": "transform_query3",
        "generate": "generate",
        "web_search":"web_search1"
    },
)
workflow.add_edge("transform_query3", "retrieve")



workflow.add_edge("web_search1","generate_web1")

workflow.add_edge("generate_web1",END)

workflow.add_edge("generate", "grade_generation")

workflow.add_conditional_edges(
    "grade_generation",
    decide_after_grade_generation,
    {
        "useful": END,              # Good answer
        "not useful": "transform_query3",   # Regenerate with same docs
        "not supported": "generate", # Regenerate (hallucination)
        "web_search": "web_search2"  # Max retries, go to web search
    }
)
workflow.add_edge("web_search2","generate_web2")
workflow.add_edge("generate_web2",END)



# Compile the graph
app = workflow.compile()

print("‚úÖ Workflow compiled successfully!")



def get_full_chat_history(max_exchanges=3):
    """
    Get recent chat history from memory

    Args:
        max_exchanges: Number of recent conversation pairs to keep (default: 3)
        Each exchange = 1 user message + 1 assistant message = 2 messages total
        Reduced from 5 to 3 to prevent context overflow

    Returns:
        Formatted chat history string
    """
    try:
        memory_vars = conversation_memory.load_memory_variables({})
        if "chat_history" in memory_vars:
            messages = memory_vars["chat_history"]

            if messages:
                # Keep only last N exchanges (N*2 messages)
                recent_messages = messages[-(max_exchanges * 2):]

                formatted = []
                for msg in recent_messages:
                    if hasattr(msg, 'type'):
                        role = "User" if msg.type == "human" else "Assistant"
                        content = msg.content
                        # Truncate individual messages to prevent overflow
                        content = truncate_text(content, max_tokens=500)
                        formatted.append(f"{role}: {content}")
                    else:
                        formatted.append(str(msg))

                chat_history = "\n".join(formatted)

                # Ensure total chat history doesn't exceed limit
                return truncate_text(chat_history, max_tokens=2000)
    except Exception as e:
        print(f"  ‚ö†Ô∏è Error loading history: {e}")
    return ""

print("‚úì get_full_chat_history with limit created")

def clear_memory():
    """X√≥a to√†n b·ªô b·ªô nh·ªõ h·ªôi tho·∫°i"""
    conversation_memory.clear()
    print("‚ú® ƒê√£ x√≥a to√†n b·ªô b·ªô nh·ªõ h·ªôi tho·∫°i th√†nh c√¥ng!")


# === Test 11: Clear memory ===
print("\nüìù TEST 11: CLEAR MEMORY")
clear_memory()

def test_graph(question: str):
    """Test graph with proper initialization"""
    print("\n" + "#"*80)
    print("ü§ñ TESTING GRAPH")
    print("#"*80)
    print(f"Question: {question}")

    real_chat_history = get_full_chat_history(max_exchanges=5)

    initial_state = {
        "question": question,
        "generation": "",
        "documents": [],
        "original_question": question,
        "chat_history": real_chat_history,
        "retries": 0,
        "generation_retries": 0,  # ‚úÖ CRITICAL: Initialize to 0, not None
        "original_chat_history": "",
        "web_urls": "",
        "hallucination_detected": False,
        "answer_quality": "",
        "grade_result": ""
    }

    final_state = app.invoke(initial_state)

    print("\n" + "#"*80)
    print("‚úÖ COMPLETE")
    print(f"Answer: {final_state.get('generation', '')}")
    print(f"Query retries used: {final_state.get('retries', 0)}")
    print(f"Generation retries used: {final_state.get('generation_retries', 0)}")
    print("#"*80 + "\n")

    return final_state

# ========== RUN TESTS (COMMENTED OUT FOR MODULE IMPORT) ==========
# Uncomment below to run standalone tests

# if __name__ == "__main__":
#     print("\n" + "="*80)
#     print("üß™ TESTING WITH REAL MEMORY")
#     print("="*80)
#
#     test_cases = [
#         "b·∫°n l√†m ƒë∆∞·ª£c g√¨?",
#         "b·∫°n bi·∫øt ƒë∆∞·ª£c bao nhi√™u ƒëi·ªÅu lu·∫≠t",
#         "k·ªÉ v·ªÅ ƒëi·ªÅu 1 v√† 2",
#         "n·∫øu b√¢y gi·ªù t√¥i mu·ªën l√†m 2 ƒëi·ªÅu ƒë√≥, t√¥i c·∫ßn l√†m nh·ªØng g√¨ ·ªü t·ª´ng ƒëi·ªÅu lu·∫≠t?",
#         "h∆∞·ªõng d·∫´n t∆∞∆°ng t·ª± cho ƒëi·ªÅu 4",
#         "c√°ch n·∫•u m√¨ x√†o ngon?",
#     ]
#
#     for question in test_cases:
#         result = test_graph(question)
#
#         try:
#             conversation_memory.save_context(
#                 {"input": question},
#                 {"generation": result.get('generation', 'No response')}
#             )
#             print(f"üíæ Saved to memory: Q='{question[:30]}...' A='{result.get('generation', '')[:30]}...'\n")
#         except Exception as e:
#             print(f"‚ö†Ô∏è  Could not save to memory: {e}\n")
#
#     print("="*80)
#     print("‚úÖ All tests complete!")
#     print("="*80)
#
#     print("\n" + "="*80)
#     print("üìö FINAL MEMORY STATE")
#     print("="*80)
#     final_history = get_full_chat_history(max_exchanges=10)
#     print(f"Total history: {len(final_history)} chars\n")
#     print(final_history)
#     print("="*80)

print("‚úÖ EPR Chatbot Core Module Loaded Successfully!")


# ============================================================================
# üöÄ PERFORMANCE OPTIMIZATIONS: ASYNC + STREAMING
# ============================================================================

import asyncio
from typing import AsyncIterator, Dict, Any

print("\n" + "="*80)
print("üöÄ Loading Performance Optimizations...")
print("="*80)

# ========== ASYNC PARALLEL RETRIEVAL ==========

async def retrieve_faq_async(query: str, score_threshold: float = 0.6):
    """Async version of FAQ retrieval"""
    print("  üîç [ASYNC] Retrieving FAQ...")

    # Run synchronous retrieval in thread pool
    loop = asyncio.get_event_loop()
    documents = await loop.run_in_executor(
        None,
        retrieve_faq_top1,
        query,
        score_threshold
    )

    print(f"  ‚úÖ [ASYNC] FAQ retrieval done: {len(documents)} docs")
    return documents


async def retrieve_legal_async(question: str):
    """Async version of legal document retrieval"""
    print("  üìö [ASYNC] Retrieving legal docs...")

    # Run synchronous retrieval in thread pool
    loop = asyncio.get_event_loop()

    try:
        documents = await loop.run_in_executor(
            None,
            fallback_retriever.invoke,
            question
        )
    except Exception as e:
        print(f"  ‚ö†Ô∏è [ASYNC] Error: {e}, falling back to similarity search")
        documents = await loop.run_in_executor(
            None,
            vectorstore_fix.similarity_search,
            question,
            5
        )

    print(f"  ‚úÖ [ASYNC] Legal retrieval done: {len(documents)} docs")
    return documents


async def parallel_retrieve(query: str, faq_threshold: float = 0.6):
    """
    Retrieve FAQ and legal documents in parallel for maximum speed

    Args:
        query: User's question
        faq_threshold: Minimum score for FAQ match

    Returns:
        dict: {
            'faq_docs': list of FAQ documents,
            'legal_docs': list of legal documents,
            'faq_time': float (seconds),
            'legal_time': float (seconds)
        }
    """
    import time

    print("\n" + "="*80)
    print("‚ö° PARALLEL RETRIEVAL")
    print("="*80)
    print(f"Query: {query}")

    start_time = time.time()

    # Run both retrievals in parallel
    faq_docs, legal_docs = await asyncio.gather(
        retrieve_faq_async(query, faq_threshold),
        retrieve_legal_async(query),
        return_exceptions=True
    )

    total_time = time.time() - start_time

    # Handle exceptions
    if isinstance(faq_docs, Exception):
        print(f"  ‚ö†Ô∏è FAQ retrieval failed: {faq_docs}")
        faq_docs = []

    if isinstance(legal_docs, Exception):
        print(f"  ‚ö†Ô∏è Legal retrieval failed: {legal_docs}")
        legal_docs = []

    print(f"  ‚ö° Total parallel retrieval time: {total_time:.2f}s")
    print(f"  üìä Results: FAQ={len(faq_docs)}, Legal={len(legal_docs)}")
    print("="*80)

    return {
        'faq_docs': faq_docs,
        'legal_docs': legal_docs,
        'total_time': total_time
    }


# ========== STREAMING LLM GENERATION ==========

def create_streaming_llm():
    """Create an LLM instance configured for streaming"""
    return ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0,
        streaming=True
    )

streaming_llm = create_streaming_llm()


async def generate_answer_streaming(query: str, documents: list, source_type: str = "faq") -> AsyncIterator[str]:
    """
    Generate answer with streaming for real-time display

    Args:
        query: User question
        documents: Retrieved documents
        source_type: "faq" or "legal"

    Yields:
        str: Chunks of the generated response
    """
    if not documents:
        yield "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p. B·∫°n c√≥ th·ªÉ h·ªèi chi ti·∫øt h∆°n kh√¥ng?"
        return

    # GPT-3.5-turbo context limit
    MAX_CONTEXT_TOKENS = 15000  # Leave buffer for response

    # Create appropriate prompt based on source
    if source_type == "faq":
        doc = documents[0]
        faq_question = doc.metadata.get("C√¢u_h·ªèi", "")
        faq_answer = doc.page_content

        # Truncate FAQ answer if too long
        faq_answer = truncate_text(faq_answer, max_tokens=2000)

        prompt = ChatPromptTemplate.from_messages([
            ("system", """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ lu·∫≠t EPR Vi·ªát Nam.
Tr·∫£ l·ªùi d·ª±a tr√™n FAQ, gi·ªØ th√¥ng tin ch√≠nh x√°c, ng·∫Øn g·ªçn v√† th√¢n thi·ªán."""),
            ("user", """C√¢u h·ªèi FAQ: {faq_question}
C√¢u tr·∫£ l·ªùi FAQ: {faq_answer}

C√¢u h·ªèi ng∆∞·ªùi d√πng: {user_question}

Tr·∫£ l·ªùi:""")
        ])

        chain = prompt | streaming_llm

        async for chunk in chain.astream({
            "faq_question": faq_question,
            "faq_answer": faq_answer,
            "user_question": query
        }):
            if hasattr(chunk, 'content'):
                yield chunk.content

    else:  # legal documents
        # Limit documents to prevent context overflow
        # Max 4 documents, each with max 1000 tokens
        context = format_docs(documents, max_docs=4, max_tokens_per_doc=1000)

        # Verify total context size
        context_tokens = count_tokens(context)
        query_tokens = count_tokens(query)
        system_prompt_tokens = 100  # Rough estimate

        total_input_tokens = context_tokens + query_tokens + system_prompt_tokens

        print(f"   üìä Context size: {context_tokens} tokens")
        print(f"   üìä Query size: {query_tokens} tokens")
        print(f"   üìä Total input: {total_input_tokens} tokens")

        if total_input_tokens > MAX_CONTEXT_TOKENS:
            print(f"   ‚ö†Ô∏è Context too large ({total_input_tokens} tokens), further reducing...")
            # Further reduce if still too large
            context = format_docs(documents, max_docs=3, max_tokens_per_doc=600)
            context_tokens = count_tokens(context)
            print(f"   ‚úÖ Reduced to {context_tokens} tokens")

        prompt = ChatPromptTemplate.from_messages([
            ("system", """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t EPR Vi·ªát Nam.
Tr·∫£ l·ªùi d·ª±a HO√ÄN TO√ÄN tr√™n t√†i li·ªáu, tr√≠ch d·∫´n ƒêi·ªÅu/Ch∆∞∆°ng c·ª• th·ªÉ."""),
            ("user", """T√†i li·ªáu ph√°p lu·∫≠t:
{context}

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi:""")
        ])

        chain = prompt | streaming_llm

        async for chunk in chain.astream({
            "context": context,
            "question": query
        }):
            if hasattr(chunk, 'content'):
                yield chunk.content


# ========== OPTIMIZED CHATBOT PIPELINE ==========

async def optimized_chatbot_pipeline(
    query: str,
    chat_history: str = "",
    faq_threshold: float = 0.6,
    use_parallel: bool = True
) -> AsyncIterator[Dict[str, Any]]:
    """
    Optimized chatbot pipeline with parallel retrieval and streaming

    Args:
        query: User's question
        chat_history: Previous conversation context
        faq_threshold: Minimum FAQ match score
        use_parallel: If True, retrieve FAQ + legal docs in parallel

    Yields:
        dict: Status updates and response chunks
    """

    print("\n" + "üîπ"*40)
    print("üöÄ OPTIMIZED PIPELINE START")
    print("üîπ"*40)

    # Step 0a: Rewrite question based on chat history (if needed)
    original_query = query
    if chat_history:
        print("---REWRITING QUESTION BASED ON CHAT HISTORY---")
        print(f"  Original query: {original_query}")
        try:
            # Use the question rewriter to contextualize the question
            rewritten_query = question_rewriter_legal.invoke({
                "question": query,
                "chat_history": chat_history
            })
            print(f"  Rewritten query: {rewritten_query}")
            # Use the rewritten query for retrieval
            query = rewritten_query
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error in question rewriting: {e}")
            print(f"  ‚û°Ô∏è Continuing with original query")
            # Continue with original query if rewriting fails
    else:
        print("---NO CHAT HISTORY - USING ORIGINAL QUESTION---")
        print(f"  Query: {query}")

    # Step 0b: Check if this is chitchat BEFORE any retrieval
    print("---CHECKING IF CHITCHAT---")
    try:
        # Use the FAQ router to check if this is chitchat
        route_result = question_router_faq.invoke({
            "question": query,
            "chat_history": chat_history
        })

        datasource = route_result.get("datasource") if isinstance(route_result, dict) else getattr(route_result, "datasource", None)
        print(f"   Routing decision: {datasource}")

        if datasource == 'chitchat':
            print("   ‚úÖ Detected as chitchat - generating friendly response")
            yield {
                'type': 'status',
                'message': 'üí¨ Generating friendly response...',
                'stage': 'chitchat'
            }

            # Call chitchat function
            state = {
                "question": query,
                "chat_history": chat_history
            }
            result_state = chitchat(state)
            chitchat_response = result_state.get("generation", "Xin ch√†o!")

            # Stream the chitchat response
            yield {
                'type': 'response_chunk',
                'chunk': chitchat_response,
                'stage': 'streaming'
            }

            # Complete
            yield {
                'type': 'response_complete',
                'text': chitchat_response,
                'documents': [],
                'source': 'chitchat',
                'stage': 'complete'
            }

            print("üîπ"*40)
            print("‚úÖ CHITCHAT COMPLETE")
            print("üîπ"*40 + "\n")
            return
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error in chitchat routing: {e}")
        # Continue to retrieval if routing fails

    print("   ‚û°Ô∏è Not chitchat - proceeding to document retrieval")

    # Step 1: Yield status - starting retrieval
    yield {
        'type': 'status',
        'message': 'üîç Searching knowledge base...',
        'stage': 'retrieval'
    }

    # Step 2: Parallel retrieval
    if use_parallel:
        results = await parallel_retrieve(query, faq_threshold)
        faq_docs = results['faq_docs']
        legal_docs = results['legal_docs']
    else:
        # Sequential fallback
        faq_docs = await retrieve_faq_async(query, faq_threshold)
        legal_docs = []
        if not faq_docs:
            legal_docs = await retrieve_legal_async(query)

    # Step 3: Determine which documents to use
    documents_to_use = []
    source_type = None

    if faq_docs:
        documents_to_use = faq_docs
        source_type = "faq"
        yield {
            'type': 'status',
            'message': '‚úÖ Found answer in FAQ',
            'stage': 'generation',
            'source': 'faq'
        }
    elif legal_docs:
        documents_to_use = legal_docs
        source_type = "legal"
        yield {
            'type': 'status',
            'message': '‚úÖ Found relevant legal documents',
            'stage': 'generation',
            'source': 'legal'
        }
    else:
        # No documents found - try web search
        yield {
            'type': 'status',
            'message': 'üåê Searching web for additional information...',
            'stage': 'web_search'
        }

        # Call web search
        web_state = {
            "question": query
        }
        web_result = web_search(web_state)
        web_urls = web_result.get("web_urls", "")

        if web_urls:
            yield {
                'type': 'response_chunk',
                'chunk': web_urls,
                'stage': 'streaming'
            }

            yield {
                'type': 'response_complete',
                'text': web_urls,
                'documents': [],
                'source': 'web_search',
                'stage': 'complete'
            }
        else:
            yield {
                'type': 'response_complete',
                'text': 'Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong c∆° s·ªü d·ªØ li·ªáu ho·∫∑c tr√™n web.',
                'documents': [],
                'source': None,
                'stage': 'complete'
            }

        print("üîπ"*40)
        print("‚úÖ WEB SEARCH COMPLETE")
        print("üîπ"*40 + "\n")
        return

    # Step 4: Stream the response
    full_response = ""

    async for chunk in generate_answer_streaming(query, documents_to_use, source_type):
        full_response += chunk
        yield {
            'type': 'response_chunk',
            'chunk': chunk,
            'stage': 'streaming'
        }

    # Step 5: Final metadata
    yield {
        'type': 'response_complete',
        'text': full_response,
        'documents': documents_to_use,
        'source': source_type,
        'stage': 'complete'
    }

    print("üîπ"*40)
    print("‚úÖ OPTIMIZED PIPELINE COMPLETE")
    print("üîπ"*40 + "\n")


# ========== HELPER FUNCTION FOR STREAMLIT ==========

def run_optimized_chatbot(query: str, chat_history: str = ""):
    """
    Synchronous wrapper for Streamlit
    Returns an async generator that can be consumed by Streamlit
    """
    return optimized_chatbot_pipeline(query, chat_history)


print("‚úÖ Performance optimizations loaded!")
print("   - Async parallel retrieval")
print("   - Streaming LLM responses")
print("   - Optimized pipeline")
print("="*80 + "\n")



